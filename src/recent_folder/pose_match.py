# reference_pose_matcher.py
# âœ… ë‹¨ì¼ ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ê¸°ë°˜ í¬ì¦ˆ ì˜¤ë²„ë ˆì´ + ì‹¤ì‹œê°„ ì›¹ìº  í”¼ë“œë°±

import cv2
import mediapipe as mp
import numpy as np
from pathlib import Path

PROJECT_ROOT = Path(__file__).resolve().parent
while PROJECT_ROOT != PROJECT_ROOT.parent and not ((PROJECT_ROOT / "data").exists() and (PROJECT_ROOT / "src").exists()):
    PROJECT_ROOT = PROJECT_ROOT.parent

# ğŸ”¸ MediaPipe pose ì´ˆê¸°í™” (ì •ì§€ ì´ë¯¸ì§€ìš© + ì‹¤ì‹œê°„ìš© ê°ê° ì„¤ì •)
mp_pose = mp.solutions.pose
pose_estimator = mp_pose.Pose(static_image_mode=True, model_complexity=2)  # ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ì²˜ë¦¬ìš©
predictor = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5)  # ì‹¤ì‹œê°„ ì›¹ìº  ì²˜ë¦¬ìš©
mp_drawing = mp.solutions.drawing_utils

# ğŸ”¸ ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ë¡œë”© ë° í¬ì¦ˆ ì¶”ì¶œ
REFERENCE_PATH = PROJECT_ROOT / "data" / "sample_images" / "test1.jpg"
ref_img = cv2.imread(str(REFERENCE_PATH))  # ì´ë¯¸ì§€ ì½ê¸°
ref_rgb = cv2.cvtColor(ref_img, cv2.COLOR_BGR2RGB)  # BGR -> RGB ë³€í™˜
ref_result = pose_estimator.process(ref_rgb)  # í¬ì¦ˆ ì¶”ë¡  ìˆ˜í–‰

# ğŸ”¸ í¬ì¦ˆ ê²€ì¶œ ì‹¤íŒ¨ ì‹œ ì¢…ë£Œ
if not ref_result.pose_landmarks:
    raise ValueError("ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ì—ì„œ í¬ì¦ˆë¥¼ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# ğŸ”¸ í¬ì¦ˆ keypoint ì¢Œí‘œ ì¶”ì¶œ (ì •ê·œí™” ì¢Œí‘œ â†’ ì‹¤ì œ í”½ì…€ ì¢Œí‘œ)
ref_landmarks = ref_result.pose_landmarks.landmark
ref_kps = [(int(l.x * ref_img.shape[1]), int(l.y * ref_img.shape[0])) for l in ref_landmarks]

# ğŸ”¸ ì›¹ìº  ì‹¤í–‰ ì‹œì‘
cap = cv2.VideoCapture(0)
print("ì›¹ìº  ì‹¤í–‰ ì¤‘... 'q'ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œ")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # ì‹¤ì‹œê°„ í”„ë ˆì„ RGB ë³€í™˜
    result = predictor.process(frame_rgb)  # ì‹¤ì‹œê°„ í¬ì¦ˆ ì¶”ì •

    if result.pose_landmarks:
        # ğŸ”¸ í˜„ì¬ í”„ë ˆì„ì˜ í¬ì¦ˆ keypoint ì¶”ì¶œ
        live_landmarks = result.pose_landmarks.landmark
        live_kps = [(int(l.x * frame.shape[1]), int(l.y * frame.shape[0])) for l in live_landmarks]

        # ğŸ”¸ ì‹¤ì‹œê°„ ìŠ¤ì¼ˆë ˆí†¤ ì‹œê°í™”
        mp_drawing.draw_landmarks(frame, result.pose_landmarks, mp_pose.POSE_CONNECTIONS)

        # ğŸ”¸ ë ˆí¼ëŸ°ìŠ¤ì™€ í˜„ì¬ í”„ë ˆì„ì˜ keypoint ê±°ë¦¬ ë¹„êµ (í‰ê·  ê±°ë¦¬ â†’ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°)
        distances = []
        for a, b in zip(ref_kps, live_kps):
            if a != (0, 0) and b != (0, 0):
                d = np.linalg.norm(np.array(a) - np.array(b))
                distances.append(d)
        score = max(0, 100 - int(np.mean(distances))) if distances else 0  # ì ìˆ˜ëŠ” 0~100 ë²”ìœ„ë¡œ í‘œì‹œ

        # ğŸ”¸ ì ìˆ˜ í…ìŠ¤íŠ¸ ì¶œë ¥
        cv2.putText(frame, f"Pose Match: {score}%", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)

    # ğŸ”¸ ë ˆí¼ëŸ°ìŠ¤ ìŠ¤ì¼ˆë ˆí†¤ì„ ë°˜íˆ¬ëª…í•˜ê²Œ ì˜¤ë²„ë ˆì´ (íŒŒë€ìƒ‰ ì„ )
    overlay = frame.copy()
    for idx1, idx2 in mp_pose.POSE_CONNECTIONS:
        if ref_kps[idx1] != (0, 0) and ref_kps[idx2] != (0, 0):
            cv2.line(overlay, ref_kps[idx1], ref_kps[idx2], (255, 255, 0), 2)
    frame = cv2.addWeighted(overlay, 0.4, frame, 0.6, 0)  # ì˜¤ë²„ë ˆì´ ì ìš© (0.4: ë ˆí¼ëŸ°ìŠ¤, 0.6: ì‹¤ì‹œê°„)

    # ğŸ”¸ í™”ë©´ ì¶œë ¥ ë° ì¢…ë£Œ ì¡°ê±´ í™•ì¸
    cv2.imshow("Reference Pose Match", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# ğŸ”¸ ì¢…ë£Œ ì²˜ë¦¬
cap.release()
cv2.destroyAllWindows()
