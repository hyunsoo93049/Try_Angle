# -*- coding: utf-8 -*-
"""
TryAngle ê³ ë„í™” ë²„ì „ (CLIP + YOLO Pose + Camera Angle + ColorTone)
---------------------------------------------------------------
1ï¸âƒ£ CLIP : ì¥ë©´ ê°ì„±(ë°°ê²½, í†¤, ë¶„ìœ„ê¸°)
2ï¸âƒ£ YOLO Pose : ì¸ë¬¼ì˜ êµ¬ë„Â·í”„ë ˆì´ë°
3ï¸âƒ£ ColorTone : ìƒ‰ê° ë° ì¡°ëª… ìœ ì‚¬ë„
4ï¸âƒ£ Camera Angle : ì‹œì (ë†’ì´/ê±°ë¦¬) ì°¨ì´ ë¶„ì„
"""

import cv2, torch, clip, numpy as np
from ultralytics import YOLO
from PIL import Image
from composition_module import analyze_composition
from feedback_module import generate_feedback

# ---------------------------------------------------------
# ëª¨ë¸ ì´ˆê¸°í™”
# ---------------------------------------------------------
print("ëª¨ë¸ ë¡œë“œ ì¤‘...")
device = "cuda" if torch.cuda.is_available() else "cpu"
pose_model = YOLO("yolov8s-pose.pt")
clip_model, preprocess = clip.load("ViT-B/32", device=device)
print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# ---------------------------------------------------------
# ê²½ë¡œ ì„¤ì •
# ---------------------------------------------------------
ref_path = "C:/try_angle/data/sample_images/cafe1.jpg"
tgt_path = "C:/try_angle/data/sample_images/cafe5.jpg"

ref_img = cv2.imread(ref_path)
tgt_img = cv2.imread(tgt_path)
if ref_img is None or tgt_img is None:
    raise FileNotFoundError("ì´ë¯¸ì§€ ê²½ë¡œ í™•ì¸ í•„ìš”")

# ---------------------------------------------------------
# ìƒ‰ê°/ì¡°ëª… ìœ ì‚¬ë„ ê³„ì‚° (HSV íˆìŠ¤í† ê·¸ë¨)
# ---------------------------------------------------------
def color_tone_similarity(img1, img2):
    hsv1 = cv2.cvtColor(img1, cv2.COLOR_BGR2HSV)
    hsv2 = cv2.cvtColor(img2, cv2.COLOR_BGR2HSV)
    hist1 = cv2.calcHist([hsv1],[0,1,2],None,[24,8,8],[0,180,0,256,0,256])
    hist2 = cv2.calcHist([hsv2],[0,1,2],None,[24,8,8],[0,180,0,256,0,256])
    cv2.normalize(hist1,hist1); cv2.normalize(hist2,hist2)
    sim = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
    return float((sim + 1) / 2)  # 0~1

# ---------------------------------------------------------
# CLIP ê°ì„±(ì¥ë©´) ìœ ì‚¬ë„ ê³„ì‚°
# ---------------------------------------------------------
def clip_similarity(path1, path2):
    img1 = preprocess(Image.open(path1)).unsqueeze(0).to(device)
    img2 = preprocess(Image.open(path2)).unsqueeze(0).to(device)
    with torch.no_grad():
        f1 = clip_model.encode_image(img1); f2 = clip_model.encode_image(img2)
        f1 /= f1.norm(dim=-1, keepdim=True)
        f2 /= f2.norm(dim=-1, keepdim=True)
    sim = torch.cosine_similarity(f1, f2).item()
    # CLIPì€ 0.25~0.35ê°€ 'ìœ ì‚¬'í•œ ë²”ìœ„ì´ë¯€ë¡œ ì •ê·œí™”
    normed = np.clip((sim - 0.2) / (0.35 - 0.2), 0, 1)
    return float(normed)

# ---------------------------------------------------------
# Pose ë¶„ì„ ë° êµ¬ë„ ì •ë³´
# ---------------------------------------------------------
def get_pose_info(image):
    res = pose_model(image)
    if res[0].keypoints is None:
        return None, None
    kpts = res[0].keypoints.xy[0].cpu().numpy()
    bbox = res[0].boxes.xyxy[0].cpu().numpy()
    comp = analyze_composition(image, kpts, bbox)
    return kpts, comp

# ---------------------------------------------------------
# ì¹´ë©”ë¼ ì•µê¸€ ë¹„êµ (ì‹œì  ë†’ë‚®ì´)
# ---------------------------------------------------------
def camera_angle_difference(ref_kp, tgt_kp):
    # ì½”ì™€ ëˆˆ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¹´ë©”ë¼ ì‹œì  ì¶”ì •
    if ref_kp is None or tgt_kp is None:
        return None
    # y ì¢Œí‘œ(í™”ë©´ì—ì„œ ì•„ë˜ë¡œ ì¦ê°€)
    ref_eye = np.mean(ref_kp[[1,2,3,4]], axis=0)[1]  # ëŒ€ëµ ëˆˆë¼ì¸
    tgt_eye = np.mean(tgt_kp[[1,2,3,4]], axis=0)[1]
    diff = (tgt_eye - ref_eye) / 480.0  # 480 ê¸°ì¤€ ì •ê·œí™”
    return float(diff)

# ---------------------------------------------------------
# ì‹¤í–‰
# ---------------------------------------------------------
ref_kp, ref_comp = get_pose_info(ref_img)
tgt_kp, tgt_comp = get_pose_info(tgt_img)

color_sim = color_tone_similarity(ref_img, tgt_img)
clip_sim = clip_similarity(ref_path, tgt_path)
angle_diff = camera_angle_difference(ref_kp, tgt_kp)

emotion_score = round(clip_sim * 100, 2)
color_score = round(color_sim * 100, 2)

print(f"ğŸ“· [ë ˆí¼ëŸ°ìŠ¤ êµ¬ë„]: {ref_comp['score']:.2f}")
print(f"ğŸ“¸ [ë‚´ ì‚¬ì§„ êµ¬ë„]: {tgt_comp['score']:.2f}")
print(f"ğŸ¨ [ìƒ‰ê° ìœ ì‚¬ë„]: {color_score}%")
print(f"ğŸ’« [ê°ì„±(CLIP) ìœ ì‚¬ë„]: {emotion_score}%")

# ---------------------------------------------------------
# í”¼ë“œë°± ìƒì„±
# ---------------------------------------------------------
# êµ¬ì²´ì  ì´ìœ (Compositionì—ì„œ ë„˜ì–´ì˜´)
reasons = []
if not tgt_comp["on_rule_of_thirds"]:
    reasons.append("ì¸ë¬¼ì´ ì‚¼ë¶„í• ì„ ì—ì„œ ë²—ì–´ë‚˜ ìˆìŠµë‹ˆë‹¤.")
if tgt_comp["size_ratio"] > 0.45:
    reasons.append("ì¸ë¬¼ì´ í™”ë©´ì„ ê³¼ë„í•˜ê²Œ ì°¨ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•½ê°„ ë©€ë¦¬ì„œ ì´¬ì˜í•´ë³´ì„¸ìš”.")
if tgt_comp["headroom_ratio"] and tgt_comp["headroom_ratio"] < 0.05:
    reasons.append("ë¨¸ë¦¬ ìœ„ ì—¬ë°±ì´ ë„ˆë¬´ ì ì–´ìš”. ì¹´ë©”ë¼ë¥¼ ì•½ê°„ ìœ„ë¡œ ì˜¬ë ¤ë³´ì„¸ìš”.")

# ì¹´ë©”ë¼ ì•µê¸€ ì°¨ì´ ê¸°ë°˜ í”¼ë“œë°±
if angle_diff is not None:
    if angle_diff > 0.1:
        reasons.append("ì¹´ë©”ë¼ê°€ ë„ˆë¬´ ë‚®ì•„ìš”. ì•½ê°„ ë†’ì—¬ë³´ì„¸ìš”.")
    elif angle_diff < -0.1:
        reasons.append("ì¹´ë©”ë¼ê°€ ë„ˆë¬´ ë†’ì•„ìš”. ì¸ë¬¼ ëˆˆë†’ì´ì— ë§ì¶°ë³´ì„¸ìš”.")

summary = "ì „ë°˜ì ìœ¼ë¡œ ê°ì„±ì€ ë¹„ìŠ·í•˜ì§€ë§Œ êµ¬ë„ì™€ ì‹œì  ë³´ì •ì´ í•„ìš”í•´ìš”."

feedback = generate_feedback(
    pose_conf=None,
    composition_score=tgt_comp["score"],
    emotion_score=emotion_score,
    reasons=reasons,
    summary=summary
)

# ---------------------------------------------------------
# ê²°ê³¼ ì¶œë ¥
# ---------------------------------------------------------
print("\nğŸ’¬ [AI í”¼ë“œë°±]")
for line in feedback:
    print(line)

# ---------------------------------------------------------
# ì‹œê°í™”
# ---------------------------------------------------------
cv2.putText(tgt_img, f"Composition: {tgt_comp['score']:.1f}", (20,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)
cv2.putText(tgt_img, f"Emotion: {emotion_score:.1f}%", (20,80), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,0), 2)
cv2.putText(tgt_img, f"ColorTone: {color_score:.1f}%", (20,120), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,200,255), 2)
cv2.imshow("Reference", ref_img)
cv2.imshow("Your Photo (Analyzed)", tgt_img)
cv2.waitKey(0)
cv2.destroyAllWindows()
