# TryAngle v1.5 완전 설계 문서

## 📋 프로젝트 개요

### 핵심 컨셉
구도와 프레이밍에 집중한 AI 카메라 가이드 앱
- **포즈 중심 (v1)** → **구도/프레이밍 중심 (v1.5)**
- 2687장 레퍼런스 사진의 "좋은 구도" 패턴 학습
- 실시간 정량적 피드백 제공

### 핵심 혁신: 압축감 기반 거리 피드백
기존 앱들과의 차별점:
- ❌ 기존: "얼굴 중앙에", "그리드 라인"
- ✅ v1.5: **"1.2m 뒤로 또는 줌 인"** (거리 + 압축감)

---

## 🏗️ 전체 아키텍처

```
오프라인 학습 (RTX 4070 Super - 한 번만)
    ↓
레퍼런스 2687장 정밀 분석
    ↓
테마별 패턴 DB (JSON 2-5MB)
    ↓
앱에 내장
    ↓
실시간 촬영 (iPhone 15fps)
```

---

## 🔬 Phase 1: 오프라인 학습 파이프라인

### 사용 환경
- GPU: RTX 4070 Super
- 총 소요 시간: 4-6시간 (1회만)
- 결과물: 2-5MB JSON 파일

### Step 1: 테마 자동 분류

**목적:** 2687장을 테마별로 그룹화

**방법 1 (권장): Grounding DINO Rules-Based + DINOv2 Clustering**

```python
# Step 1-1: Grounding DINO로 배경 객체 검출
for image in 2687_photos:
    objects = grounding_dino(image,
        ["window", "beach", "tree", "street", "table", "snow"])

    # 규칙 기반 분류
    if "window" in objects and "table" in objects:
        theme = "cafe_indoor"
    elif "beach" in objects:
        theme = "beach"
    elif "tree" in objects and not "street":
        theme = "park_nature"
    elif "snow" in objects:
        theme = "winter_outdoor"
    # ...

# Step 1-2: DINOv2로 애매한 케이스 clustering
ambiguous_images = [img for img in images if theme == "unknown"]
embeddings = dinov2_large(ambiguous_images)  # 384D vectors
clusters = kmeans(embeddings, n_clusters=5)
# 각 클러스터 수동 라벨링

# Step 1-3: 수동 검증 (선택적)
# 샘플링으로 정확도 확인 후 필요시 수정
```

**정확도:**
- Grounding DINO Rules: 90-95%
- DINOv2 Clustering: 95-98% (수동 라벨링 후)

**결과 예시:**
- cafe_indoor: 520장
- winter_outdoor: 380장
- beach: 290장
- park_nature: 410장
- ...

**소요 시간:** 약 1-2시간 (CLIP보다 느리지만 더 정확)

---

### Step 2: 정밀 객체 검출

**목적:** 인물 + 배경 객체 정확한 위치 추출

**모델:** Grounding DINO-B (Base)

**방법:**
```python
for image in each_theme_group:
    # 인물 검출
    person = grounding_dino(image, "person")

    # 테마별 배경 객체
    if theme == "cafe":
        window = grounding_dino(image, "window")
        chair = grounding_dino(image, "chair")
        table = grounding_dino(image, "table")

    # 공간 관계 계산
    person_to_window = calculate_distance(person, window)
    window_direction = get_direction(person, window)
```

**추출 정보:**
```json
{
  "person_bbox": [250, 100, 300, 600],
  "background_objects": {
    "window": {
      "bbox": [720, 300, 300, 600],
      "direction_from_person": "right",
      "distance": 0.25
    }
  }
}
```

**소요 시간:** 약 2-3시간 (2687장 × 3초)

---

### Step 2-1: Pose Type 자동 검출 (핵심!)

**목적:** 상반신/하반신/전신 등 포즈 타입 구분

**모델:** RTMPose (133 keypoints)

**방법:**
```python
for image in 2687_photos:
    # RTMPose로 keypoints 추출
    keypoints = rtmpose(image)  # 133개 (얼굴+몸+손)

    # Confidence threshold 기반 분류
    visible_upper = check_visibility(keypoints, [
        "nose", "shoulders", "elbows"
    ], threshold=0.3)

    visible_lower = check_visibility(keypoints, [
        "hips", "knees", "ankles"
    ], threshold=0.3)

    # Pose Type 판정
    if visible_upper and not visible_lower:
        pose_type = "upper_body"  # 상반신
    elif visible_upper and visible_lower:
        ankle_in_frame = keypoints["ankle"].y < image_height
        if ankle_in_frame:
            pose_type = "full_body"  # 전신
        else:
            pose_type = "half_body"  # 반신
    elif "face" only:
        pose_type = "closeup"  # 클로즈업
    else:
        pose_type = "unknown"

    # 앉음/서있음 판정 (선택적)
    if visible_lower:
        knee_angle = calculate_angle(hip, knee, ankle)
        sitting = knee_angle < 100  # 각도로 판단
```

**Occlusion Handling (옷/가림 대응):**
```python
# 1. Confidence threshold 낮게 설정 (0.3)
# 2. 보이는 관절만으로 판단
# 3. Multi-frame averaging (실시간용)
# 4. Conservative classification
if uncertain:
    pose_type = "upper_body"  # 안전하게 상반신으로
```

**결과 예시:**
```json
{
  "filename": "IMG_1234.jpg",
  "theme": "cafe_indoor",
  "pose_type": "half_body",
  "sitting": true,
  "visible_keypoints": {
    "upper": ["nose", "shoulders", "elbows"],
    "lower": ["hips", "knees"]
  }
}
```

**중요:** RTMPose는 133개 keypoint를 제공하므로 얼굴 랜드마크까지 활용 가능!

**소요 시간:** 약 1시간 (2687장 × 1.5초)

---

### Step 3: 깊이 분석 (핵심!)

**목적:** 카메라 거리/압축감 판단

**모델:** Depth Anything V2 Large

**방법:**
```python
for image in 2687_photos:
    # Depth map 생성
    depth_map = depth_anything_v2_large(image)

    # 핵심 지표 계산
    person_depth = mean(depth_map[person_mask])
    background_depth = mean(depth_map[0:h/3, :])
    foreground_depth = mean(depth_map[3*h/4:h, :])

    # Compression Index
    total_range = background_depth - foreground_depth
    compression_index = 1.0 - (total_range / 255.0)
    # 0 = 광각 (큰 범위)
    # 1 = 망원 (작은 범위, 압축됨)
```

**주의:** 카메라 각도는 depth map이 아닌 **iPhone Gyroscope**로 감지!

**추출 정보:**
```json
{
  "depth_analysis": {
    "person_depth": 165,
    "background_depth": 220,
    "foreground_depth": 80,
    "compression_index": 0.45,
    "camera_type": "normal_to_tele"
  }
}
```

**카메라 각도 감지 (실시간만 해당):**
```swift
// iOS CMMotionManager 사용
let motionManager = CMMotionManager()
if motionManager.isDeviceMotionAvailable {
    motionManager.startDeviceMotionUpdates(to: queue) { motion, error in
        let pitch = motion.attitude.pitch * 180 / .pi
        // pitch: -90 (하향) ~ 0 (수평) ~ 90 (상향)
        // 카메라 각도: -pitch (카메라는 반대 방향)
        let cameraAngle = -pitch
    }
}
// 정확도: ±2도 (95% 신뢰도)
```

**소요 시간:** 약 1-2시간

---

### Step 4: 구도 특징 계산

**각 사진마다 계산:**

1. **위치 특징**
```python
person_center_x = (bbox.x + bbox.width/2) / image_width
person_center_y = (bbox.y + bbox.height/2) / image_height
→ (0.35, 0.42)
```

2. **크기 특징**
```python
person_size_ratio = (bbox.width * bbox.height) / (img_w * img_h)
→ 0.32
```

3. **여백 특징**
```python
left_margin = bbox.x / image_width
right_margin = (image_width - bbox.x - bbox.width) / image_width
top_margin = bbox.y / image_height
bottom_margin = (image_height - bbox.y - bbox.height) / image_height
→ (0.22, 0.46, 0.18, 0.38)
```

4. **삼분할 점수**
```python
third_points = [(1/3, 1/3), (2/3, 1/3), (1/3, 2/3), (2/3, 2/3)]
min_distance = min(euclidean(person_center, point) for point in third_points)
rule_of_thirds_score = 1.0 - (min_distance / 0.5)
→ 0.92
```

**소요 시간:** 즉시 (단순 수학)

---

### Step 5: Hierarchical Pattern 통계 계산

**핵심:** Theme + Pose Type 조합으로 세분화!

**왜?** 앉은 상반신과 서있는 전신을 섞으면 통계가 무의미해짐

**예시: cafe_indoor (520장) → 4개 sub-pattern으로 분할**

```python
cafe_indoor_images = filter_by_theme("cafe_indoor")  # 520장

# Pose Type별 분할
patterns = {
    "cafe_indoor_closeup": [],      # 50장
    "cafe_indoor_upper_body": [],   # 120장
    "cafe_indoor_half_body": [],    # 280장 (가장 많음!)
    "cafe_indoor_full_body": []     # 70장
}

for img in cafe_indoor_images:
    key = f"{img.theme}_{img.pose_type}"
    patterns[key].append(img)

# 각 sub-pattern별로 통계 계산
for pattern_name, images in patterns.items():
    if len(images) < 20:  # 너무 적으면 skip
        continue

    stats = calculate_statistics(images)
    save_pattern(pattern_name, stats)
```

**결과: cafe_indoor_half_body (280장)**

```json
{
  "theme": "cafe_indoor",
  "pose_type": "half_body",
  "sample_count": 280,

  "position": {
    "mean": [0.35, 0.42],
    "std": [0.08, 0.10],
    "optimal_range": {
      "x": [0.25, 0.45],
      "y": [0.30, 0.55]
    }
  },

  "size": {
    "mean": 0.32,
    "std": 0.06,
    "optimal_range": [0.28, 0.38]
  },

  "margins": {
    "left": {"mean": 0.22, "std": 0.05},
    "right": {"mean": 0.46, "std": 0.08},
    "asymmetry": "right_heavy"
  },

  "camera": {
    "compression_index": {
      "mean": 0.45,
      "std": 0.12
    },
    "type": "normal_to_tele",
    "focal_length_estimate": "50-70mm",
    "angle": {
      "mean": 12,
      "std": 5,
      "range": [5, 20]
    }
  },

  "pose_requirements": {
    "sitting": true,
    "visible_joints": ["shoulders", "elbows", "hips", "knees"]
  },

  "background": {
    "window": {
      "presence_rate": 0.78,
      "typical_direction": "right",
      "distance_from_person": {"mean": 0.25}
    }
  }
}
```

**중요:** 이제 "cafe 반신 앉은 사진"만 비교하므로 통계가 의미있음!

---

## 💾 패턴 DB 구조 (JSON)

**Hierarchical Structure: theme → pose_type → sub-patterns**

```json
{
  "cafe_indoor": {
    "theme_description": "카페 실내",
    "total_samples": 520,

    "sub_patterns": {
      "cafe_indoor_half_body": {
        "theme": "cafe_indoor",
        "pose_type": "half_body",
        "sample_count": 280,
        "description": "카페 반신 (가장 인기)",

        "composition": {
          "position": {
            "mean": [0.35, 0.42],
            "std": [0.08, 0.10],
            "acceptable_range": {
              "x": [0.25, 0.45],
              "y": [0.30, 0.55]
            }
          },
          "size": {
            "mean": 0.32,
            "optimal_range": [0.28, 0.38]
          },
          "margins": {
            "left": {"mean": 0.22, "std": 0.05},
            "right": {"mean": 0.46, "std": 0.08},
            "top": {"mean": 0.18, "std": 0.06},
            "bottom": {"mean": 0.38, "std": 0.10}
          }
        },

        "camera": {
          "angle": {
            "mean": 12,
            "std": 5,
            "range": [5, 20],
            "interpretation": "slight_high_angle"
          },
          "compression_index": {
            "mean": 0.45,
            "std": 0.12,
            "range": [0.30, 0.65]
          },
          "type": "normal_to_tele",
          "focal_length_estimate": "50-70mm"
        },

        "pose_requirements": {
          "sitting": true,
          "visible_joints": ["shoulders", "elbows", "hips", "knees"],
          "min_confidence": 0.3
        },

        "background": {
          "required_objects": ["window"],
          "common_objects": ["chair", "table", "plant"],
          "window": {
            "presence_rate": 0.78,
            "typical_direction": "right",
            "distance_from_person": {"mean": 0.25, "std": 0.08}
          }
        },

        "scoring_weights": {
          "position_match": 0.20,
          "size_match": 0.15,
          "margin_match": 0.15,
          "compression_match": 0.20,
          "angle_match": 0.15,
          "pose_match": 0.15
        },

        "exemplars": [
          {"filename": "IMG_1234.jpg", "score": 98},
          {"filename": "IMG_2345.jpg", "score": 96}
        ]
      },

      "cafe_indoor_upper_body": {
        "theme": "cafe_indoor",
        "pose_type": "upper_body",
        "sample_count": 120,
        "description": "카페 상반신 클로즈업",

        "composition": {
          "position": {
            "mean": [0.48, 0.38],
            "std": [0.06, 0.08]
          }
        }
      }
    }
  },

  "park_nature": {
    "theme_description": "공원/자연",
    "total_samples": 410,

    "sub_patterns": {
      "park_full_body": {...},
      "park_half_body": {...}
    }
  }
}
```

**핵심 개선:**
- ✅ Theme + Pose Type 조합으로 정밀 분류
- ✅ 앉음/서있음 구분
- ✅ 카메라 각도 포함 (gyroscope 기반)
- ✅ Pose requirements 명시

---

## 📱 Phase 2: 실시간 피드백 시스템

### 실시간 파이프라인 (iPhone)

**목표 성능:** 15fps (66ms/frame)

**주의:** RTMPose = YOLOX (detector) + RTMPose (pose estimator)

```
프레임 캡처
  ↓
YOLOX person bbox (5ms)
  ↓
RTMPose keypoints (5ms)
  ↓
Pose Type 판정 (1ms)
  ↓
Gyroscope 각도 읽기 (즉시, 백그라운드)
  ↓
MiDaS Small depth 추정 (50ms)
  ↓
구도 특징 계산 (2ms)
  ↓
패턴 매칭 (현재 pose_type에 맞는 패턴과 비교, 3ms)
  ↓
3단계 피드백 생성 (1ms)
  ↓
UI 업데이트

총: 67ms → 15fps ✅
```

**RTMPose 상세:**
```swift
// RTMPoseRunner.swift 이미 구현됨
let result = rtmPoseRunner.run(image)

// result.boundingBox: YOLOX가 검출한 person bbox
// result.keypoints: RTMPose가 추정한 133개 keypoints
// result.confidences: 각 keypoint 신뢰도

// Pose Type 판정
let poseType = determinePoseType(
    keypoints: result.keypoints,
    confidences: result.confidences
)
// → "half_body", "upper_body", "full_body", "closeup"
```

### 3단계 피드백 시스템

#### Tier 1: 위치 피드백 (즉각)
```
current.position.x < ideal.position.x - 0.05
→ "← 왼쪽으로"

current.position.x > ideal.position.x + 0.05
→ "→ 오른쪽으로"

else
→ "✓ 위치 완벽"
```

#### Tier 2: 거리 피드백 (핵심!)
```
compression_diff = current.compression - ideal.compression

if compression_diff > 0.15:  // 너무 망원
    distance = estimate_distance(compression_diff)
    → "📏 {distance}m 더 가까이"
    → "또는 줌 아웃"

elif compression_diff < -0.15:  // 너무 광각
    distance = estimate_distance(compression_diff)
    → "📏 {distance}m 뒤로"
    → "또는 줌 인"

else:
    → "✓ 거리 완벽"
```

#### Tier 3: 종합 점수
```
overall_score = weighted_average(
    position_match,
    size_match,
    margin_match,
    compression_match
)

if score >= 95:
    → "🎉 완벽! 촬영하세요"
    + 햅틱 피드백
    + 초록색 테두리

elif score >= 85:
    → "👍 훌륭합니다 ({score}/100)"

else:
    → "조정 필요 ({score}/100)"
```

---

## 🎨 UI 레이아웃

```
┌─────────────────────────────────┐
│                                 │
│    [카메라 뷰파인더]            │
│                                 │
│         ┌─────────┐             │
│         │   👤    │             │ ← YOLO bbox 오버레이
│         └─────────┘             │
│                                 │
│    ┌─ ─ ─ ─ ─ ─ ─┐            │ ← 이상적 위치 (점선)
│                                 │
├─────────────────────────────────┤
│  ← 왼쪽으로                     │ ← Tier 1
│  📏 1.2m 뒤로 또는 줌 인        │ ← Tier 2 (핵심!)
│  ⭐ 구도 점수: 87/100          │ ← Tier 3
│                                 │
│  [카페 창가 패턴]               │
│  [대표 예시 보기]               │
└─────────────────────────────────┘
```

---

## 🎯 핵심 차별점

### 압축감 기반 거리 피드백 (세계 최초)

**문제:**
```
사진 A: 가까이서 광각 (24mm)
사진 B: 멀리서 망원 (85mm)

→ 두 사진 모두 인물 크기 같을 수 있음
→ 하지만 완전히 다른 느낌!
```

**해결:**
```
Depth Map으로 압축감 측정
→ "1.2m 뒤로" 정량적 피드백
→ "인스타 느낌"의 핵심 재현
```

### 기존 앱 vs TryAngle v1.5

| 기능 | 기존 앱 | TryAngle v1.5 |
|------|---------|---------------|
| 위치 가이드 | "그리드 라인" | "왼쪽으로 10cm" |
| 거리 가이드 | ❌ 없음 | "1.2m 뒤로 또는 줌 인" ⭐ |
| 점수 | ❌ 없음 | "87/100, 3점만 더" |
| 테마 | ❌ 없음 | "카페 창가 패턴 93% 일치" |
| 피드백 | 시각적만 | 햅틱 + 음성 + 시각 |

---

## 📊 예상 성능

### 오프라인 학습 (RTX 4070 Super)
```
- CLIP 테마 분류: 30분
- Grounding DINO 객체 검출: 2-3시간
- Depth Anything V2 깊이 분석: 1-2시간
- 통계 계산: 10분

총: 4-6시간 (1회만)
결과물: 2-5MB JSON
```

### 실시간 성능 (iPhone 15 Pro)
```
- YOLO bbox: 10ms
- MiDaS Small: 50ms
- 계산 + 매칭: 5ms
- 피드백: 1ms

총: 66ms → 15fps ✅

배터리: 중간 (MiDaS 주요 소모)
발열: 낮음
```

---

## 🎯 MVP Strategy (추천!)

### 2687장 전체 vs 300장 샘플

**문제:** 2687장 전체 학습은 시간 소요 (4-6시간) + 검증 어려움

**해결:** MVP로 300장 샘플부터 시작!

### MVP 구성

**테마 선택 (2-3개):**
```
1. cafe_indoor - 150장
   - 가장 인기있는 테마
   - 창가, 테이블 등 명확한 배경 객체

2. park_nature - 100장
   - 야외 전신 사진
   - 다양한 pose type 테스트

3. (선택) beach - 50장
   - 특수 케이스
```

**Pose Type 분포:**
```
cafe_indoor 150장:
- half_body: 80장 (가장 많음)
- upper_body: 40장
- full_body: 20장
- closeup: 10장

park_nature 100장:
- full_body: 60장
- half_body: 30장
- upper_body: 10장
```

### MVP 장점

1. **빠른 iteration**
   - 학습 시간: 30분 (vs 4-6시간)
   - 검증 쉬움
   - 문제 발견 빠름

2. **충분한 샘플 수**
   - cafe_half_body: 80장 → 통계적으로 충분
   - 패턴 추출 가능

3. **점진적 확장**
   - MVP 검증 후
   - 테마 추가 쉬움
   - JSON만 업데이트

### MVP 실행 계획

```python
# Step 1: 수동으로 300장 선별
selected_images = {
    "cafe_indoor": select_best_samples("cafe", 150),
    "park_nature": select_best_samples("park", 100),
    "beach": select_best_samples("beach", 50)
}

# Step 2: MVP 학습 (30분)
for theme, images in selected_images.items():
    # Grounding DINO
    extract_objects(images)

    # RTMPose
    extract_pose_types(images)

    # Depth Anything V2
    extract_depth(images)

    # 통계 계산
    calculate_patterns(theme)

# Step 3: JSON 생성
save_mvp_patterns("patterns_mvp_v1.json")  # ~500KB

# Step 4: 앱 테스트
# cafe에서 실제 촬영하며 피드백 검증

# Step 5: 검증 후 전체 확장
if mvp_validated:
    expand_to_full_dataset(2687)
```

### 검증 기준

```
✅ cafe_half_body 패턴 매칭 정확도 > 85%
✅ 위치 피드백 실시간 동작 (15fps)
✅ 압축감 거리 피드백 체감 가능
✅ Pose Type 자동 판정 정확도 > 90%
✅ 사용자 테스트 긍정적 반응

→ 검증 완료 시 전체 2687장으로 확장!
```

---

## 🚀 개발 로드맵 (MVP 우선)

### Phase 0: MVP 데이터 준비 (3일)
```
📊 300장 이미지 선별 (cafe 150, park 100, beach 50)
🔧 RTX 4070S 환경 세팅
   - Grounding DINO 설치
   - Depth Anything V2 설치
   - RTMPose 설치
📈 MVP 학습 파이프라인 실행 (30분)
💾 patterns_mvp_v1.json 생성 (~500KB)

→ 테스트용 패턴 DB 완성
```

### Phase 1: 실시간 피드백 구현 (1주)
```
✅ RTMPose bbox/keypoints 활용 (이미 있음)
✅ Pose Type 자동 판정 구현
✅ Gyroscope 각도 감지 추가
✅ 위치/크기/여백 계산
✅ 패턴 매칭 (MVP 패턴 기준)
✅ Tier 1 피드백 (위치)

→ 기본 피드백 동작
```

### Phase 2: 압축감 거리 피드백 (1주)
```
⭐ MiDaS Small 온디바이스 통합
⭐ Compression Index 계산
⭐ 거리 추정 알고리즘
⭐ Tier 2 피드백 (거리)
⭐ Tier 3 피드백 (종합 점수)

→ 핵심 차별점 완성
```

### Phase 3: MVP 검증 (3일)
```
🧪 Cafe에서 실제 촬영 테스트
📊 정확도 측정 (목표: 85%+)
👥 사용자 피드백 수집
🐛 버그 수정

→ MVP 검증 완료
```

### Phase 4: 전체 데이터 확장 (선택적)
```
📊 2687장 전체 학습 (4-6시간)
💾 patterns_full_v1.json 생성 (~2-5MB)
🔄 앱에 패턴 DB 업데이트
✨ 테마 자동 전환 추가

→ 완전한 v1.5
```

### Phase 5: Polish (1주)
```
✨ UI/UX 개선
✨ 햅틱 피드백
✨ 예시 사진 보기 기능
🎨 아이콘/그래픽
📝 튜토리얼

→ 출시 준비
```

**총 개발 기간:** 3-4주 (MVP) / 5-6주 (Full)

---

## 🔑 핵심 인사이트

### 1. Hierarchical Pattern의 중요성
```
❌ 잘못된 방식:
   cafe_indoor 전체 평균 (앉은 반신 + 서있는 전신 섞임)
   → 통계가 무의미!

✅ 올바른 방식:
   cafe_indoor_half_body (앉은 반신만)
   cafe_indoor_full_body (서있는 전신만)
   → 각각 의미있는 통계!

핵심: Theme + Pose Type 조합이 필수!
```

### 2. RTMPose vs YOLO
```
현재 앱 구조 (RTMPoseRunner.swift):
1. YOLOX → Person bbox 검출 (빠름)
2. RTMPose → 133개 keypoints (정밀)

왜 둘 다 필요?
- YOLOX: 빠른 person detection
- RTMPose: 얼굴 랜드마크 포함 정밀 pose

v1.5에서:
- YOLOX bbox → 구도 분석 (위치, 크기, 여백)
- RTMPose keypoints → Pose Type 판정 (half/full/upper)
```

### 3. Occlusion Handling (가림 대응)
```
문제: 옷/테이블로 관절 가려짐

해결 전략:
1. Confidence threshold 낮게 (0.3)
2. 보이는 관절만으로 판단
3. Multi-frame averaging (실시간)
4. Conservative classification
   예: 불확실하면 "upper_body"

RTMPose 장점:
- 133개 keypoint → 얼굴 랜드마크 활용 가능
- Confidence per keypoint
- Robust to partial occlusion
```

### 4. Grounding DINO의 역할
```
✅ 할 수 있는 것:
   - 객관적 측정 (어디에 뭐가 있는지)
   - 인물 + 배경 객체 위치
   - 테마 분류 (rules-based)

❌ 할 수 없는 것:
   - 주관적 판단 (좋은 구도인지)
   - 여백이 적절한지

→ 오프라인 학습에만 사용!
```

### 5. 여백 판단의 원리
```
Grounding DINO = 측정 도구
통계 분석 = 패턴 발견

"좋은 사진" 500장 측정
→ 평균 여백 22%-46%
→ 이걸 "이상적 여백"으로 사용

→ 귀납적 추론!
```

### 6. 압축감 측정
```
EXIF Focal Length: 편집된 사진엔 없음 ❌
Depth Map: 항상 계산 가능 ✅

Compression Index:
- 0 = 광각 (큰 depth 범위)
- 1 = 망원 (작은 depth 범위)

→ "가까이 vs 멀리+줌" 구분 가능!
```

### 7. 카메라 각도 감지
```
❌ Depth Map Gradient: 부정확
✅ iPhone Gyroscope: 95% 정확도

CMMotionManager:
- pitch: 카메라 상하 각도
- roll: 카메라 좌우 기울기
- 실시간, 배터리 영향 거의 없음

→ 레퍼런스 사진의 angle과 비교!
```

---

## 💡 왜 이 방식이 똑똑한가?

### 장점

1. **Grounding DINO 강점 활용**
   - 느리지만 정밀 → 오프라인 학습에
   - 배경 객체 인식 → 테마 분류에

2. **실시간은 경량 모델**
   - YOLO + MiDaS Small
   - 미리 학습한 패턴 활용

3. **사용자는 차이 못 느낌**
   - 피드백 즉각적 (66ms)
   - 정확도 85-90%면 충분
   - 네트워크 불필요

4. **확장 가능**
   - 새 테마 추가 쉬움
   - 모델 업데이트 독립적
   - JSON만 교체

---

## 📝 다음 단계

1. RTX 4070 Super 환경 세팅
2. Grounding DINO + Depth Anything V2 설치
3. 2687장 학습 파이프라인 구축
4. JSON DB 생성
5. iOS 앱 통합

---

**최초 작성일:** 2025-11-30
**최종 업데이트:** 2025-11-30
**브랜치:** v1.5_ios
**버전:** 1.1

## 📝 변경 이력

### v1.1 (2025-11-30)
- ✅ CLIP → Grounding DINO rules-based + DINOv2 clustering으로 변경
- ✅ Hierarchical Pattern 구조 추가 (theme + pose_type)
- ✅ RTMPose 133 keypoints 기반 Pose Type 자동 검출 추가
- ✅ Occlusion Handling 전략 추가
- ✅ Gyroscope 각도 감지 (depth gradient 제거)
- ✅ MVP Strategy 추가 (300장 샘플)
- ✅ 개발 로드맵 MVP 기반으로 재구성
- ✅ RTMPose vs YOLO 명확화

### v1.0 (2025-11-30)
- 최초 문서 작성
