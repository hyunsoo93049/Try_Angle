# -*- coding: utf-8 -*-
"""
TryAngle í†µí•© ë¹„êµ (CLIP + YOLO Pose + Tri-Path DINO + MiDaS + ColorTone)
ì•ˆì •í™” + OpenCV ì‹œê°í™” ë²„ì „ v2.6
- ì›ë³¸ ë¹„ìœ¨ 100% ìœ ì§€
- ë³„ë„ ì°½ìœ¼ë¡œ í‘œì‹œ
- í¬ì¦ˆ ìœ ì‚¬ë„ ì¶”ê°€
"""

import cv2
import numpy as np
import torch
from ultralytics import YOLO
from composition_module import analyze_composition, calculate_pose_similarity
from feedback_module import generate_feedback
from emotion_module import EmotionAnalyzer
from dino_module import dino_similarity
from midas_module import camera_height_diff
import os


# ---------------------------------------------------------
# ì•ˆì „ ë¦¬ì‚¬ì´ì¦ˆ (ê¸´ ë³€ ê¸°ì¤€ 1200px ì´í•˜ë¡œ ì œí•œ)
# ---------------------------------------------------------
MAX_SIZE = 1200  # í™”ë©´ì— ë§ê²Œ ì¡°ì •
def safe_resize(img, max_size=MAX_SIZE):
    """ì´ë¯¸ì§€ í¬ê¸°ê°€ ë„ˆë¬´ í¬ë©´ ë¹„ìœ¨ ìœ ì§€í•˜ë©° ì¶•ì†Œ"""
    h, w = img.shape[:2]
    scale = max_size / max(h, w)
    if scale < 1.0:
        img = cv2.resize(img, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_AREA)
    return img


# ---------------------------------------------------------
# ìƒ‰ê°/ì¡°ëª… ìœ ì‚¬ë„ (HSV íˆìŠ¤í† ê·¸ë¨ ìƒê´€)
# ---------------------------------------------------------
def color_tone_similarity(img1_bgr, img2_bgr):
    hsv1 = cv2.cvtColor(img1_bgr, cv2.COLOR_BGR2HSV)
    hsv2 = cv2.cvtColor(img2_bgr, cv2.COLOR_BGR2HSV)
    hist1 = cv2.calcHist([hsv1],[0,1,2],None,[24,8,8],[0,180,0,256,0,256])
    hist2 = cv2.calcHist([hsv2],[0,1,2],None,[24,8,8],[0,180,0,256,0,256])
    cv2.normalize(hist1, hist1)
    cv2.normalize(hist2, hist2)
    sim = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
    return float((sim + 1.0) * 0.5)


# ---------------------------------------------------------
# YOLO Pose: í‚¤í¬ì¸íŠ¸ & bbox ì¶”ì¶œ
# ---------------------------------------------------------
def extract_pose_info(image_bgr, yolo_model):
    """
    YOLO Pose ê²°ê³¼ì—ì„œ ì¸ë¬¼ bbox, keypoints ì¶”ì¶œ ë° composition ë¶„ì„ ìˆ˜í–‰
    """
    res = yolo_model(image_bgr, verbose=False)
    if not res or res[0].keypoints is None:
        return None, None, None

    r = res[0]
    boxes = r.boxes.xyxy
    kpts = r.keypoints.xy

    if boxes is None or len(boxes) == 0:
        return None, None, None

    bbox = boxes[0].detach().cpu().numpy()
    x1, y1, x2, y2 = bbox

    # ìƒë‹¨ margin ì¶”ê°€ (ë¨¸ë¦¬ ì§¤ë¦¼ ë°©ì§€)
    margin_y = int((y2 - y1) * 0.15)
    y1 = max(0, y1 - margin_y)
    y2 = y2 + margin_y
    if y2 < y1:
        y1, y2 = y2, y1
    bbox = np.array([x1, y1, x2, y2])

    kpts = kpts[0].detach().cpu().numpy() if kpts is not None and len(kpts) > 0 else None

    comp = analyze_composition(image_bgr, kpts, bbox)
    return kpts, comp, bbox


# ---------------------------------------------------------
# ë©”ì¸ í•¨ìˆ˜
# ---------------------------------------------------------
def main(ref_path, tgt_path, yolo_weights="yolov8s-pose.pt"):
    ref_img = safe_resize(cv2.imread(ref_path))
    tgt_img = safe_resize(cv2.imread(tgt_path))
    if ref_img is None or tgt_img is None:
        raise FileNotFoundError("ì´ë¯¸ì§€ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("ğŸ”§ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    yolo_pose = YOLO(yolo_weights)
    emo = EmotionAnalyzer(device=device)

    # 1ï¸âƒ£ YOLO Pose
    ref_kp, ref_comp, ref_bbox = extract_pose_info(ref_img, yolo_pose)
    tgt_kp, tgt_comp, tgt_bbox = extract_pose_info(tgt_img, yolo_pose)

    # í¬ì¦ˆ ìœ ì‚¬ë„ ê³„ì‚°
    pose_similarity = calculate_pose_similarity(ref_kp, tgt_kp)

    # 2ï¸âƒ£ CLIP ê°ì„±
    clip_score = emo.compare_to_reference(ref_path, tgt_img)

    # 3ï¸âƒ£ Tri-Path DINO
    try:
        dino_sim = dino_similarity(ref_path, tgt_path, ref_bbox, tgt_bbox, alpha=0.5, beta=0.3, gamma=0.2)
    except Exception as e:
        print(f"[ê²½ê³ ] DINO ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        dino_sim = None

    # 4ï¸âƒ£ MiDaS
    try:
        height_diff = camera_height_diff(ref_path, tgt_path)
    except Exception as e:
        print(f"[ê²½ê³ ] MiDaS ì‹œì  ê³„ì‚° ì‹¤íŒ¨: {e}")
        height_diff = None

    # 5ï¸âƒ£ ìƒ‰ê°
    color_sim = color_tone_similarity(ref_img, tgt_img)

    # -----------------------------------------------------
    # ê²°ê³¼ ì¶œë ¥
    # -----------------------------------------------------
    print("\n===== ANALYSIS =====")
    if ref_comp: print(f"ğŸ“· [ë ˆí¼ëŸ°ìŠ¤ êµ¬ë„]: {ref_comp['score']:.2f}")
    if tgt_comp: print(f"ğŸ“¸ [ë‚´ ì‚¬ì§„ êµ¬ë„]: {tgt_comp['score']:.2f}")
    print(f"ğŸ¨ [ìƒ‰ê° ìœ ì‚¬ë„]: {color_sim*100:.2f}%")
    print(f"ğŸ’« [ê°ì„±(CLIP) ìœ ì‚¬ë„]: {clip_score:.2f}%")
    print(f"ğŸ•º [í¬ì¦ˆ ìœ ì‚¬ë„]: {pose_similarity['score']:.2f} ({pose_similarity['details']})")
    if dino_sim is not None:
        print(f"ğŸ§© [DINO êµ¬ë„ ìœ ì‚¬ë„ (Tri-Path)]: {dino_sim:.3f}")
    if height_diff is not None:
        trend = "í•˜ì´ì•µê¸€â†‘" if height_diff > 0 else ("ë¡œìš°ì•µê¸€â†“" if height_diff < 0 else "ìœ ì‚¬")
        print(f"ğŸ“ [MiDaS ì‹œì  ì°¨ì´]: {height_diff:.3f}  â†’ {trend}")

    # -----------------------------------------------------
    # í”¼ë“œë°± ìƒì„±
    # -----------------------------------------------------
    reasons = []
    if tgt_comp:
        if not tgt_comp["on_rule_of_thirds"]:
            reasons.append("ì¸ë¬¼ì´ ì‚¼ë¶„í• ì„ ì—ì„œ ë²—ì–´ë‚˜ ìˆìŠµë‹ˆë‹¤.")
        if tgt_comp["size_ratio"] > 0.45:
            reasons.append("ì¸ë¬¼ì´ í™”ë©´ì„ ê³¼ë„í•˜ê²Œ ì°¨ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        if tgt_comp["headroom_ratio"] < 0.08:
            reasons.append("ë¨¸ë¦¬ ìœ„ ì—¬ë°±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        if tgt_comp["headroom_ratio"] > 0.18:
            reasons.append("ë¨¸ë¦¬ ìœ„ ì—¬ë°±ì´ ë„“ì–´ ì¸ë¬¼ì´ ì‘ê²Œ ëŠê»´ì§‘ë‹ˆë‹¤.")

    summary = "ê°ì„±Â·í”„ë ˆì„Â·ì‹œì Â·í¬ì¦ˆë¥¼ ì¢…í•© ë¹„êµí–ˆìŠµë‹ˆë‹¤."
    extras = {
        "size_ratio": tgt_comp.get("size_ratio") if tgt_comp else None,
        "headroom_ratio": tgt_comp.get("headroom_ratio") if tgt_comp else None,
        "dino_sim": dino_sim,
        "height_diff": height_diff,
        "color_sim": color_sim,
        "pose_similarity": pose_similarity,
    }
    comp_score = tgt_comp["score"] if tgt_comp else None

    feedback = generate_feedback(
        pose_conf=None,
        composition_score=comp_score,
        emotion_score=clip_score,
        reasons=reasons,
        summary=summary,
        extras=extras
    )

    print("\nğŸ’¬ [AI í”¼ë“œë°±]")
    for line in feedback:
        print(line)

    # -----------------------------------------------------
    # ì‹œê°í™” (OpenCV - ì›ë³¸ ë¹„ìœ¨ 100% ìœ ì§€)
    # -----------------------------------------------------
    
    # íƒ€ê²Ÿ ì´ë¯¸ì§€ì— ì •ë³´ ì˜¤ë²„ë ˆì´
    overlay = tgt_img.copy()
    y = 30
    line_height = 35
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.7
    thickness = 2
    
    # í…ìŠ¤íŠ¸ ë°°ê²½ ì¶”ê°€ í•¨ìˆ˜
    def add_text_with_bg(img, text, pos, color, bg_color=(0, 0, 0)):
        (w, h), _ = cv2.getTextSize(text, font, font_scale, thickness)
        x, y_pos = pos
        cv2.rectangle(img, (x, y_pos - h - 5), (x + w + 5, y_pos + 5), bg_color, -1)
        cv2.putText(img, text, (x, y_pos), font, font_scale, color, thickness)
    
    # ê° ì ìˆ˜ í‘œì‹œ
    if comp_score:
        add_text_with_bg(overlay, f"Composition: {comp_score:.1f}", (10, y), (0, 255, 0))
        y += line_height
    
    # í¬ì¦ˆ ìœ ì‚¬ë„ (ìƒ‰ìƒ ì½”ë“œ)
    if pose_similarity:
        pose_score = pose_similarity.get("score", 0.0)
        if pose_score >= 70:
            pose_color = (0, 255, 0)  # ì´ˆë¡
        elif pose_score >= 50:
            pose_color = (0, 165, 255)  # ì£¼í™©
        else:
            pose_color = (0, 0, 255)  # ë¹¨ê°•
        add_text_with_bg(overlay, f"Pose: {pose_score:.1f}", (10, y), pose_color)
        y += line_height
    
    add_text_with_bg(overlay, f"Emotion(CLIP): {clip_score:.1f}%", (10, y), (255, 255, 0))
    y += line_height
    
    add_text_with_bg(overlay, f"ColorTone: {color_sim*100:.1f}%", (10, y), (0, 200, 255))
    y += line_height
    
    if dino_sim is not None:
        add_text_with_bg(overlay, f"DINO TriPath: {dino_sim:.3f}", (10, y), (255, 128, 0))
        y += line_height
    
    if height_diff is not None:
        angle_text = "High" if height_diff > 0.12 else ("Low" if height_diff < -0.12 else "Eye")
        add_text_with_bg(overlay, f"MiDaS: {height_diff:.3f} ({angle_text})", (10, y), (200, 200, 255))
        y += line_height
    
    # YOLO bbox í‘œì‹œ
    if tgt_bbox is not None:
        x1, y1, x2, y2 = map(int, tgt_bbox)
        cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 255, 0), 3)
        cv2.putText(overlay, "Person", (x1, y1 - 10), font, 0.6, (0, 255, 0), 2)
    
    # ë³„ë„ ì°½ìœ¼ë¡œ í‘œì‹œ (WINDOW_AUTOSIZE = ë¹„ìœ¨ ìœ ì§€)
    cv2.namedWindow("Reference Image", cv2.WINDOW_AUTOSIZE)
    cv2.imshow("Reference Image", ref_img)
    
    cv2.namedWindow("Your Photo (Analyzed)", cv2.WINDOW_AUTOSIZE)
    cv2.imshow("Your Photo (Analyzed)", overlay)
    
    # ì°½ ìœ„ì¹˜ ì¡°ì • (ì™¼ìª½/ì˜¤ë¥¸ìª½)
    cv2.moveWindow("Reference Image", 50, 50)
    ref_w = ref_img.shape[1]
    cv2.moveWindow("Your Photo (Analyzed)", 50 + ref_w + 30, 50)
    
    print("\nâœ… ì´ë¯¸ì§€ê°€ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë¬´ í‚¤ë‚˜ ëˆŒëŸ¬ ì¢…ë£Œí•˜ì„¸ìš”...")
    cv2.waitKey(0)
    cv2.destroyAllWindows()


# ---------------------------------------------------------
# ì‹¤í–‰
# ---------------------------------------------------------
if __name__ == "__main__":
    default_ref = "C:/try_angle/data/sample_images/1.jpg"
    default_tgt = "C:/try_angle/data/sample_images/2.jpg"
    
    if not os.path.exists(default_ref) or not os.path.exists(default_tgt):
        print("âš ï¸ ê¸°ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ í™•ì¸ í•„ìš”.")
        print(f"   ë ˆí¼ëŸ°ìŠ¤: {default_ref}")
        print(f"   íƒ€ê²Ÿ: {default_tgt}")
    else:
        main(default_ref, default_tgt)
        