"""
precompute_reference_features.py
- ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ì˜ íŠ¹ì„±ì„ 'í•œ ë²ˆë§Œ' ê³„ì‚°í•´ ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
- ì €ì¥ë¬¼: .npz (CLIP/DINO ì„ë² ë”©, MiDaS depth ìš”ì•½, ì„ íƒ: êµ¬ë„ìš”ì•½)
- ì¶”í›„ ì‹¤ì‹œê°„ ë¹„êµ ë‹¨ê³„ì—ì„œëŠ” ì´ .npzë¥¼ ë¶ˆëŸ¬ 'ë¹„êµë§Œ' ìˆ˜í–‰ â†’ í”„ë ˆì„ ëŠê¹€ ìµœì†Œí™”

[ê¶Œì¥ ì„¤ì¹˜]
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install opencv-python pillow numpy
pip install git+https://github.com/openai/CLIP.git
pip install timm
# (ì„ íƒ) YOLO í¬ì¦ˆ & êµ¬ë„ìš”ì•½ìš©
pip install ultralytics
# (ì„ íƒ) MiDaS(ê¹Šì´) ëª¨ë¸ ê°€ì¤‘ì¹˜ ìë™ ë‹¤ìš´ë¡œë“œ
# torch.hubê°€ ì¸í„°ë„·ì—ì„œ weightsë¥¼ ë°›ìŠµë‹ˆë‹¤.
"""

import os
import json
import time
import argparse
from typing import Optional, Dict, Any, Tuple

import cv2
import numpy as np
import torch
from PIL import Image

# --------- CLIP ----------
import clip

# --------- DINO(v2 via timm pooling ëŒ€ì²´) ----------
import timm
import torch.nn.functional as F

# --------- (ì„ íƒ) YOLO pose + êµ¬ë„ìš”ì•½ ----------
try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except Exception:
    YOLO_AVAILABLE = False

# --------- (ì„ íƒ) MiDaS depth ----------
# torch.hubì—ì„œ ëª¨ë¸ì„ ë°›ì•„ì˜µë‹ˆë‹¤(ìµœì´ˆ 1íšŒ ì¸í„°ë„· í•„ìš”)
def _load_midas(device: str):
    midas = torch.hub.load("intel-isl/MiDaS", "DPT_Hybrid")  # ì†ë„/ì •í™•ë„ ê· í˜•
    midas.to(device)
    midas.eval()
    midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")
    transform = midas_transforms.dpt_transform
    return midas, transform

# --------- ìœ í‹¸ ----------
def _ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def _to_tensor(img_bgr: np.ndarray) -> Image.Image:
    return Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))

def _l2norm(x: torch.Tensor) -> torch.Tensor:
    return F.normalize(x, dim=-1)

# --------- CLIP ----------
def compute_clip_embedding(model, preprocess, device, img_bgr: np.ndarray) -> np.ndarray:
    pil = _to_tensor(img_bgr)
    with torch.no_grad():
        image = preprocess(pil).unsqueeze(0).to(device)
        feat = model.encode_image(image)
        feat = _l2norm(feat)
    return feat.squeeze(0).float().cpu().numpy()

# --------- DINO ----------
def load_dino_model(device: str):
    """
    timmì˜ DINO ê³„ì—´ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  GAPë¡œ ì „ì—­ ì„ë² ë”©ì„ ë§Œë“­ë‹ˆë‹¤.
    - ëŒ€ì•ˆ: facebookresearch/dinov2 hub ì‚¬ìš© ê°€ëŠ¥(ì¸í„°ë„· í•„ìš”)
    """
    model_name = "vit_small_patch14_dinov2"  # ê°€ë²¼ìš´ í¸
    model = timm.create_model(model_name, pretrained=True)
    model.eval().to(device)
    # íŠ¹ì§• ì¶”ì¶œìš©: ë§ˆì§€ë§‰ ë¶„ë¥˜ê¸° ì „ì— global average pooling ì‚¬ìš©
    return model, model_name

def compute_dino_embedding(dino_model, device, img_bgr: np.ndarray) -> np.ndarray:
    # timm ê¸°ë³¸ ì „ì²˜ë¦¬
    cfg = timm.data.resolve_model_data_config(dino_model)
    tfm = timm.data.create_transform(**cfg, is_training=False)
    img = _to_tensor(img_bgr)
    x = tfm(img).unsqueeze(0).to(device)
    with torch.no_grad():
        feats = dino_model.forward_features(x)
        # timm vit forward_features ê²°ê³¼ì— ë”°ë¼ í‚¤ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ â†’ í’€ë§ ì²˜ë¦¬
        if isinstance(feats, dict) and "x_norm_clstoken" in feats:
            vec = feats["x_norm_clstoken"]  # [B, C]
        elif isinstance(feats, dict) and "pool" in feats:
            vec = feats["pool"]             # [B, C]
        else:
            # ìµœí›„ì˜ ë³´ë£¨: GAP
            if isinstance(feats, (list, tuple)):
                feats = feats[-1]
            vec = feats.mean(dim=(2, 3)) if feats.dim() == 4 else feats
    vec = _l2norm(vec)
    return vec.squeeze(0).float().cpu().numpy()

# --------- YOLO Pose & êµ¬ë„ìš”ì•½(ì„ íƒ) ----------
def summarize_composition_with_yolo(image_bgr: np.ndarray) -> Dict[str, Any]:
    """
    YOLOv8 poseë¡œ ì‚¬ëŒ keypointsì™€ bboxë¥¼ ì–»ê³ ,
    ê°„ë‹¨í•œ êµ¬ë„ ì§€í‘œë¥¼ ìš”ì•½í•´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
    (ë¶„ì„ ì†ë„ë¥¼ ìœ„í•´ ìµœì†Œí•œë§Œ ê³„ì‚°)
    """
    if not YOLO_AVAILABLE:
        return {"enabled": False, "reason": "ultralytics(YOLO) ë¯¸ì„¤ì¹˜"}

    pose = YOLO("yolov8s-pose.pt")
    res = pose(image_bgr)
    r0 = res[0]

    if r0.keypoints is None or r0.boxes is None or len(r0.keypoints) == 0:
        return {"enabled": True, "detected": False}

    kpts = r0.keypoints.xy[0].cpu().numpy()  # (K,2)
    bbox = r0.boxes.xyxy[0].cpu().numpy()    # (x1,y1,x2,y2)
    h, w = image_bgr.shape[:2]

    cx, cy = np.mean(kpts, axis=0)
    cx = float(np.clip(cx, 0, w - 1))
    cy = float(np.clip(cy, 0, h - 1))

    # ì‚¼ë¶„í• ì„  ê·¼ì ‘ ì—¬ë¶€
    tol = 0.06
    thirds_x = [w / 3, 2 * w / 3]
    thirds_y = [h / 3, 2 * h / 3]
    on_thirds = any(abs(cx - tx) <= w * tol for tx in thirds_x) and \
                any(abs(cy - ty) <= h * tol for ty in thirds_y)

    # ì¸ë¬¼ ë¹„ìœ¨, í—¤ë“œë£¸ ìš”ì•½
    x1, y1, x2, y2 = bbox
    bw, bh = (x2 - x1), (y2 - y1)
    size_ratio = float((bw * bh) / (w * h))
    head_y = float(np.min(kpts[:, 1]))
    headroom_ratio = float(np.clip(head_y / h, 0, 1))

    return {
        "enabled": True,
        "detected": True,
        "center": (float(cx), float(cy)),
        "on_rule_of_thirds": bool(on_thirds),
        "size_ratio": size_ratio,
        "headroom_ratio": headroom_ratio,
        "bbox": [float(x1), float(y1), float(x2), float(y2)],
        "image_wh": [int(w), int(h)]
    }

# --------- MiDaS depth ìš”ì•½(ì„ íƒ) ----------
def summarize_depth(midas, midas_tf, device: str, img_bgr: np.ndarray) -> Dict[str, Any]:
    """
    ê¹Šì´ ì „ì²´ ë¶„í¬ë¥¼ ê°„ë‹¨íˆ ìš”ì•½(íˆìŠ¤í† ê·¸ë¨)í•˜ê³ ,
    ì¤‘ì•™ 40%ì˜ì—­ì˜ í‰ê· /ë¶„ì‚°ì„ ê³„ì‚°í•´ ì €ì¥(í”¼ì‚¬ì²´ ê·¼ì ‘ ê°€ì •)í•©ë‹ˆë‹¤.
    """
    img = _to_tensor(img_bgr)
    x = midas_tf(img).to(device)
    with torch.no_grad():
        pred = midas(x)
        depth = torch.nn.functional.interpolate(
            pred.unsqueeze(1),
            size=img_bgr.shape[:2],
            mode="bicubic",
            align_corners=False
        ).squeeze().cpu().numpy()

    d = depth.astype(np.float32)
    d = (d - d.min()) / max(1e-6, (d.max() - d.min()))  # 0~1 ì •ê·œí™”

    h, w = d.shape
    x0, x1 = int(w * 0.3), int(w * 0.7)
    y0, y1 = int(h * 0.3), int(h * 0.7)
    center_crop = d[y0:y1, x0:x1]

    hist, _ = np.histogram(d, bins=16, range=(0, 1), density=True)
    return {
        "hist16": hist.astype(np.float32),
        "global_mean": float(d.mean()),
        "global_std": float(d.std()),
        "center_mean": float(center_crop.mean()),
        "center_std": float(center_crop.std())
    }

# --------- ë©”ì¸ ---------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--ref", required=True, help="ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ê²½ë¡œ")
    ap.add_argument("--out_dir", default="./ref_cache", help="ì €ì¥ í´ë”(.npz)")
    ap.add_argument("--name", default=None, help="ì €ì¥ íŒŒì¼ëª…(í™•ì¥ì ì œì™¸). ê¸°ë³¸: ì´ë¯¸ì§€íŒŒì¼ëª… ê¸°ë°˜")
    ap.add_argument("--no_clip", action="store_true", help="CLIP ìƒëµ")
    ap.add_argument("--no_dino", action="store_true", help="DINO ìƒëµ")
    ap.add_argument("--no_depth", action="store_true", help="MiDaS ìƒëµ")
    ap.add_argument("--no_comp", action="store_true", help="YOLO êµ¬ë„ìš”ì•½ ìƒëµ")
    args = ap.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    _ensure_dir(args.out_dir)

    # ì´ë¯¸ì§€ ë¡œë“œ
    img = cv2.imread(args.ref)
    if img is None:
        raise FileNotFoundError(f"ì´ë¯¸ì§€ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.ref}")

    # ë©”íƒ€
    meta: Dict[str, Any] = {
        "source_path": os.path.abspath(args.ref),
        "saved_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "device": device,
        "versions": {}
    }

    # ---- CLIP ----
    clip_feat = None
    if not args.no_clip:
        clip_model, preprocess = clip.load("ViT-B/32", device=device)
        clip_feat = compute_clip_embedding(clip_model, preprocess, device, img)
        meta["versions"]["clip"] = "ViT-B/32"

    # ---- DINO ----
    dino_feat = None
    if not args.no_dino:
        dino_model, dino_name = load_dino_model(device)
        dino_feat = compute_dino_embedding(dino_model, device, img)
        meta["versions"]["dino"] = dino_name

    # ---- YOLO êµ¬ë„ìš”ì•½ ----
    comp = None
    if not args.no_comp:
        comp = summarize_composition_with_yolo(img)
        meta["versions"]["yolo_pose"] = "yolov8s-pose.pt" if YOLO_AVAILABLE else "not_available"

    # ---- MiDaS ----
    depth = None
    if not args.no_depth:
        midas, midas_tf = _load_midas(device)
        depth = summarize_depth(midas, midas_tf, device, img)
        meta["versions"]["midas"] = "DPT_Hybrid"

    # ---- ì €ì¥ ----
    base = args.name or os.path.splitext(os.path.basename(args.ref))[0]
    out_path = os.path.join(args.out_dir, f"{base}.npz")

    np.savez_compressed(
        out_path,
        clip_feat=clip_feat if clip_feat is not None else np.array([]),
        dino_feat=dino_feat if dino_feat is not None else np.array([]),
        depth_hist=depth["hist16"] if depth else np.array([]),
        depth_stats=np.array([
            depth["global_mean"], depth["global_std"],
            depth["center_mean"], depth["center_std"]
        ]) if depth else np.array([]),
        # compëŠ” dict â†’ json ë³„ë„ ì €ì¥
    )
    # composition ìš”ì•½/ë©”íƒ€ëŠ” jsonìœ¼ë¡œ ì €ì¥
    with open(out_path.replace(".npz", ".json"), "w", encoding="utf-8") as f:
        json.dump({"meta": meta, "composition": comp}, f, ensure_ascii=False, indent=2)

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {out_path}")
    print(f"ğŸ“ ë©”íƒ€/êµ¬ë„ìš”ì•½: {out_path.replace('.npz', '.json')}")


if __name__ == "__main__":
    main()
