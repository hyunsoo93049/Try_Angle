# ============================================================
# ğŸ¯ Phase 1: Image Interpretation Layer
# ê° ëª¨ë¸ì˜ ì¶œë ¥ì„ êµ¬ì²´ì ì¸ ì •ë³´ë¡œ í•´ì„í•˜ëŠ” ê³„ì¸µ
# ============================================================

import os
import cv2
import numpy as np
import torch
from PIL import Image
from typing import Dict, List, Optional

# ê¸°ì¡´ feature extractor
from feature_extraction.feature_extractor import extract_features_full
import clip
import open_clip


# ============================================================
# 1. DINO ê¸°ë°˜ êµ¬ì„± ë¶„ì„
# ============================================================

class CompositionAnalyzer:
    """
    DINO ë²¡í„°ë¥¼ í†µí•´ ì´ë¯¸ì§€ êµ¬ì„±ì„ ë¶„ì„
    - ê°ì²´ ìœ„ì¹˜
    - ëŒ€ì¹­ì„±
    - ë°¸ëŸ°ìŠ¤
    """
    
    def __init__(self, image_path: str, dino_feature: np.ndarray):
        self.image_path = image_path
        self.dino_feature = dino_feature  # (384,)
        self.image = cv2.imread(image_path)
        self.height, self.width = self.image.shape[:2]
        
    def analyze(self) -> Dict:
        """
        DINO ë²¡í„°ë¡œë¶€í„° êµ¬ì„± íŠ¹ì„±ì„ ì¶”ì¶œ
        (í˜„ì¬ëŠ” ë²¡í„° í†µê³„, í–¥í›„ DINO ê³µê°„ í•´ì„ ê³ ë„í™”)
        """
        
        # DINO ë²¡í„°ì˜ í†µê³„ì  íŠ¹ì„±
        dino_mean = float(np.mean(self.dino_feature))
        dino_std = float(np.std(self.dino_feature))
        dino_entropy = self._entropy(self.dino_feature)
        
        # ì´ë¯¸ì§€ ì¤‘ì‹¬ ë°€ì§‘ë„ ì¶”ì • (DINOëŠ” spatial attentionì´ ìˆìŒ)
        # ì‹¤ì œë¡œëŠ” DINOì˜ attention mapì„ ì¶”ì¶œí•˜ë©´ ë” ì •í™•
        composition_score = self._estimate_composition_score()
        
        # ëŒ€ì¹­ì„± ì ìˆ˜ (ì´ë¯¸ì§€ í”½ì…€ ê¸°ë°˜)
        symmetry_score = self._estimate_symmetry()
        
        return {
            "dino_mean": dino_mean,
            "dino_std": dino_std,
            "dino_entropy": dino_entropy,
            "composition_score": composition_score,  # 0-100
            "symmetry_score": symmetry_score,  # 0-100
            "is_balanced": composition_score > 60,
            "is_symmetric": symmetry_score > 60,
        }
    
    def _entropy(self, arr: np.ndarray) -> float:
        """íŠ¹ì„± ë²¡í„°ì˜ ì—”íŠ¸ë¡œí”¼ (ì–¼ë§ˆë‚˜ ë§ì€ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ”ê°€?)"""
        hist, _ = np.histogram(arr, bins=32, range=(arr.min(), arr.max()))
        hist = hist / (hist.sum() + 1e-8)
        entropy = -np.sum(hist * np.log(hist + 1e-8))
        return float(entropy)
    
    def _estimate_composition_score(self) -> float:
        """
        ì´ë¯¸ì§€ í”½ì…€ ê¸°ë°˜ êµ¬ì„± í‰ê°€
        (í–¥í›„: DINO attention mapìœ¼ë¡œ ëŒ€ì²´)
        """
        gray = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)
        
        # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ë¶€ë“œëŸ¬ìš´ ë°ê¸° ë¶„í¬ ìƒì„±
        blurred = cv2.GaussianBlur(gray, (51, 51), 0)
        
        # ì¤‘ì‹¬ë¶€ì™€ ì „ì²´ì˜ ë°ê¸° ë¹„ìœ¨
        h, w = gray.shape
        center = blurred[h//4:3*h//4, w//4:3*w//4]
        overall = blurred
        
        center_brightness = np.mean(center)
        overall_brightness = np.mean(overall)
        
        # ì¤‘ì‹¬ì´ ë°ìœ¼ë©´ good composition
        score = min(100, max(0, (center_brightness / (overall_brightness + 1e-8)) * 80 + 20))
        return float(score)
    
    def _estimate_symmetry(self) -> float:
        """ì¢Œìš° ëŒ€ì¹­ì„± ì ìˆ˜"""
        gray = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)
        gray = cv2.resize(gray, (256, 256))
        
        flipped = cv2.flip(gray, 1)
        
        # MSE ê¸°ë°˜ ìœ ì‚¬ë„
        mse = np.mean((gray.astype(float) - flipped.astype(float)) ** 2)
        max_mse = 255 ** 2
        
        symmetry = max(0, 100 - (mse / max_mse) * 100)
        return float(symmetry)


# ============================================================
# 2. MiDaS ê¸°ë°˜ ì¹´ë©”ë¼ ë¶„ì„
# ============================================================

class CameraAnalyzer:
    """
    MiDaS ê¹Šì´ ë§µìœ¼ë¡œë¶€í„° ì¹´ë©”ë¼ ì •ë³´ ì¶”ì¶œ
    - ì´¬ì˜ ê±°ë¦¬
    - ì´¬ì˜ ê°ë„ (ìœ„/ê°€ìš´ë°/ì•„ë˜)
    - ì´ˆì  ê¹Šì´
    """
    
    def __init__(self, image_path: str, midas_stats: np.ndarray):
        self.image_path = image_path
        self.midas_mean = float(midas_stats[0])
        self.midas_std = float(midas_stats[1])
        self.image = cv2.imread(image_path)
        
    def analyze(self) -> Dict:
        """
        MiDaS í†µê³„ë¡œë¶€í„° ì¹´ë©”ë¼ íŠ¹ì„± ì¶”ì¶œ
        """
        
        # ê¹Šì´ ë¶„í¬ë¡œë¶€í„° ì´¬ì˜ ê±°ë¦¬ ì¶”ì •
        # (ì‹¤ì œ MiDaS ì¶œë ¥ê°’ì€ ìƒëŒ€ì  ê¹Šì´)
        distance_estimate = self._estimate_distance()
        
        # ê¹Šì´ í‘œì¤€í¸ì°¨ë¡œë¶€í„° ì´¬ì˜ ê°ë„ ì¶”ì •
        # stdê°€ í¬ë©´ = ìœ„/ì•„ë˜ì—ì„œ ì´¬ì˜í•œ ëŠë‚Œ
        angle_estimate = self._estimate_angle()
        
        # ì´ˆì  ê¹Šì´
        focus_depth = self._estimate_focus_depth()
        
        return {
            "mean_depth": self.midas_mean,
            "depth_std": self.midas_std,
            "estimated_distance": distance_estimate,  # cm (ìƒëŒ€ê°’)
            "estimated_angle": angle_estimate,  # -45~+45 (ìŒìˆ˜=ìœ„, ì–‘ìˆ˜=ì•„ë˜)
            "focus_depth": focus_depth,  # 0-100 (ê¹Šì´ê°)
            "is_wide_angle": distance_estimate > 150,
            "is_close_up": distance_estimate < 50,
        }
    
    def _estimate_distance(self) -> float:
        """
        ê¹Šì´ í‰ê· ê°’ìœ¼ë¡œë¶€í„° ê±°ë¦¬ ì¶”ì • (ì •ê·œí™”)
        """
        # MiDaS ì¶œë ¥ì€ ëŒ€ëŒ€ë¡œ 0~1 ë²”ìœ„
        # ì´ë¥¼ cm ë‹¨ìœ„ë¡œ ë³€í™˜ (ìƒëŒ€ê°’)
        distance = self.midas_mean * 200  # ì„ì˜ ìŠ¤ì¼€ì¼
        return float(distance)
    
    def _estimate_angle(self) -> float:
        """
        ê¹Šì´ í‘œì¤€í¸ì°¨ë¡œë¶€í„° ì´¬ì˜ ê°ë„ ì¶”ì •
        ë†’ì€ std = ìœ„/ì•„ë˜ì—ì„œ ì´¬ì˜í•œ ëŠë‚Œ
        
        return: -45 (ìœ„ì—ì„œ) ~ 0 (ê°€ìš´ë°) ~ +45 (ì•„ë˜ì—ì„œ)
        """
        # stdê°€ ì‘ìœ¼ë©´ ì •ë©´, í¬ë©´ ê°ì§„ ê°ë„
        angle_score = (self.midas_std - 0.05) * 100  # ì •ê·œí™”
        angle = np.clip(angle_score * 45 - 22.5, -45, 45)
        return float(angle)
    
    def _estimate_focus_depth(self) -> float:
        """ì´ˆì  ê¹Šì´ê° (0-100)"""
        # stdê°€ í¬ë©´ ê¹Šì´ê° ìˆìŒ
        focus = min(100, self.midas_std * 500)
        return float(focus)


# ============================================================
# 3. CLIP ê¸°ë°˜ ì‹¬ë¯¸ì„± ë¶„ì„
# ============================================================

class AestheticAnalyzer:
    """
    CLIP ì„ë² ë”©ê³¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì´ìš©í•´ ì‹¬ë¯¸ì„± ë¶„ì„
    - ë°ê¸° ë ˆë²¨
    - ìƒ‰ê° ì˜¨ë„
    - í¬í™”ë„
    """
    
    def __init__(self, image_path: str, clip_feature: np.ndarray):
        self.image_path = image_path
        self.clip_feature = clip_feature
        self.image = cv2.imread(image_path)
        
        # CLIP ëª¨ë¸ ë¡œë“œ
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=self.device)
        
    def analyze(self) -> Dict:
        """
        ì‹¬ë¯¸ ì†ì„± ë¶„ì„
        """
        
        # í”½ì…€ ê¸°ë°˜ ë¶„ì„
        brightness = self._analyze_brightness()
        color_temp = self._analyze_color_temperature()
        saturation = self._analyze_saturation()
        
        # CLIP ê¸°ë°˜ ê°ì • ë¶„ì„ (í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
        emotions = self._analyze_emotions_with_prompts()
        
        return {
            "brightness": brightness,  # 0-100
            "color_temperature": color_temp,  # 0-100 (0=ì°¨ê°€ì›€, 100=ë”°ëœ»í•¨)
            "saturation": saturation,  # 0-100
            "emotions": emotions,  # dict
            "overall_aesthetic_score": self._calculate_aesthetic_score(
                brightness, color_temp, saturation, emotions
            ),
        }
    
    def _analyze_brightness(self) -> float:
        """ë°ê¸° ë¶„ì„"""
        gray = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)
        brightness = np.mean(gray) / 255.0 * 100
        return float(brightness)
    
    def _analyze_color_temperature(self) -> float:
        """
        ìƒ‰ì˜¨ë„ ë¶„ì„ (ë”°ëœ»í•¨/ì°¨ê°€ì›€)
        ë”°ëœ»í•¨: ë¹¨ê°•/ë…¸ë‘ ì±„ë„ì´ ë†’ìŒ
        ì°¨ê°€ì›€: íŒŒë‘ ì±„ë„ì´ ë†’ìŒ
        """
        b, g, r = cv2.split(self.image)
        
        warm_score = (np.mean(r) + np.mean(g)) / 2
        cool_score = np.mean(b)
        
        # 0 = ì°¨ê°€ì›€, 50 = ì¤‘ë¦½, 100 = ë”°ëœ»í•¨
        if warm_score + cool_score > 0:
            temperature = (warm_score / (warm_score + cool_score)) * 100
        else:
            temperature = 50
            
        return float(temperature)
    
    def _analyze_saturation(self) -> float:
        """í¬í™”ë„ ë¶„ì„"""
        hsv = cv2.cvtColor(self.image, cv2.COLOR_BGR2HSV)
        s_channel = hsv[:, :, 1]
        saturation = np.mean(s_channel) / 255.0 * 100
        return float(saturation)
    
    def _analyze_emotions_with_prompts(self) -> Dict:
        """
        í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ê°ì • ë¶„ì„
        ëŠì¢‹ ìŠ¤íƒ€ì¼ì˜ ê°ì •ë“¤ì„ í”„ë¡¬í”„íŠ¸ë¡œ ì…ë ¥
        """
        emotions_prompts = {
            "bright": "a bright and vivid photo",
            "moody": "a moody and dark photo",
            "warm": "a warm and cozy photo",
            "cool": "a cool and calm photo",
            "vibrant": "a vibrant and saturated photo",
            "soft": "a soft and muted photo",
        }
        
        img_pil = Image.open(self.image_path).convert("RGB")
        img_tensor = self.clip_preprocess(img_pil).unsqueeze(0).to(self.device)
        
        emotions = {}
        with torch.no_grad():
            image_features = self.clip_model.encode_image(img_tensor)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            for emotion, prompt in emotions_prompts.items():
                text = clip.tokenize(prompt).to(self.device)
                text_features = self.clip_model.encode_text(text)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                similarity = float((image_features @ text_features.T).item())
                # -1~1ì„ 0~100ìœ¼ë¡œ ë³€í™˜
                score = ((similarity + 1) / 2) * 100
                emotions[emotion] = score
        
        return emotions
    
    def _calculate_aesthetic_score(self, brightness, color_temp, saturation, emotions) -> float:
        """ì¢…í•© ì‹¬ë¯¸ ì ìˆ˜"""
        # ë‹¨ìˆœ í‰ê·  (í–¥í›„ ê°€ì¤‘ì¹˜ ì ìš© ê°€ëŠ¥)
        emotion_avg = np.mean(list(emotions.values()))
        overall = (brightness + color_temp + saturation + emotion_avg) / 4
        return float(overall)


# ============================================================
# 4. OpenCLIP ê¸°ë°˜ ì¡°ëª… ë¶„ì„
# ============================================================

class LightingAnalyzer:
    """
    OpenCLIPìœ¼ë¡œ ì¡°ëª… ë°©í–¥/ìœ í˜• ë¶„ì„
    - ì •ë©´/ì¸¡ë©´/ì—­ê´‘
    - ìì—°ê´‘/ì¸ê³µê´‘
    - ì†Œí”„íŠ¸/í•˜ë“œ ë¼ì´íŠ¸
    """
    
    def __init__(self, image_path: str, openclip_feature: np.ndarray):
        self.image_path = image_path
        self.openclip_feature = openclip_feature
        
        # OpenCLIP ëª¨ë¸ ë¡œë“œ
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, _, self.preprocess = open_clip.create_model_and_transforms(
            "ViT-B-32", pretrained="laion2b_s34b_b79k", device=self.device
        )
        self.tokenizer = open_clip.get_tokenizer("ViT-B-32")
        
    def analyze(self) -> Dict:
        """ì¡°ëª… íŠ¹ì„± ë¶„ì„"""
        
        lighting_type = self._analyze_lighting_type()
        lighting_direction = self._analyze_lighting_direction()
        lighting_softness = self._analyze_lighting_softness()
        
        return {
            "lighting_type": lighting_type,  # "natural", "artificial", "mixed"
            "lighting_direction": lighting_direction,  # "front", "side", "backlit"
            "lighting_softness": lighting_softness,  # 0-100 (0=hard, 100=soft)
            "is_natural_light": lighting_type == "natural",
            "is_backlit": lighting_direction == "backlit",
        }
    
    def _analyze_lighting_type(self) -> str:
        """ì¡°ëª… ìœ í˜• íŒì • (ìì—°ê´‘/ì¸ê³µê´‘)"""
        prompts = {
            "natural": "natural light from outside, sunlight",
            "artificial": "artificial light, indoor light, lamp",
        }
        
        img_pil = Image.open(self.image_path).convert("RGB")
        img_tensor = self.preprocess(img_pil).unsqueeze(0).to(self.device)
        
        scores = {}
        with torch.no_grad():
            image_features = self.model.encode_image(img_tensor)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            for key, prompt in prompts.items():
                text = self.tokenizer(prompt).to(self.device)
                text_features = self.model.encode_text(text)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                similarity = float((image_features @ text_features.T).item())
                scores[key] = similarity
        
        # ì ìˆ˜ê°€ ë†’ì€ ê²ƒ ì„ íƒ
        return max(scores, key=scores.get)
    
    def _analyze_lighting_direction(self) -> str:
        """ì¡°ëª… ë°©í–¥ íŒì •"""
        prompts = {
            "front": "front lighting, face is well lit",
            "side": "side lighting, one side is lit",
            "backlit": "backlit, light from behind, silhouette",
        }
        
        img_pil = Image.open(self.image_path).convert("RGB")
        img_tensor = self.preprocess(img_pil).unsqueeze(0).to(self.device)
        
        scores = {}
        with torch.no_grad():
            image_features = self.model.encode_image(img_tensor)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            for key, prompt in prompts.items():
                text = self.tokenizer(prompt).to(self.device)
                text_features = self.model.encode_text(text)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                similarity = float((image_features @ text_features.T).item())
                scores[key] = similarity
        
        return max(scores, key=scores.get)
    
    def _analyze_lighting_softness(self) -> float:
        """ì¡°ëª…ì˜ ë¶€ë“œëŸ¬ì›€"""
        prompts = {
            "soft": "soft light, diffused light, no harsh shadows",
            "hard": "hard light, sharp shadows, high contrast",
        }
        
        img_pil = Image.open(self.image_path).convert("RGB")
        img_tensor = self.preprocess(img_pil).unsqueeze(0).to(self.device)
        
        scores = {}
        with torch.no_grad():
            image_features = self.model.encode_image(img_tensor)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            for key, prompt in prompts.items():
                text = self.tokenizer(prompt).to(self.device)
                text_features = self.model.encode_text(text)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                similarity = float((image_features @ text_features.T).item())
                scores[key] = similarity
        
        # softì™€ hardì˜ ìƒëŒ€ ì ìˆ˜ë¡œ 0-100 ë³€í™˜
        soft_score = scores.get("soft", 0)
        hard_score = scores.get("hard", 0)
        
        softness = ((soft_score - hard_score + 2) / 4) * 100  # -1~1ì„ 0~100ìœ¼ë¡œ
        return float(np.clip(softness, 0, 100))


# ============================================================
# 5. Color/Texture ê¸°ë°˜ ê¸°ìˆ  ë¶„ì„
# ============================================================

class TechnicalAnalyzer:
    """
    ìƒ‰ìƒ/í…ìŠ¤ì²˜ë¡œë¶€í„° ê¸°ìˆ ì  ì†ì„± ë¶„ì„
    - ëª…ë„ ë¶„í¬ (ë…¸ì¶œ)
    - ìƒ‰ìƒ ë¶„í¬
    - ë””í…Œì¼ ìˆ˜ì¤€
    - ISO/ë…¸ì¶œ ì¶”ì²œ
    """
    
    def __init__(self, image_path: str, color_feature: np.ndarray):
        self.image_path = image_path
        self.color_feature = color_feature
        self.image = cv2.imread(image_path)
        
    def analyze(self) -> Dict:
        """ê¸°ìˆ ì  ë¶„ì„"""
        
        exposure = self._analyze_exposure()
        detail_level = self._analyze_detail_level()
        noise_level = self._estimate_noise()
        white_balance = self._analyze_white_balance()
        
        return {
            "exposure": exposure,  # 0-100 (0=underexposed, 50=correct, 100=overexposed)
            "detail_level": detail_level,  # 0-100
            "noise_level": noise_level,  # 0-100 (ì¶”ì •)
            "white_balance": white_balance,  # "cool", "neutral", "warm"
            "recommended_iso": self._recommend_iso(noise_level),
            "recommended_ev_adjustment": self._recommend_ev(exposure),
        }
    
    def _analyze_exposure(self) -> float:
        """ë…¸ì¶œ ë¶„ì„"""
        gray = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)
        mean_brightness = np.mean(gray)
        
        # 0-255 ë²”ìœ„ë¥¼ 0-100ìœ¼ë¡œ (128=50)
        exposure = (mean_brightness / 255.0) * 100
        return float(exposure)
    
    def _analyze_detail_level(self) -> float:
        """ë””í…Œì¼ ìˆ˜ì¤€ (ì—£ì§€ ë°€ë„)"""
        gray = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        detail = (np.sum(edges) / edges.size) * 100
        return float(np.clip(detail, 0, 100))
    
    def _estimate_noise(self) -> float:
        """ë…¸ì´ì¦ˆ ìˆ˜ì¤€ ì¶”ì •"""
        gray = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)
        
        # Laplacian ì ìš© í›„ ë¶„ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì¶”ì •
        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        noise = np.var(laplacian) / 1000  # ì •ê·œí™”
        
        return float(np.clip(noise, 0, 100))
    
    def _analyze_white_balance(self) -> str:
        """í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤"""
        b, g, r = cv2.split(self.image)
        
        b_mean = np.mean(b)
        g_mean = np.mean(g)
        r_mean = np.mean(r)
        
        # R/G ë¹„ìœ¨ë¡œ íŒì •
        rg_ratio = r_mean / (g_mean + 1e-8)
        
        if rg_ratio > 1.05:
            return "warm"
        elif rg_ratio < 0.95:
            return "cool"
        else:
            return "neutral"
    
    def _recommend_iso(self, noise_level: float) -> int:
        """ISO ì¶”ì²œ"""
        if noise_level < 20:
            return 100
        elif noise_level < 40:
            return 200
        elif noise_level < 60:
            return 400
        else:
            return 800
    
    def _recommend_ev(self, exposure: float) -> float:
        """EV ê°’ ì¡°ì • ì¶”ì²œ (-2.0 ~ +2.0)"""
        # 50ì´ ì •ìƒì´ë¼ê³  ê°€ì •
        target = 50
        diff = target - exposure
        
        # í•œ ë‹¨ê³„ = ì•½ 10
        ev_adjustment = diff / 10 * 0.3  # 1 ë‹¨ê³„ = 0.3 EV
        return float(np.clip(ev_adjustment, -2.0, 2.0))


# ============================================================
# 6. í†µí•© í•´ì„ í´ë˜ìŠ¤
# ============================================================

class ImageInterpretation:
    """
    í•œ ì¥ì˜ ì´ë¯¸ì§€ë¥¼ ëª¨ë“  ëª¨ë¸ë¡œ ë¶„ì„í•´ì„œ ì¢…í•© ì •ë³´ ìƒì„±
    """
    
    def __init__(self, image_path: str):
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"âŒ Image not found: {image_path}")
        
        self.image_path = image_path
        print(f"ğŸ“¸ Loading image: {image_path}")
        
        # Feature ì¶”ì¶œ
        print("ğŸ”§ Extracting features...")
        features = extract_features_full(image_path)
        if features is None:
            raise RuntimeError("âŒ Feature extraction failed!")
        
        self.features = features
        
        # ê° ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.composition_analyzer = CompositionAnalyzer(
            image_path, features["dino"]
        )
        self.camera_analyzer = CameraAnalyzer(
            image_path, features["midas"]
        )
        self.aesthetic_analyzer = AestheticAnalyzer(
            image_path, features["clip"]
        )
        self.lighting_analyzer = LightingAnalyzer(
            image_path, features["openclip"]
        )
        self.technical_analyzer = TechnicalAnalyzer(
            image_path, features["color"]
        )
        
    def analyze(self) -> Dict:
        """ëª¨ë“  ë¶„ì„ ì‹¤í–‰"""
        print("\nğŸ¯ Analyzing image...")
        
        composition = self.composition_analyzer.analyze()
        camera = self.camera_analyzer.analyze()
        aesthetic = self.aesthetic_analyzer.analyze()
        lighting = self.lighting_analyzer.analyze()
        technical = self.technical_analyzer.analyze()
        
        return {
            "composition": composition,
            "camera": camera,
            "aesthetic": aesthetic,
            "lighting": lighting,
            "technical": technical,
        }
    
    def get_summary(self) -> Dict:
        """ê°„ë‹¨í•œ ìš”ì•½"""
        analysis = self.analyze()
        
        return {
            "composition_score": analysis["composition"]["composition_score"],
            "balance_status": "balanced" if analysis["composition"]["is_balanced"] else "unbalanced",
            "camera_distance": analysis["camera"]["estimated_distance"],
            "camera_angle": analysis["camera"]["estimated_angle"],
            "brightness": analysis["aesthetic"]["brightness"],
            "color_temperature": analysis["aesthetic"]["color_temperature"],
            "lighting_type": analysis["lighting"]["lighting_type"],
            "lighting_direction": analysis["lighting"]["lighting_direction"],
            "recommended_iso": analysis["technical"]["recommended_iso"],
            "recommended_ev": analysis["technical"]["recommended_ev_adjustment"],
        }


# ============================================================
# í…ŒìŠ¤íŠ¸
# ============================================================

if __name__ == "__main__":
    test_img = r"C:\try_angle\data\test_images\test1.jpg"
    
    try:
        interp = ImageInterpretation(test_img)
        result = interp.analyze()
        
        print("\n" + "="*50)
        print("ğŸ“Š FULL ANALYSIS RESULT")
        print("="*50)
        
        import json
        print(json.dumps(result, indent=2, ensure_ascii=False))
        
        print("\n" + "="*50)
        print("ğŸ“‹ SUMMARY")
        print("="*50)
        summary = interp.get_summary()
        for key, value in summary.items():
            print(f"{key:25s}: {value}")
            
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()