# ============================================================
# ğŸ”§ Load Embedded Models (Scaler + UMAP) â€” version2 compatible
# ============================================================

import os
import joblib
import numpy as np
import sys

# Model cache
sys.path.append(r"C:\try_angle\src\Multi\version3")
from utils.model_cache import model_cache

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ
FEATURE_MODEL_DIR = r"C:\try_angle\feature_models"

SCALER_CLIP_PATH      = os.path.join(FEATURE_MODEL_DIR, "scaler_clip.joblib")
SCALER_OPENCLIP_PATH  = os.path.join(FEATURE_MODEL_DIR, "scaler_openclip.joblib")
SCALER_DINO_PATH      = os.path.join(FEATURE_MODEL_DIR, "scaler_dino.joblib")
SCALER_COLOR_PATH     = os.path.join(FEATURE_MODEL_DIR, "scaler_color.joblib")
SCALER_MIDAS_PATH     = os.path.join(FEATURE_MODEL_DIR, "scaler_midas.joblib")

UMAP_MODEL_PATH       = os.path.join(FEATURE_MODEL_DIR, "umap_128d_model.joblib")

# -----------------------------
# Load models (ì‹±ê¸€í†¤)
# -----------------------------
def _load_embedder_models():
    """Embedder ëª¨ë¸ ë¡œë“œ (í•œ ë²ˆë§Œ)"""
    print("ğŸ”§ Loading embedder models...")

    scaler_clip      = joblib.load(SCALER_CLIP_PATH)
    scaler_openclip  = joblib.load(SCALER_OPENCLIP_PATH)
    scaler_dino      = joblib.load(SCALER_DINO_PATH)
    scaler_color     = joblib.load(SCALER_COLOR_PATH)
    scaler_midas     = joblib.load(SCALER_MIDAS_PATH)

    umap_model       = joblib.load(UMAP_MODEL_PATH)

    print("âœ… Embedder models loaded successfully")

    return {
        "scaler_clip": scaler_clip,
        "scaler_openclip": scaler_openclip,
        "scaler_dino": scaler_dino,
        "scaler_color": scaler_color,
        "scaler_midas": scaler_midas,
        "umap_model": umap_model
    }

def get_embedder_models():
    """Embedder ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤)"""
    return model_cache.get_or_load("embedder_models", _load_embedder_models)

def embed_features(feature_dict: dict):
    """
    feature_extractor_v2.py â†’ extract_features_v2() ê²°ê³¼(dict) ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤.
    {
        "clip": (512,),
        "openclip": (512,),
        "dino": (384,),
        "color": (150,),
        "midas": (20,),
        "yolo_pose": (15,),  # v2ì—ì„œ ì¶”ê°€
        "face": (7,)         # v2ì—ì„œ ì¶”ê°€
    }

    Note: yolo_poseì™€ faceëŠ” í•™ìŠµ ì‹œ ê°€ì¤‘ì¹˜=0ì´ë¯€ë¡œ ì‹¤ì œë¡œëŠ” ì‚¬ìš© ì•ˆ ë¨
    """
    # -----------------------------
    # ì…ë ¥ì´ path ë©´ â†’ âŒ ì˜ëª»ëœ í˜¸ì¶œ
    # -----------------------------
    if isinstance(feature_dict, str):
        raise ValueError(
            "âŒ embed_features()ëŠ” image_pathê°€ ì•„ë‹ˆë¼ feature_dictë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "ë¨¼ì € extract_features_full(image_path)ë¡œ featureë¥¼ ì¶”ì¶œí•˜ì„¸ìš”."
        )

    # ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ìºì‹œë¨)
    models = get_embedder_models()
    scaler_clip = models["scaler_clip"]
    scaler_openclip = models["scaler_openclip"]
    scaler_dino = models["scaler_dino"]
    scaler_color = models["scaler_color"]
    scaler_midas = models["scaler_midas"]
    umap_model = models["umap_model"]

    clip_vec  = feature_dict["clip"].reshape(1, -1)
    open_vec  = feature_dict["openclip"].reshape(1, -1)
    dino_vec  = feature_dict["dino"].reshape(1, -1)   # ë°˜ë“œì‹œ 384
    color_vec = feature_dict["color"].reshape(1, -1)
    midas_vec = feature_dict["midas"].reshape(1, -1)

    # yolo_pose, face (v2ì—ì„œ ì¶”ê°€, í•˜ì§€ë§Œ ê°€ì¤‘ì¹˜=0)
    yolo_pose_vec = feature_dict["yolo_pose"].reshape(1, -1)  # (1, 15)
    face_vec = feature_dict["face"].reshape(1, -1)            # (1, 7)

    # -----------------------------
    # feature_modelsì—ì„œ ë¶ˆëŸ¬ì˜¨ scaler ì ìš©
    # -----------------------------
    clip_scaled      = scaler_clip.transform(clip_vec)
    openclip_scaled  = scaler_openclip.transform(open_vec)
    dino_scaled      = scaler_dino.transform(dino_vec)
    color_scaled     = scaler_color.transform(color_vec)
    midas_scaled     = scaler_midas.transform(midas_vec)

    # pose, faceëŠ” scaling ì—†ì´ ê·¸ëƒ¥ ì‚¬ìš© (ì–´ì°¨í”¼ ê°€ì¤‘ì¹˜=0)
    # í•™ìŠµ ì‹œì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
    pose_combined = np.concatenate([yolo_pose_vec, face_vec], axis=1)  # (1, 22)

    # -----------------------------
    # 1600D ìœµí•© (512+512+384+150+20+22)
    # í•™ìŠµ ì‹œ ê°€ì¤‘ì¹˜: clip=0.3, openclip=0.3, dino=0.25, color=0.12, midas=0.03, pose=0.0
    # -----------------------------
    fusion = np.concatenate([
        clip_scaled * 0.30,
        openclip_scaled * 0.30,
        dino_scaled * 0.25,
        color_scaled * 0.12,
        midas_scaled * 0.03,
        pose_combined * 0.00  # ê°€ì¤‘ì¹˜ 0ì´ë¯€ë¡œ ì‹¤ì œë¡œëŠ” ë¬´ì‹œë¨
    ], axis=1)

    # -----------------------------
    # UMAP 128D ì¶•ì†Œ
    # -----------------------------
    vec128 = umap_model.transform(fusion)[0]
    return vec128
