# ============================================================
# ğŸ¯ TryAngle Feature Extractor v3
# Phase 3-4: Contrastive Learning ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ
# ============================================================

import os
import sys
import torch
import cv2
import numpy as np
from pathlib import Path
from typing import Dict, Optional
from PIL import Image
from torchvision import transforms

# Project root ì„¤ì •
VERSION3_DIR = Path(__file__).resolve().parents[1]
PROJECT_ROOT = VERSION3_DIR
while PROJECT_ROOT != PROJECT_ROOT.parent and not ((PROJECT_ROOT / "data").exists() and (PROJECT_ROOT / "src").exists()):
    PROJECT_ROOT = PROJECT_ROOT.parent

if str(VERSION3_DIR) not in sys.path:
    sys.path.append(str(VERSION3_DIR))

from contrastive.contrastive_model import create_contrastive_model
from utils.model_cache import model_cache

# v2ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ import
try:
    from feature_extraction.feature_extractor_v2 import (
        extract_midas_features,
        extract_color_features,
        extract_yolo_pose_features,
        extract_face_features
    )
    V2_AVAILABLE = True
except ImportError:
    print("âš ï¸ Feature Extractor v2 not available")
    V2_AVAILABLE = False


# ============================================================
# Contrastive Model ë¡œë“œ
# ============================================================

def _load_contrastive_model():
    """
    í›ˆë ¨ëœ ëŒ€ì¡° í•™ìŠµ ëª¨ë¸ ë¡œë“œ (ì‹±ê¸€í†¤)

    Returns:
        (model, device, transform)
    """
    print("  ğŸ”§ Loading contrastive model...")

    # ëª¨ë¸ ê²½ë¡œ
    model_path = VERSION3_DIR / "models" / "contrastive" / "best_model.pth"

    if not model_path.exists():
        raise FileNotFoundError(
            f"Contrastive model not found: {model_path}\n"
            f"Train the model first: python scripts/train_contrastive.py"
        )

    # Device ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ëª¨ë¸ ìƒì„±
    model = create_contrastive_model(
        backbone="resnet50",
        pretrained=False,
        projection_dim=128
    )

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()

    # Transform (ê²€ì¦ìš©)
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    print(f"  âœ… Contrastive model loaded (device: {device})")

    return model, device, transform


def get_contrastive_model():
    """
    ëŒ€ì¡° í•™ìŠµ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤)

    Returns:
        (model, device, transform)
    """
    return model_cache.get_or_load("contrastive_model", _load_contrastive_model)


# ============================================================
# Contrastive Features ì¶”ì¶œ
# ============================================================

def extract_contrastive_features(image_path: str) -> np.ndarray:
    """
    ëŒ€ì¡° í•™ìŠµ ëª¨ë¸ë¡œ íŠ¹ì§• ì¶”ì¶œ

    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ

    Returns:
        (128,) embedding ë²¡í„°
    """
    model, device, transform = get_contrastive_model()

    # ì´ë¯¸ì§€ ë¡œë“œ
    img = Image.open(image_path).convert('RGB')
    img_tensor = transform(img).unsqueeze(0).to(device)

    # íŠ¹ì§• ì¶”ì¶œ
    with torch.no_grad():
        embedding = model.get_embeddings(img_tensor)

    # NumPyë¡œ ë³€í™˜
    embedding = embedding.cpu().numpy().flatten()

    return embedding


# ============================================================
# Feature Extractor v3 (Hybrid)
# ============================================================

def extract_features_v3(image_path: str, use_v2_features: bool = True) -> Optional[Dict]:
    """
    v3 íŠ¹ì§• ì¶”ì¶œ: Contrastive Learning + v2 features

    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        use_v2_features: v2 íŠ¹ì§•ë„ í¬í•¨í• ì§€ ì—¬ë¶€

    Returns:
        {
            'contrastive': (128,),  # ëŒ€ì¡° í•™ìŠµ embedding
            'midas': (256,),        # ê¹Šì´ íŠ¹ì§• (v2)
            'color': (256,),        # ìƒ‰ìƒ íŠ¹ì§• (v2)
            'yolo_pose': (34,),     # í¬ì¦ˆ íŠ¹ì§• (v2)
            'face': (6,)            # ì–¼êµ´ íŠ¹ì§• (v2)
        }
    """
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"Image not found: {image_path}")

    features = {}

    # 1. Contrastive features (í•µì‹¬)
    try:
        contrastive_feat = extract_contrastive_features(image_path)
        features['contrastive'] = contrastive_feat
    except Exception as e:
        print(f"âš ï¸ Contrastive feature extraction failed: {e}")
        return None

    # 2. v2 features (ë³´ì¡°)
    if use_v2_features and V2_AVAILABLE:
        try:
            # MiDaS (ê¹Šì´)
            midas_feat = extract_midas_features(image_path)
            features['midas'] = midas_feat

            # Color
            color_feat = extract_color_features(image_path)
            features['color'] = color_feat

            # YOLO Pose
            yolo_feat = extract_yolo_pose_features(image_path)
            features['yolo_pose'] = yolo_feat

            # Face
            face_feat = extract_face_features(image_path)
            features['face'] = face_feat

        except Exception as e:
            print(f"âš ï¸ v2 feature extraction failed: {e}")

    return features


def extract_features_v3_contrastive_only(image_path: str) -> Optional[Dict]:
    """
    v3 íŠ¹ì§• ì¶”ì¶œ (Contrastiveë§Œ)

    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ

    Returns:
        {'contrastive': (128,)}
    """
    return extract_features_v3(image_path, use_v2_features=False)


# ============================================================
# í•˜ìœ„ í˜¸í™˜ì„±: v2ì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤
# ============================================================

def extract_features_v3_full(image_path: str) -> Optional[Dict]:
    """
    v3 ì „ì²´ íŠ¹ì§• ì¶”ì¶œ (v2 í˜¸í™˜)

    feature_extractor_v2.extract_features_v2()ì™€ ë™ì¼í•œ í¬ë§·ìœ¼ë¡œ ë°˜í™˜
    í•˜ì§€ë§Œ CLIP/OpenCLIP/DINO ëŒ€ì‹  Contrastive ì‚¬ìš©

    Returns:
        {
            'clip': (512,),      # ëŒ€ì²´ â†’ contrastiveì˜ ì¼ë¶€
            'openclip': (512,),  # ëŒ€ì²´ â†’ contrastiveì˜ ì¼ë¶€
            'dino': (384,),      # ëŒ€ì²´ â†’ contrastiveì˜ ì¼ë¶€
            'midas': (256,),
            'color': (256,),
            'yolo_pose': (34,),
            'face': (6,)
        }
    """
    features = extract_features_v3(image_path, use_v2_features=True)

    if features is None:
        return None

    # Contrastive embeddingì„ CLIP/OpenCLIP/DINOë¡œ ë¶„í• 
    # (128Dë¥¼ 42D + 43D + 43Dë¡œ ë¶„í• )
    contrastive = features['contrastive']

    # v2 í˜¸í™˜ í¬ë§·ìœ¼ë¡œ ë³€í™˜
    v2_compatible = {
        'clip': np.concatenate([contrastive[:42], np.zeros(470)]),  # 512Dë¡œ íŒ¨ë”©
        'openclip': np.concatenate([contrastive[42:85], np.zeros(469)]),  # 512Dë¡œ íŒ¨ë”©
        'dino': np.concatenate([contrastive[85:128], np.zeros(341)]),  # 384Dë¡œ íŒ¨ë”©
        'midas': features.get('midas', np.zeros(256)),
        'color': features.get('color', np.zeros(256)),
        'yolo_pose': features.get('yolo_pose', np.zeros(34)),
        'face': features.get('face', np.zeros(6))
    }

    return v2_compatible


# ============================================================
# í…ŒìŠ¤íŠ¸
# ============================================================

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
    test_img = PROJECT_ROOT / "data" / "test_images" / "test1.jpg"

    if not test_img.exists():
        print(f"âŒ Test image not found: {test_img}")
        print("Please provide a test image.")
        sys.exit(1)

    print("\n" + "="*60)
    print("ğŸ§ª Feature Extractor v3 Test")
    print("="*60)

    try:
        # Contrastive only
        print("\n1ï¸âƒ£ Contrastive features only:")
        features_contrastive = extract_features_v3_contrastive_only(str(test_img))
        if features_contrastive:
            print(f"   âœ… Contrastive: {features_contrastive['contrastive'].shape}")

        # Full features (v3)
        print("\n2ï¸âƒ£ Full features (v3):")
        features_v3 = extract_features_v3(str(test_img), use_v2_features=True)
        if features_v3:
            for key, feat in features_v3.items():
                print(f"   {key}: {feat.shape}")

        # v2 compatible
        print("\n3ï¸âƒ£ v2 compatible format:")
        features_v2_compat = extract_features_v3_full(str(test_img))
        if features_v2_compat:
            total_dim = sum(f.shape[0] for f in features_v2_compat.values())
            print(f"   Total dimension: {total_dim}D")
            for key, feat in features_v2_compat.items():
                print(f"   {key}: {feat.shape}")

        print("\n" + "="*60)
        print("âœ… All tests passed!")
        print("="*60)

    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
