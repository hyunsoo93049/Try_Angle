# ============================================================
# ğŸ¤¸ TryAngle - Pose Analyzer
# YOLO11-pose + MediaPipe í•˜ì´ë¸Œë¦¬ë“œ í¬ì¦ˆ ë¶„ì„
# ============================================================

import cv2
import numpy as np
from typing import Dict, List, Optional, Tuple
import os
import sys
from pathlib import Path

# Model cache
VERSION3_DIR = Path(__file__).resolve().parents[1]
PROJECT_ROOT = VERSION3_DIR
while PROJECT_ROOT != PROJECT_ROOT.parent and not ((PROJECT_ROOT / "data").exists() and (PROJECT_ROOT / "src").exists()):
    PROJECT_ROOT = PROJECT_ROOT.parent

if str(VERSION3_DIR) not in sys.path:
    sys.path.append(str(VERSION3_DIR))

from utils.model_cache import model_cache

# YOLO
try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    print("âš ï¸ ultralytics not installed. YOLO pose detection disabled.")
    YOLO_AVAILABLE = False

# MediaPipe
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    print("âš ï¸ mediapipe not installed. MediaPipe detection disabled.")
    MEDIAPIPE_AVAILABLE = False


class PoseAnalyzer:
    """
    YOLO11-pose + MediaPipe í•˜ì´ë¸Œë¦¬ë“œ í¬ì¦ˆ ë¶„ì„ê¸°

    ì‹œë‚˜ë¦¬ì˜¤ë³„ ìµœì  ëª¨ë¸ ì„ íƒ:
    - ì „ì‹ /ë’·ëª¨ìŠµ/ì˜†ëª¨ìŠµ/ë©€ë¦¬: YOLOë§Œ
    - ì–¼êµ´ í´ë¡œì¦ˆì—…: YOLO + MediaPipe Face
    - ì† ì œìŠ¤ì²˜: YOLO + MediaPipe Hands
    - ë””í…Œì¼ í•„ìš”: YOLO + MediaPipe Pose
    """

    # YOLO 17ê°œ í‚¤í¬ì¸íŠ¸ (COCO format)
    YOLO_KEYPOINTS = [
        'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',
        'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',
        'left_wrist', 'right_wrist', 'left_hip', 'right_hip',
        'left_knee', 'right_knee', 'left_ankle', 'right_ankle'
    ]

    def __init__(self, yolo_model_path: str = None):
        """
        Args:
            yolo_model_path: YOLO ëª¨ë¸ ê²½ë¡œ. Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        """
        if not YOLO_AVAILABLE:
            raise ImportError("ultralytics package required. Install: pip install ultralytics")

        # YOLO ëª¨ë¸ ë¡œë“œ (ì‹±ê¸€í†¤ ìºì‹±)
        if yolo_model_path is None:
            yolo_model_path = VERSION3_DIR / "yolo11s-pose.pt"

        if not os.path.exists(yolo_model_path):
            raise FileNotFoundError(f"YOLO model not found: {yolo_model_path}")

        # ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ YOLO ëª¨ë¸ ë¡œë“œ
        def load_yolo():
            print(f"  ğŸ”§ Loading YOLO11-pose from {os.path.basename(yolo_model_path)}...")
            return YOLO(yolo_model_path)

        self.yolo = model_cache.get_or_load("yolo_pose", load_yolo)

        # MediaPipe ì´ˆê¸°í™” (lazy loading)
        self.mp_pose = None
        self.mp_face = None
        self.mp_hands = None

        if MEDIAPIPE_AVAILABLE:
            self.mp = mp
            print("  âœ… MediaPipe available")
        else:
            print("  âš ï¸ MediaPipe not available - YOLO only mode")

    def _init_mediapipe_pose(self):
        """MediaPipe Pose ì´ˆê¸°í™” (í•„ìš”ì‹œ)"""
        if self.mp_pose is None and MEDIAPIPE_AVAILABLE:
            self.mp_pose = self.mp.solutions.pose.Pose(
                static_image_mode=True,
                model_complexity=2,
                min_detection_confidence=0.5
            )

    def _init_mediapipe_face(self):
        """MediaPipe Face Mesh ì´ˆê¸°í™” (í•„ìš”ì‹œ)"""
        if self.mp_face is None and MEDIAPIPE_AVAILABLE:
            self.mp_face = self.mp.solutions.face_mesh.FaceMesh(
                static_image_mode=True,
                max_num_faces=1,
                min_detection_confidence=0.5
            )

    def _init_mediapipe_hands(self):
        """MediaPipe Hands ì´ˆê¸°í™” (í•„ìš”ì‹œ)"""
        if self.mp_hands is None and MEDIAPIPE_AVAILABLE:
            self.mp_hands = self.mp.solutions.hands.Hands(
                static_image_mode=True,
                max_num_hands=2,
                min_detection_confidence=0.5
            )

    def analyze(self, image_path: str) -> Dict:
        """
        ì´ë¯¸ì§€ì—ì„œ í¬ì¦ˆ ì¶”ì¶œ (ì‹œë‚˜ë¦¬ì˜¤ ìë™ íŒë‹¨)

        Returns:
            {
                'scenario': 'full_body' | 'face_closeup' | 'hand_gesture' | 'back_view',
                'yolo_keypoints': [...],
                'mediapipe_pose': [...] (optional),
                'mediapipe_face': [...] (optional),
                'mediapipe_hands': [...] (optional),
                'merged_keypoints': {...},
                'confidence': float,
                'bbox': [x1, y1, x2, y2]
            }
        """
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"Image not found: {image_path}")

        # ì´ë¯¸ì§€ ë¡œë“œ
        img = cv2.imread(image_path)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        h, w = img.shape[:2]

        # Step 1: YOLO ì‹¤í–‰ (í•­ìƒ)
        yolo_result = self._run_yolo(img_rgb, h, w)

        if yolo_result is None or yolo_result['confidence'] < 0.3:
            return {
                'scenario': 'no_person',
                'yolo_keypoints': None,
                'merged_keypoints': None,
                'confidence': 0.0,
                'bbox': None
            }

        # Step 2: ì‹œë‚˜ë¦¬ì˜¤ íŒë‹¨
        scenario = self._detect_scenario(yolo_result, h, w)

        # Step 3: ì‹œë‚˜ë¦¬ì˜¤ë³„ MediaPipe ì¶”ê°€ ì‹¤í–‰
        result = {
            'scenario': scenario,
            'yolo_keypoints': yolo_result['keypoints'],
            'yolo_confidence': yolo_result['confidence'],
            'bbox': yolo_result['bbox']
        }

        if scenario == 'face_closeup' and MEDIAPIPE_AVAILABLE:
            self._init_mediapipe_face()
            mp_face_result = self._run_mediapipe_face(img_rgb)
            result['mediapipe_face'] = mp_face_result

        elif scenario == 'hand_gesture' and MEDIAPIPE_AVAILABLE:
            self._init_mediapipe_hands()
            mp_hands_result = self._run_mediapipe_hands(img_rgb)
            result['mediapipe_hands'] = mp_hands_result

        elif scenario in ['full_body', 'upper_body'] and MEDIAPIPE_AVAILABLE:
            self._init_mediapipe_pose()
            mp_pose_result = self._run_mediapipe_pose(img_rgb)
            result['mediapipe_pose'] = mp_pose_result

        # Step 4: í‚¤í¬ì¸íŠ¸ ë³‘í•©
        result['merged_keypoints'] = self._merge_keypoints(result)
        result['confidence'] = yolo_result['confidence']

        return result

    def _run_yolo(self, img_rgb: np.ndarray, h: int, w: int) -> Optional[Dict]:
        """YOLO í¬ì¦ˆ ê²€ì¶œ"""
        results = self.yolo(img_rgb, verbose=False)

        if len(results) == 0 or len(results[0].keypoints) == 0:
            return None

        # ì²« ë²ˆì§¸ ì‚¬ëŒë§Œ (ê°€ì¥ confidence ë†’ì€)
        result = results[0]

        if result.keypoints is None or len(result.keypoints.data) == 0:
            return None

        keypoints_data = result.keypoints.data[0]  # [17, 3] (x, y, conf)
        boxes = result.boxes.data[0]  # [x1, y1, x2, y2, conf, class]

        # ì •ê·œí™”ëœ ì¢Œí‘œë¡œ ë³€í™˜
        keypoints = []
        for i, kp_name in enumerate(self.YOLO_KEYPOINTS):
            x, y, conf = keypoints_data[i]
            keypoints.append({
                'name': kp_name,
                'x': float(x) / w,  # ì •ê·œí™” (0~1)
                'y': float(y) / h,
                'confidence': float(conf)
            })

        return {
            'keypoints': keypoints,
            'confidence': float(boxes[4]),
            'bbox': [float(boxes[0])/w, float(boxes[1])/h,
                     float(boxes[2])/w, float(boxes[3])/h]
        }

    def _detect_scenario(self, yolo_result: Dict, h: int, w: int) -> str:
        """
        ì‹œë‚˜ë¦¬ì˜¤ ìë™ íŒë‹¨

        Returns:
            'full_body' | 'upper_body' | 'face_closeup' | 'hand_gesture' | 'back_view'
        """
        bbox = yolo_result['bbox']
        keypoints = yolo_result['keypoints']

        # bbox í¬ê¸°
        bbox_width = bbox[2] - bbox[0]
        bbox_height = bbox[3] - bbox[1]
        bbox_area = bbox_width * bbox_height

        # í‚¤í¬ì¸íŠ¸ ì‹ ë¢°ë„
        kp_dict = {kp['name']: kp for kp in keypoints}

        # ì–¼êµ´ í‚¤í¬ì¸íŠ¸ ì‹ ë¢°ë„
        face_conf = np.mean([
            kp_dict['nose']['confidence'],
            kp_dict['left_eye']['confidence'],
            kp_dict['right_eye']['confidence']
        ])

        # ì† í‚¤í¬ì¸íŠ¸ ì‹ ë¢°ë„
        hand_conf = np.mean([
            kp_dict['left_wrist']['confidence'],
            kp_dict['right_wrist']['confidence']
        ])

        # í•˜ì²´ í‚¤í¬ì¸íŠ¸ ì‹ ë¢°ë„
        lower_body_conf = np.mean([
            kp_dict['left_knee']['confidence'],
            kp_dict['right_knee']['confidence'],
            kp_dict['left_ankle']['confidence'],
            kp_dict['right_ankle']['confidence']
        ])

        # íŒë‹¨ ë¡œì§
        if bbox_area > 0.4 and face_conf > 0.7:
            return 'face_closeup'
        elif hand_conf > 0.7 and bbox_height < 0.6:
            return 'hand_gesture'
        elif lower_body_conf > 0.5:
            return 'full_body'
        elif face_conf < 0.3:
            return 'back_view'
        else:
            return 'upper_body'

    def _run_mediapipe_pose(self, img_rgb: np.ndarray) -> Optional[Dict]:
        """MediaPipe Pose ì‹¤í–‰ (33 keypoints)"""
        if self.mp_pose is None:
            return None

        results = self.mp_pose.process(img_rgb)

        if results.pose_landmarks is None:
            return None

        keypoints = []
        for i, lm in enumerate(results.pose_landmarks.landmark):
            keypoints.append({
                'id': i,
                'x': lm.x,
                'y': lm.y,
                'z': lm.z,
                'visibility': lm.visibility
            })

        return {
            'keypoints': keypoints,
            'count': len(keypoints)
        }

    def _run_mediapipe_face(self, img_rgb: np.ndarray) -> Optional[Dict]:
        """MediaPipe Face Mesh ì‹¤í–‰ (468 keypoints)"""
        if self.mp_face is None:
            return None

        results = self.mp_face.process(img_rgb)

        if results.multi_face_landmarks is None or len(results.multi_face_landmarks) == 0:
            return None

        face_landmarks = results.multi_face_landmarks[0]
        keypoints = []
        for i, lm in enumerate(face_landmarks.landmark):
            keypoints.append({
                'id': i,
                'x': lm.x,
                'y': lm.y,
                'z': lm.z
            })

        # ì£¼ìš” í¬ì¸íŠ¸ë§Œ ì¶”ì¶œ (ëˆˆ, ì½”, ì…)
        key_indices = {
            'nose_tip': 1,
            'left_eye': 33,
            'right_eye': 263,
            'left_mouth': 61,
            'right_mouth': 291,
            'chin': 152
        }

        key_points = {}
        for name, idx in key_indices.items():
            key_points[name] = {
                'x': keypoints[idx]['x'],
                'y': keypoints[idx]['y'],
                'z': keypoints[idx]['z']
            }

        return {
            'keypoints': keypoints,
            'key_points': key_points,
            'count': len(keypoints)
        }

    def _run_mediapipe_hands(self, img_rgb: np.ndarray) -> Optional[Dict]:
        """MediaPipe Hands ì‹¤í–‰ (21 keypoints per hand)"""
        if self.mp_hands is None:
            return None

        results = self.mp_hands.process(img_rgb)

        if results.multi_hand_landmarks is None:
            return None

        hands = []
        for hand_idx, hand_landmarks in enumerate(results.multi_hand_landmarks):
            keypoints = []
            for i, lm in enumerate(hand_landmarks.landmark):
                keypoints.append({
                    'id': i,
                    'x': lm.x,
                    'y': lm.y,
                    'z': lm.z
                })

            handedness = results.multi_handedness[hand_idx].classification[0].label

            hands.append({
                'handedness': handedness,  # 'Left' or 'Right'
                'keypoints': keypoints,
                'count': len(keypoints)
            })

        return {
            'hands': hands,
            'hand_count': len(hands)
        }

    def _merge_keypoints(self, result: Dict) -> Dict:
        """
        YOLO + MediaPipe í‚¤í¬ì¸íŠ¸ ë³‘í•©

        Returns:
            {
                'base': {...},  # YOLO 17ê°œ (í•­ìƒ)
                'face': {...},  # MediaPipe Face (optional)
                'hands': {...}, # MediaPipe Hands (optional)
                'pose_33': {...} # MediaPipe Pose 33ê°œ (optional)
            }
        """
        merged = {
            'base': {}  # YOLO keypoints
        }

        # YOLO keypoints (base)
        for kp in result['yolo_keypoints']:
            merged['base'][kp['name']] = {
                'x': kp['x'],
                'y': kp['y'],
                'confidence': kp['confidence']
            }

        # MediaPipe Face
        if 'mediapipe_face' in result and result['mediapipe_face'] is not None:
            merged['face'] = result['mediapipe_face']['key_points']

        # MediaPipe Hands
        if 'mediapipe_hands' in result and result['mediapipe_hands'] is not None:
            merged['hands'] = result['mediapipe_hands']['hands']

        # MediaPipe Pose
        if 'mediapipe_pose' in result and result['mediapipe_pose'] is not None:
            merged['pose_33'] = {}
            for kp in result['mediapipe_pose']['keypoints']:
                merged['pose_33'][f'point_{kp["id"]}'] = {
                    'x': kp['x'],
                    'y': kp['y'],
                    'z': kp['z'],
                    'visibility': kp['visibility']
                }

        return merged


# ============================================================
# í¬ì¦ˆ ë¹„êµ í•¨ìˆ˜
# ============================================================

def compare_poses(ref_pose: Dict, user_pose: Dict) -> Dict:
    """
    ë ˆí¼ëŸ°ìŠ¤ vs ì‚¬ìš©ì í¬ì¦ˆ ë¹„êµ

    Returns:
        {
            'similarity': float (0~1),
            'angle_differences': {...},
            'position_differences': {...},
            'feedback': [...]
        }
    """
    if ref_pose is None or user_pose is None:
        return {
            'similarity': 0.0,
            'feedback': ['í¬ì¦ˆë¥¼ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤']
        }

    # ì‹œë‚˜ë¦¬ì˜¤ ì²´í¬
    if ref_pose['scenario'] != user_pose['scenario']:
        return {
            'similarity': 0.0,
            'feedback': [f"âš ï¸ í¬ì¦ˆ íƒ€ì…ì´ ë‹¤ë¦…ë‹ˆë‹¤ (ë ˆí¼ëŸ°ìŠ¤: {ref_pose['scenario']}, í˜„ì¬: {user_pose['scenario']})"]
        }

    ref_kp = ref_pose['merged_keypoints']['base']
    user_kp = user_pose['merged_keypoints']['base']

    # ê°ë„ ë¹„êµ
    angle_diffs = _compare_angles(ref_kp, user_kp)

    # ìœ„ì¹˜ ë¹„êµ
    position_diffs = _compare_positions(ref_kp, user_kp)

    # ìœ ì‚¬ë„ ê³„ì‚°
    similarity = _calculate_similarity(angle_diffs, position_diffs)

    # í”¼ë“œë°± ìƒì„±
    feedback = _generate_pose_feedback(angle_diffs, position_diffs, ref_kp, user_kp)

    return {
        'similarity': similarity,
        'angle_differences': angle_diffs,
        'position_differences': position_diffs,
        'feedback': feedback
    }


def _compare_angles(ref_kp: Dict, user_kp: Dict, conf_threshold: float = 0.5) -> Dict:
    """ì£¼ìš” ê´€ì ˆ ê°ë„ ë¹„êµ"""
    angles = {}

    # íŒ”ê¿ˆì¹˜ ê°ë„ (ì™¼ìª½)
    if all(k in ref_kp and ref_kp[k]['confidence'] > conf_threshold for k in ['left_shoulder', 'left_elbow', 'left_wrist']):
        ref_angle = _calculate_angle(
            ref_kp['left_shoulder'], ref_kp['left_elbow'], ref_kp['left_wrist']
        )
        user_angle = _calculate_angle(
            user_kp['left_shoulder'], user_kp['left_elbow'], user_kp['left_wrist']
        )
        angles['left_elbow'] = user_angle - ref_angle

    # íŒ”ê¿ˆì¹˜ ê°ë„ (ì˜¤ë¥¸ìª½)
    if all(k in ref_kp and ref_kp[k]['confidence'] > conf_threshold for k in ['right_shoulder', 'right_elbow', 'right_wrist']):
        ref_angle = _calculate_angle(
            ref_kp['right_shoulder'], ref_kp['right_elbow'], ref_kp['right_wrist']
        )
        user_angle = _calculate_angle(
            user_kp['right_shoulder'], user_kp['right_elbow'], user_kp['right_wrist']
        )
        angles['right_elbow'] = user_angle - ref_angle

    # ì–´ê¹¨ ê°ë„ (íŒ” ë“¤ì–´ì˜¬ë¦¼ ì •ë„)
    if all(k in ref_kp and ref_kp[k]['confidence'] > conf_threshold for k in ['left_shoulder', 'left_elbow', 'left_hip']):
        ref_angle = _calculate_angle(
            ref_kp['left_hip'], ref_kp['left_shoulder'], ref_kp['left_elbow']
        )
        user_angle = _calculate_angle(
            user_kp['left_hip'], user_kp['left_shoulder'], user_kp['left_elbow']
        )
        angles['left_shoulder'] = user_angle - ref_angle

    if all(k in ref_kp and ref_kp[k]['confidence'] > conf_threshold for k in ['right_shoulder', 'right_elbow', 'right_hip']):
        ref_angle = _calculate_angle(
            ref_kp['right_hip'], ref_kp['right_shoulder'], ref_kp['right_elbow']
        )
        user_angle = _calculate_angle(
            user_kp['right_hip'], user_kp['right_shoulder'], user_kp['right_elbow']
        )
        angles['right_shoulder'] = user_angle - ref_angle

    # ì–¼êµ´ ê°ë„ (ê³ ê°œ ì¢Œìš°)
    if all(k in ref_kp and ref_kp[k]['confidence'] > 0.5 for k in ['nose', 'left_eye', 'right_eye']):
        # ì½”ì™€ ì–‘ ëˆˆìœ¼ë¡œ ì–¼êµ´ ê°ë„ ê³„ì‚°
        ref_angle = _calculate_angle(
            ref_kp['left_eye'], ref_kp['nose'], ref_kp['right_eye']
        )
        user_angle = _calculate_angle(
            user_kp['left_eye'], user_kp['nose'], user_kp['right_eye']
        )
        angles['face_angle'] = user_angle - ref_angle

    return angles


def _compare_positions(ref_kp: Dict, user_kp: Dict, conf_threshold: float = 0.3) -> Dict:
    """ì£¼ìš” í‚¤í¬ì¸íŠ¸ ìƒëŒ€ ìœ„ì¹˜ ë¹„êµ"""
    positions = {}

    # ì†ëª© ë†’ì´ ë¹„êµ
    if 'left_wrist' in ref_kp and ref_kp['left_wrist']['confidence'] > conf_threshold:
        positions['left_wrist_y'] = user_kp['left_wrist']['y'] - ref_kp['left_wrist']['y']

    if 'right_wrist' in ref_kp and ref_kp['right_wrist']['confidence'] > conf_threshold:
        positions['right_wrist_y'] = user_kp['right_wrist']['y'] - ref_kp['right_wrist']['y']

    # ê³ ê°œ ê¸°ìš¸ê¸° (ê·€)
    if all(k in ref_kp and ref_kp[k]['confidence'] > 0.4 for k in ['left_ear', 'right_ear']):
        ref_head_tilt = (ref_kp['left_ear']['y'] - ref_kp['right_ear']['y'])
        user_head_tilt = (user_kp['left_ear']['y'] - user_kp['right_ear']['y'])
        positions['head_tilt'] = user_head_tilt - ref_head_tilt

    # ì½” ìœ„ì¹˜ (ì–¼êµ´ ìƒí•˜ ìœ„ì¹˜)
    if 'nose' in ref_kp and ref_kp['nose']['confidence'] > 0.7:
        positions['nose_y'] = user_kp['nose']['y'] - ref_kp['nose']['y']

    # ì–´ê¹¨ ë„ˆë¹„ ë¹„êµ
    if all(k in ref_kp and ref_kp[k]['confidence'] > 0.5 for k in ['left_shoulder', 'right_shoulder']):
        ref_shoulder_width = abs(ref_kp['left_shoulder']['x'] - ref_kp['right_shoulder']['x'])
        user_shoulder_width = abs(user_kp['left_shoulder']['x'] - user_kp['right_shoulder']['x'])
        positions['shoulder_width'] = user_shoulder_width - ref_shoulder_width

    return positions


def _calculate_angle(p1: Dict, p2: Dict, p3: Dict) -> float:
    """3ì ìœ¼ë¡œ ê°ë„ ê³„ì‚° (p2ê°€ ê¼­ì§“ì )"""
    v1 = np.array([p1['x'] - p2['x'], p1['y'] - p2['y']])
    v2 = np.array([p3['x'] - p2['x'], p3['y'] - p2['y']])

    cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
    angle = np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))

    return float(angle)


def _calculate_similarity(angle_diffs: Dict, position_diffs: Dict) -> float:
    """ì „ì²´ ìœ ì‚¬ë„ ê³„ì‚° (0~1)"""
    if not angle_diffs and not position_diffs:
        return 0.0

    # ê°ë„ ì°¨ì´ ì ìˆ˜
    angle_scores = []
    for diff in angle_diffs.values():
        score = max(0, 1 - abs(diff) / 90.0)  # 90ë„ ì´ìƒ ì°¨ì´ë©´ 0ì 
        angle_scores.append(score)

    # ìœ„ì¹˜ ì°¨ì´ ì ìˆ˜
    position_scores = []
    for diff in position_diffs.values():
        score = max(0, 1 - abs(diff) / 0.5)  # 50% ì´ìƒ ì°¨ì´ë©´ 0ì 
        position_scores.append(score)

    all_scores = angle_scores + position_scores

    if not all_scores:
        return 0.0

    return float(np.mean(all_scores))


def _generate_pose_feedback(angle_diffs: Dict, position_diffs: Dict,
                           ref_kp: Dict, user_kp: Dict) -> List[str]:
    """êµ¬ì²´ì ì¸ í¬ì¦ˆ í”¼ë“œë°± ìƒì„±"""
    feedback = []

    # ê°ë„ í”¼ë“œë°± (ì„ê³„ê°’ ë†’ì—¬ì„œ ì•ˆì •í™”)
    if 'left_elbow' in angle_diffs and abs(angle_diffs['left_elbow']) > 25:  # 15 -> 25
        if angle_diffs['left_elbow'] > 0:
            feedback.append(f"ì™¼íŒ” íŒ”ê¿ˆì¹˜ë¥¼ {abs(angle_diffs['left_elbow']):.0f}ë„ ë” í´ì„¸ìš”")
        else:
            feedback.append(f"ì™¼íŒ” íŒ”ê¿ˆì¹˜ë¥¼ {abs(angle_diffs['left_elbow']):.0f}ë„ ë” êµ¬ë¶€ë¦¬ì„¸ìš”")

    if 'right_elbow' in angle_diffs and abs(angle_diffs['right_elbow']) > 25:  # 15 -> 25
        if angle_diffs['right_elbow'] > 0:
            feedback.append(f"ì˜¤ë¥¸íŒ” íŒ”ê¿ˆì¹˜ë¥¼ {abs(angle_diffs['right_elbow']):.0f}ë„ ë” í´ì„¸ìš”")
        else:
            feedback.append(f"ì˜¤ë¥¸íŒ” íŒ”ê¿ˆì¹˜ë¥¼ {abs(angle_diffs['right_elbow']):.0f}ë„ ë” êµ¬ë¶€ë¦¬ì„¸ìš”")

    if 'left_shoulder' in angle_diffs and abs(angle_diffs['left_shoulder']) > 30:  # 20 -> 30
        if angle_diffs['left_shoulder'] > 0:
            feedback.append(f"ì™¼íŒ”ì„ {abs(angle_diffs['left_shoulder']):.0f}ë„ ë” ì˜¬ë¦¬ì„¸ìš”")
        else:
            feedback.append(f"ì™¼íŒ”ì„ {abs(angle_diffs['left_shoulder']):.0f}ë„ ë” ë‚´ë¦¬ì„¸ìš”")

    if 'right_shoulder' in angle_diffs and abs(angle_diffs['right_shoulder']) > 30:  # 20 -> 30
        if angle_diffs['right_shoulder'] > 0:
            feedback.append(f"ì˜¤ë¥¸íŒ”ì„ {abs(angle_diffs['right_shoulder']):.0f}ë„ ë” ì˜¬ë¦¬ì„¸ìš”")
        else:
            feedback.append(f"ì˜¤ë¥¸íŒ”ì„ {abs(angle_diffs['right_shoulder']):.0f}ë„ ë” ë‚´ë¦¬ì„¸ìš”")

    # ì–¼êµ´ ê°ë„
    if 'face_angle' in angle_diffs and abs(angle_diffs['face_angle']) > 5:
        if angle_diffs['face_angle'] > 0:
            feedback.append(f"ì–¼êµ´ì„ {abs(angle_diffs['face_angle']):.0f}ë„ ë” ì™¼ìª½ìœ¼ë¡œ ëŒë¦¬ì„¸ìš”")
        else:
            feedback.append(f"ì–¼êµ´ì„ {abs(angle_diffs['face_angle']):.0f}ë„ ë” ì˜¤ë¥¸ìª½ìœ¼ë¡œ ëŒë¦¬ì„¸ìš”")

    # ìœ„ì¹˜ í”¼ë“œë°±
    if 'left_wrist_y' in position_diffs and abs(position_diffs['left_wrist_y']) > 0.1:
        if position_diffs['left_wrist_y'] > 0:
            feedback.append(f"ì™¼ì†ì„ í™”ë©´ ê¸°ì¤€ ìœ„ìª½ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”")
        else:
            feedback.append(f"ì™¼ì†ì„ í™”ë©´ ê¸°ì¤€ ì•„ë˜ìª½ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”")

    if 'right_wrist_y' in position_diffs and abs(position_diffs['right_wrist_y']) > 0.1:
        if position_diffs['right_wrist_y'] > 0:
            feedback.append(f"ì˜¤ë¥¸ì†ì„ í™”ë©´ ê¸°ì¤€ ìœ„ìª½ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”")
        else:
            feedback.append(f"ì˜¤ë¥¸ì†ì„ í™”ë©´ ê¸°ì¤€ ì•„ë˜ìª½ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”")

    if 'head_tilt' in position_diffs and abs(position_diffs['head_tilt']) > 0.05:
        if position_diffs['head_tilt'] > 0:
            feedback.append("ê³ ê°œë¥¼ ì™¼ìª½ìœ¼ë¡œ ê¸°ìš¸ì´ì„¸ìš”")
        else:
            feedback.append("ê³ ê°œë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê¸°ìš¸ì´ì„¸ìš”")

    if 'nose_y' in position_diffs and abs(position_diffs['nose_y']) > 0.08:
        if position_diffs['nose_y'] > 0:
            feedback.append("ê³ ê°œë¥¼ ìœ„ë¡œ ë“¤ì–´ ì˜¬ë¦¬ì„¸ìš”")
        else:
            feedback.append("ê³ ê°œë¥¼ ì•„ë˜ë¡œ ìˆ™ì´ì„¸ìš”")

    if 'shoulder_width' in position_diffs and abs(position_diffs['shoulder_width']) > 0.1:
        if position_diffs['shoulder_width'] > 0:
            feedback.append("ì–´ê¹¨ë¥¼ ì¢í˜€ì£¼ì„¸ìš” (ì •ë©´ì„ ë” í–¥í•˜ì„¸ìš”)")
        else:
            feedback.append("ì–´ê¹¨ë¥¼ í´ì£¼ì„¸ìš” (ì¸¡ë©´ì„ ë” í–¥í•˜ì„¸ìš”)")

    if not feedback:
        feedback.append("âœ… í¬ì¦ˆê°€ ì ì ˆí•©ë‹ˆë‹¤")

    return feedback


# ============================================================
# í…ŒìŠ¤íŠ¸
# ============================================================
if __name__ == "__main__":
    test_img = PROJECT_ROOT / "data" / "test_images" / "test1.jpg"
    
    try:
        analyzer = PoseAnalyzer()
        result = analyzer.analyze(str(test_img))

        print("\n" + "="*60)
        print("ğŸ¤¸ POSE ANALYSIS RESULT")
        print("="*60)

        print(f"\nğŸ“‹ Scenario: {result['scenario']}")
        print(f"ğŸ¯ Confidence: {result['confidence']:.2f}")
        print(f"ğŸ“¦ BBox: {result['bbox']}")

        print(f"\nğŸ¦´ YOLO Keypoints (17):")
        for kp in result['yolo_keypoints'][:5]:  # ìƒìœ„ 5ê°œë§Œ
            print(f"  - {kp['name']}: ({kp['x']:.3f}, {kp['y']:.3f}) conf={kp['confidence']:.2f}")
        print("  ...")

        if 'mediapipe_face' in result:
            print(f"\nğŸ˜Š MediaPipe Face: {result['mediapipe_face']['count']} keypoints")

        if 'mediapipe_hands' in result:
            print(f"\nğŸ¤š MediaPipe Hands: {result['mediapipe_hands']['hand_count']} hands")

        if 'mediapipe_pose' in result:
            print(f"\nğŸ§ MediaPipe Pose: {result['mediapipe_pose']['count']} keypoints")

        print("\n" + "="*60)

    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
