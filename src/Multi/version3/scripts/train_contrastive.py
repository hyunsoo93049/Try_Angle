# ============================================================
# ğŸ“ Contrastive Learning Training Script
# Phase 3-3: ëŒ€ì¡° í•™ìŠµ ëª¨ë¸ í›ˆë ¨
# ============================================================

import os
import sys
import json
import time
from pathlib import Path
from typing import Dict, Tuple
import numpy as np

# PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image

# Project root ì„¤ì •
VERSION3_DIR = Path(__file__).resolve().parents[1]
PROJECT_ROOT = VERSION3_DIR
while PROJECT_ROOT != PROJECT_ROOT.parent and not ((PROJECT_ROOT / "data").exists() and (PROJECT_ROOT / "src").exists()):
    PROJECT_ROOT = PROJECT_ROOT.parent

if str(VERSION3_DIR) not in sys.path:
    sys.path.append(str(VERSION3_DIR))

from contrastive.contrastive_model import create_contrastive_model, BinaryContrastiveLoss


# ============================================================
# Dataset
# ============================================================

class ContrastiveDataset(Dataset):
    """
    ëŒ€ì¡° í•™ìŠµ ë°ì´í„°ì…‹

    pairs.jsonì—ì„œ ì´ë¯¸ì§€ ìŒì„ ë¡œë“œí•˜ê³  augmentation ì ìš©
    """

    def __init__(self, pairs_json_path: str, transform=None):
        """
        Args:
            pairs_json_path: pairs.json íŒŒì¼ ê²½ë¡œ
            transform: ì´ë¯¸ì§€ ë³€í™˜ (augmentation)
        """
        with open(pairs_json_path, 'r') as f:
            self.pairs = json.load(f)

        self.transform = transform

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        pair = self.pairs[idx]

        # ì´ë¯¸ì§€ ë¡œë“œ
        img1 = Image.open(pair['img1']).convert('RGB')
        img2 = Image.open(pair['img2']).convert('RGB')

        # Transform ì ìš©
        if self.transform:
            img1 = self.transform(img1)
            img2 = self.transform(img2)

        label = pair['label']

        return img1, img2, label


# ============================================================
# Data Augmentation
# ============================================================

def get_train_transform(image_size: int = 224):
    """
    í•™ìŠµìš© ì´ë¯¸ì§€ augmentation

    Args:
        image_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°

    Returns:
        torchvision transforms
    """
    return transforms.Compose([
        transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),
        transforms.RandomGrayscale(p=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])


def get_val_transform(image_size: int = 224):
    """
    ê²€ì¦ìš© ì´ë¯¸ì§€ transform (augmentation ì—†ìŒ)

    Args:
        image_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°

    Returns:
        torchvision transforms
    """
    return transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])


# ============================================================
# Training Loop
# ============================================================

def train_one_epoch(
    model: nn.Module,
    dataloader: DataLoader,
    criterion: nn.Module,
    optimizer: optim.Optimizer,
    device: torch.device,
    epoch: int
) -> Dict:
    """
    1 epoch í•™ìŠµ

    Args:
        model: ëŒ€ì¡° í•™ìŠµ ëª¨ë¸
        dataloader: í•™ìŠµ ë°ì´í„°ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        optimizer: ì˜µí‹°ë§ˆì´ì €
        device: ë””ë°”ì´ìŠ¤ (cuda/cpu)
        epoch: í˜„ì¬ epoch

    Returns:
        í•™ìŠµ í†µê³„
    """
    model.train()

    total_loss = 0.0
    num_batches = 0

    start_time = time.time()

    for batch_idx, (img1, img2, labels) in enumerate(dataloader):
        # Move to device
        img1 = img1.to(device)
        img2 = img2.to(device)
        labels = labels.float().to(device)

        # Forward pass
        _, emb1 = model(img1)
        _, emb2 = model(img2)

        # Compute loss
        loss = criterion(emb1, emb2, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Statistics
        total_loss += loss.item()
        num_batches += 1

        # Progress
        if (batch_idx + 1) % 10 == 0:
            print(f"   [{batch_idx + 1}/{len(dataloader)}] Loss: {loss.item():.4f}")

    elapsed = time.time() - start_time
    avg_loss = total_loss / num_batches

    return {
        'loss': avg_loss,
        'time': elapsed
    }


def validate(
    model: nn.Module,
    dataloader: DataLoader,
    criterion: nn.Module,
    device: torch.device
) -> Dict:
    """
    ê²€ì¦

    Args:
        model: ëŒ€ì¡° í•™ìŠµ ëª¨ë¸
        dataloader: ê²€ì¦ ë°ì´í„°ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        device: ë””ë°”ì´ìŠ¤

    Returns:
        ê²€ì¦ í†µê³„
    """
    model.eval()

    total_loss = 0.0
    num_batches = 0

    # ì •í™•ë„ ê³„ì‚°ì„ ìœ„í•œ ë³€ìˆ˜
    correct_positive = 0
    total_positive = 0
    correct_negative = 0
    total_negative = 0

    with torch.no_grad():
        for img1, img2, labels in dataloader:
            # Move to device
            img1 = img1.to(device)
            img2 = img2.to(device)
            labels = labels.float().to(device)

            # Forward pass
            _, emb1 = model(img1)
            _, emb2 = model(img2)

            # Compute loss
            loss = criterion(emb1, emb2, labels)

            total_loss += loss.item()
            num_batches += 1

            # Accuracy: ê±°ë¦¬ ê¸°ë°˜ ë¶„ë¥˜
            distances = torch.pairwise_distance(emb1, emb2, p=2)
            predictions = (distances < 0.5).float()  # 0.5 threshold

            # Positive/Negative ì •í™•ë„
            positive_mask = (labels == 1)
            negative_mask = (labels == 0)

            if positive_mask.sum() > 0:
                correct_positive += (predictions[positive_mask] == 1).sum().item()
                total_positive += positive_mask.sum().item()

            if negative_mask.sum() > 0:
                correct_negative += (predictions[negative_mask] == 0).sum().item()
                total_negative += negative_mask.sum().item()

    avg_loss = total_loss / num_batches
    positive_acc = correct_positive / total_positive if total_positive > 0 else 0
    negative_acc = correct_negative / total_negative if total_negative > 0 else 0
    overall_acc = (correct_positive + correct_negative) / (total_positive + total_negative)

    return {
        'loss': avg_loss,
        'positive_acc': positive_acc,
        'negative_acc': negative_acc,
        'overall_acc': overall_acc
    }


# ============================================================
# Main Training Function
# ============================================================

def train_contrastive_model(
    dataset_dir: str,
    output_dir: str,
    backbone: str = "resnet50",
    batch_size: int = 32,
    num_epochs: int = 50,
    learning_rate: float = 1e-4,
    weight_decay: float = 1e-4,
    freeze_backbone: bool = False,
    device: str = "cuda"
):
    """
    ëŒ€ì¡° í•™ìŠµ ëª¨ë¸ í›ˆë ¨

    Args:
        dataset_dir: ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ (train/pairs.json, val/pairs.json í¬í•¨)
        output_dir: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬
        backbone: ë°±ë³¸ ëª¨ë¸ ("resnet50", "resnet101", "efficientnet_b0")
        batch_size: ë°°ì¹˜ ì‚¬ì´ì¦ˆ
        num_epochs: í•™ìŠµ epoch ìˆ˜
        learning_rate: í•™ìŠµë¥ 
        weight_decay: Weight decay (L2 regularization)
        freeze_backbone: ë°±ë³¸ ê³ ì • ì—¬ë¶€
        device: ë””ë°”ì´ìŠ¤ ("cuda" or "cpu")
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“ Contrastive Learning Training")
    print(f"{'='*60}")
    print(f"Dataset: {dataset_dir}")
    print(f"Output: {output_dir}")
    print(f"Backbone: {backbone}")
    print(f"Batch size: {batch_size}")
    print(f"Epochs: {num_epochs}")
    print(f"Learning rate: {learning_rate}")
    print(f"Device: {device}\n")

    # Device ì„¤ì •
    device = torch.device(device if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}\n")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Dataset ë¡œë“œ
    print("ğŸ“‚ Loading datasets...")
    train_dataset = ContrastiveDataset(
        pairs_json_path=Path(dataset_dir) / "train" / "pairs.json",
        transform=get_train_transform()
    )
    val_dataset = ContrastiveDataset(
        pairs_json_path=Path(dataset_dir) / "val" / "pairs.json",
        transform=get_val_transform()
    )

    print(f"  âœ… Train: {len(train_dataset)} pairs")
    print(f"  âœ… Val: {len(val_dataset)} pairs")

    # DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,  # Windows compatibility
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,  # Windows compatibility
        pin_memory=True
    )

    # ëª¨ë¸ ìƒì„±
    print(f"\nğŸ”§ Creating model...")
    model = create_contrastive_model(
        backbone=backbone,
        pretrained=True,
        freeze_backbone=freeze_backbone,
        projection_dim=128
    )
    model = model.to(device)
    print(f"  âœ… Model created: {backbone}")

    # Loss & Optimizer
    criterion = BinaryContrastiveLoss(margin=1.0)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

    # Learning rate scheduler
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_positive_acc': [],
        'val_negative_acc': [],
        'val_overall_acc': []
    }

    best_val_loss = float('inf')

    # Training loop
    print(f"\n{'='*60}")
    print(f"ğŸš€ Training started...")
    print(f"{'='*60}\n")

    for epoch in range(num_epochs):
        print(f"Epoch {epoch + 1}/{num_epochs}")
        print("-" * 60)

        # Train
        train_stats = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch)
        print(f"  Train Loss: {train_stats['loss']:.4f} ({train_stats['time']:.1f}s)")

        # Validate
        val_stats = validate(model, val_loader, criterion, device)
        print(f"  Val Loss: {val_stats['loss']:.4f}")
        print(f"  Val Accuracy: {val_stats['overall_acc']:.2%} (Pos: {val_stats['positive_acc']:.2%}, Neg: {val_stats['negative_acc']:.2%})")

        # Update scheduler
        scheduler.step()

        # Save history
        history['train_loss'].append(train_stats['loss'])
        history['val_loss'].append(val_stats['loss'])
        history['val_positive_acc'].append(val_stats['positive_acc'])
        history['val_negative_acc'].append(val_stats['negative_acc'])
        history['val_overall_acc'].append(val_stats['overall_acc'])

        # Save best model
        if val_stats['loss'] < best_val_loss:
            best_val_loss = val_stats['loss']
            checkpoint_path = output_dir / "best_model.pth"
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_stats['loss'],
                'val_acc': val_stats['overall_acc']
            }, checkpoint_path)
            print(f"  ğŸ’¾ Best model saved! (val_loss: {best_val_loss:.4f})")

        # Save checkpoint every 10 epochs
        if (epoch + 1) % 10 == 0:
            checkpoint_path = output_dir / f"checkpoint_epoch_{epoch + 1}.pth"
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
            }, checkpoint_path)
            print(f"  ğŸ’¾ Checkpoint saved: epoch {epoch + 1}")

        print()

    # Save final model
    final_path = output_dir / "final_model.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'history': history
    }, final_path)

    # Save history
    history_path = output_dir / "training_history.json"
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)

    print(f"\n{'='*60}")
    print(f"âœ… Training Complete!")
    print(f"{'='*60}")
    print(f"\nğŸ’¾ Models saved to: {output_dir}")
    print(f"   best_model.pth: Best validation loss ({best_val_loss:.4f})")
    print(f"   final_model.pth: Final model")
    print(f"   training_history.json: Training metrics")

    return model, history


# ============================================================
# ì‹¤í–‰
# ============================================================

if __name__ == "__main__":
    # ì„¤ì •
    dataset_dir = PROJECT_ROOT / "data" / "contrastive_dataset"
    output_dir = VERSION3_DIR / "models" / "contrastive"

    if not dataset_dir.exists():
        print(f"âŒ Dataset not found: {dataset_dir}")
        print("Run prepare_contrastive_data.py first!")
        sys.exit(1)

    try:
        model, history = train_contrastive_model(
            dataset_dir=str(dataset_dir),
            output_dir=str(output_dir),
            backbone="resnet50",
            batch_size=32,
            num_epochs=50,
            learning_rate=1e-4,
            weight_decay=1e-4,
            freeze_backbone=False,
            device="cuda"
        )

    except Exception as e:
        print(f"\nâŒ Error during training: {e}")
        import traceback
        traceback.print_exc()
