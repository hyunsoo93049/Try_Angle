# ============================================================
# ğŸ¯ Contrastive Learning Model
# Phase 3-2: PyTorch ê¸°ë°˜ ëŒ€ì¡° í•™ìŠµ ëª¨ë¸
# ============================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple
import torchvision.models as models


# ============================================================
# Encoder Network
# ============================================================

class ImageEncoder(nn.Module):
    """
    ì´ë¯¸ì§€ ì¸ì½”ë” (ResNet50 ê¸°ë°˜)

    ì‚¬ì§„ ìŠ¤íƒ€ì¼ì„ ì¸ì‹í•˜ê¸° ìœ„í•œ feature extraction
    """

    def __init__(self, backbone: str = "resnet50", pretrained: bool = True, freeze_backbone: bool = False):
        """
        Args:
            backbone: ë°±ë³¸ ëª¨ë¸ ("resnet50", "resnet101", "efficientnet_b0")
            pretrained: ImageNet pretrained weights ì‚¬ìš©
            freeze_backbone: ë°±ë³¸ ë ˆì´ì–´ ê³ ì • (transfer learning)
        """
        super().__init__()

        self.backbone_name = backbone

        # ë°±ë³¸ ì„ íƒ
        if backbone == "resnet50":
            resnet = models.resnet50(pretrained=pretrained)
            self.encoder = nn.Sequential(*list(resnet.children())[:-1])  # FC ë ˆì´ì–´ ì œê±°
            self.feature_dim = 2048

        elif backbone == "resnet101":
            resnet = models.resnet101(pretrained=pretrained)
            self.encoder = nn.Sequential(*list(resnet.children())[:-1])
            self.feature_dim = 2048

        elif backbone == "efficientnet_b0":
            efficientnet = models.efficientnet_b0(pretrained=pretrained)
            self.encoder = nn.Sequential(*list(efficientnet.children())[:-1])
            self.feature_dim = 1280

        else:
            raise ValueError(f"Unknown backbone: {backbone}")

        # ë°±ë³¸ ê³ ì • ì˜µì…˜
        if freeze_backbone:
            for param in self.encoder.parameters():
                param.requires_grad = False

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (B, 3, H, W) ì´ë¯¸ì§€ í…ì„œ

        Returns:
            (B, feature_dim) feature ë²¡í„°
        """
        features = self.encoder(x)  # (B, feature_dim, 1, 1)
        features = features.flatten(1)  # (B, feature_dim)
        return features


# ============================================================
# Projection Head
# ============================================================

class ProjectionHead(nn.Module):
    """
    Projection Head (MLP)

    Feature spaceë¥¼ contrastive learningì— ì í•©í•œ embedding spaceë¡œ ë³€í™˜
    SimCLR ë…¼ë¬¸ ì°¸ê³ : 2-layer MLP with ReLU
    """

    def __init__(self, input_dim: int = 2048, hidden_dim: int = 512, output_dim: int = 128):
        """
        Args:
            input_dim: ì¸ì½”ë” ì¶œë ¥ ì°¨ì›
            hidden_dim: íˆë“  ë ˆì´ì–´ ì°¨ì›
            output_dim: ìµœì¢… embedding ì°¨ì›
        """
        super().__init__()

        self.projection = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (B, input_dim) feature ë²¡í„°

        Returns:
            (B, output_dim) embedding ë²¡í„°
        """
        return self.projection(x)


# ============================================================
# Complete Contrastive Model
# ============================================================

class ContrastiveModel(nn.Module):
    """
    ì™„ì „í•œ ëŒ€ì¡° í•™ìŠµ ëª¨ë¸

    Encoder + Projection Head
    """

    def __init__(
        self,
        backbone: str = "resnet50",
        pretrained: bool = True,
        freeze_backbone: bool = False,
        projection_dim: int = 128
    ):
        """
        Args:
            backbone: ë°±ë³¸ ëª¨ë¸ ì´ë¦„
            pretrained: ImageNet pretrained weights ì‚¬ìš©
            freeze_backbone: ë°±ë³¸ ê³ ì • ì—¬ë¶€
            projection_dim: ìµœì¢… embedding ì°¨ì›
        """
        super().__init__()

        # Encoder
        self.encoder = ImageEncoder(backbone, pretrained, freeze_backbone)

        # Projection Head
        self.projection_head = ProjectionHead(
            input_dim=self.encoder.feature_dim,
            hidden_dim=512,
            output_dim=projection_dim
        )

        self.projection_dim = projection_dim

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x: (B, 3, H, W) ì´ë¯¸ì§€ í…ì„œ

        Returns:
            features: (B, feature_dim) encoder ì¶œë ¥
            embeddings: (B, projection_dim) projection head ì¶œë ¥
        """
        features = self.encoder(x)
        embeddings = self.projection_head(features)

        # L2 ì •ê·œí™” (cosine similarity ì‚¬ìš©ì„ ìœ„í•´)
        embeddings = F.normalize(embeddings, dim=1)

        return features, embeddings

    def get_embeddings(self, x: torch.Tensor) -> torch.Tensor:
        """
        ì¶”ë¡  ì‹œ ì‚¬ìš©: embeddingë§Œ ë°˜í™˜

        Args:
            x: (B, 3, H, W) ì´ë¯¸ì§€ í…ì„œ

        Returns:
            (B, projection_dim) ì •ê·œí™”ëœ embedding
        """
        _, embeddings = self.forward(x)
        return embeddings


# ============================================================
# Contrastive Loss (InfoNCE / NT-Xent)
# ============================================================

class ContrastiveLoss(nn.Module):
    """
    InfoNCE Loss (Normalized Temperature-scaled Cross Entropy)

    SimCLR, MoCo ë“±ì—ì„œ ì‚¬ìš©ë˜ëŠ” ëŒ€ì¡° í•™ìŠµ ì†ì‹¤ í•¨ìˆ˜
    """

    def __init__(self, temperature: float = 0.07):
        """
        Args:
            temperature: ì˜¨ë„ íŒŒë¼ë¯¸í„° (ì‘ì„ìˆ˜ë¡ hard negative mining)
        """
        super().__init__()
        self.temperature = temperature

    def forward(self, embeddings1: torch.Tensor, embeddings2: torch.Tensor) -> torch.Tensor:
        """
        Args:
            embeddings1: (B, D) ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì˜ embedding
            embeddings2: (B, D) ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ì˜ embedding

        Returns:
            loss: scalar ì†ì‹¤ ê°’
        """
        batch_size = embeddings1.shape[0]
        device = embeddings1.device

        # Concatenate embeddings: (2B, D)
        embeddings = torch.cat([embeddings1, embeddings2], dim=0)

        # Compute similarity matrix: (2B, 2B)
        similarity_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature

        # Create labels (positive pairs)
        # [0, 1], [1, 0], [2, 3], [3, 2], ...
        labels = torch.arange(batch_size, device=device)
        labels = torch.cat([labels + batch_size, labels])  # (2B,)

        # Mask out self-similarity
        mask = torch.eye(2 * batch_size, device=device, dtype=torch.bool)
        similarity_matrix = similarity_matrix.masked_fill(mask, -1e9)

        # Cross-entropy loss
        loss = F.cross_entropy(similarity_matrix, labels)

        return loss


class BinaryContrastiveLoss(nn.Module):
    """
    Binary Contrastive Loss

    Positive pair (label=1): ê±°ë¦¬ ìµœì†Œí™”
    Negative pair (label=0): ê±°ë¦¬ ìµœëŒ€í™”
    """

    def __init__(self, margin: float = 1.0):
        """
        Args:
            margin: negative pairì˜ ìµœì†Œ ê±°ë¦¬
        """
        super().__init__()
        self.margin = margin

    def forward(self, embeddings1: torch.Tensor, embeddings2: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """
        Args:
            embeddings1: (B, D) ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì˜ embedding
            embeddings2: (B, D) ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ì˜ embedding
            labels: (B,) 0 (negative) or 1 (positive)

        Returns:
            loss: scalar ì†ì‹¤ ê°’
        """
        # Euclidean distance
        distances = F.pairwise_distance(embeddings1, embeddings2)

        # Positive loss: minimize distance
        positive_loss = labels * torch.pow(distances, 2)

        # Negative loss: maximize distance (up to margin)
        negative_loss = (1 - labels) * torch.pow(torch.clamp(self.margin - distances, min=0.0), 2)

        # Total loss
        loss = torch.mean(positive_loss + negative_loss)

        return loss


# ============================================================
# ëª¨ë¸ ìƒì„± í—¬í¼ í•¨ìˆ˜
# ============================================================

def create_contrastive_model(
    backbone: str = "resnet50",
    pretrained: bool = True,
    freeze_backbone: bool = False,
    projection_dim: int = 128
) -> ContrastiveModel:
    """
    ëŒ€ì¡° í•™ìŠµ ëª¨ë¸ ìƒì„±

    Args:
        backbone: ë°±ë³¸ ëª¨ë¸ ("resnet50", "resnet101", "efficientnet_b0")
        pretrained: ImageNet pretrained weights ì‚¬ìš©
        freeze_backbone: ë°±ë³¸ ê³ ì • (transfer learning)
        projection_dim: ìµœì¢… embedding ì°¨ì›

    Returns:
        ContrastiveModel ì¸ìŠ¤í„´ìŠ¤
    """
    model = ContrastiveModel(
        backbone=backbone,
        pretrained=pretrained,
        freeze_backbone=freeze_backbone,
        projection_dim=projection_dim
    )

    return model


# ============================================================
# í…ŒìŠ¤íŠ¸
# ============================================================

if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸ§ª Contrastive Model Test")
    print("="*60)

    # ëª¨ë¸ ìƒì„±
    model = create_contrastive_model(backbone="resnet50", pretrained=False)

    # ë”ë¯¸ ì…ë ¥
    batch_size = 4
    images = torch.randn(batch_size, 3, 224, 224)

    # Forward pass
    features, embeddings = model(images)

    print(f"\nâœ… Model created successfully!")
    print(f"   Backbone: ResNet50")
    print(f"   Features shape: {features.shape}")
    print(f"   Embeddings shape: {embeddings.shape}")

    # Loss í…ŒìŠ¤íŠ¸
    loss_fn = ContrastiveLoss(temperature=0.07)

    images1 = torch.randn(batch_size, 3, 224, 224)
    images2 = torch.randn(batch_size, 3, 224, 224)

    _, emb1 = model(images1)
    _, emb2 = model(images2)

    loss = loss_fn(emb1, emb2)

    print(f"\nâœ… Loss computed successfully!")
    print(f"   Loss: {loss.item():.4f}")

    # Binary loss í…ŒìŠ¤íŠ¸
    binary_loss_fn = BinaryContrastiveLoss(margin=1.0)
    labels = torch.tensor([1, 1, 0, 0])  # positive, positive, negative, negative
    binary_loss = binary_loss_fn(emb1, emb2, labels)

    print(f"\nâœ… Binary loss computed successfully!")
    print(f"   Binary Loss: {binary_loss.item():.4f}")

    print("\n" + "="*60)
    print("âœ… All tests passed!")
    print("="*60)
