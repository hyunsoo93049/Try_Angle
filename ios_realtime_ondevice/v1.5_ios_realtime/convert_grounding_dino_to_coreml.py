"""
Grounding DINO to CoreML Conversion Script
ì‘ì„±ì¼: 2025-12-05

Grounding DINO ëª¨ë¸ì„ iOSì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” CoreML í˜•ì‹ìœ¼ë¡œ ë³€í™˜
"""

import torch
import coremltools as ct
from PIL import Image
import numpy as np

def convert_grounding_dino_to_coreml():
    """
    Grounding DINOë¥¼ CoreMLë¡œ ë³€í™˜

    ì£¼ì˜ì‚¬í•­:
    1. í…ìŠ¤íŠ¸ ì¸ì½”ë”ì™€ ì´ë¯¸ì§€ ì¸ì½”ë”ë¥¼ ë¶„ë¦¬í•˜ì—¬ ë³€í™˜
    2. ëª¨ë¸ í¬ê¸° ìµœì í™”ë¥¼ ìœ„í•´ quantization ì ìš©
    3. iOS 15+ íƒ€ê²Ÿ
    """

    print("ğŸ”„ Grounding DINO CoreML ë³€í™˜ ì‹œì‘...")

    try:
        # 1. Grounding DINO ëª¨ë¸ ë¡œë“œ
        # ì‹¤ì œ êµ¬í˜„ì‹œ groundingdino íŒ¨í‚¤ì§€ í•„ìš”
        from groundingdino.models import build_model
        from groundingdino.util.slconfig import SLConfig

        # ì„¤ì • íŒŒì¼ ê²½ë¡œ
        config_file = "groundingdino/config/GroundingDINO_SwinT_OGC.py"
        checkpoint = "weights/groundingdino_swint_ogc.pth"

        # ëª¨ë¸ ë¹Œë“œ
        args = SLConfig.fromfile(config_file)
        model = build_model(args)
        checkpoint = torch.load(checkpoint, map_location="cpu")
        model.load_state_dict(checkpoint["model"], strict=False)
        model.eval()

        print("âœ… Grounding DINO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    except ImportError:
        print("âš ï¸ Grounding DINO íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì²´ ë°©ë²• ì‚¬ìš©...")
        # ëŒ€ì²´: DETR ê¸°ë°˜ ê°„ë‹¨í•œ ê°ì²´ ê²€ì¶œ ëª¨ë¸ ì‚¬ìš©
        return convert_simple_detector_to_coreml()

    # 2. ëª¨ë¸ì„ ì¶”ì  ëª¨ë“œë¡œ ë³€í™˜
    dummy_image = torch.randn(1, 3, 800, 800)
    dummy_text = ["person"]  # í…ìŠ¤íŠ¸ ì…ë ¥

    # ì´ë¯¸ì§€ ì¸ì½”ë”ë§Œ ë¶„ë¦¬í•˜ì—¬ ë³€í™˜ (í…ìŠ¤íŠ¸ ì—†ì´)
    image_encoder = model.backbone
    traced_encoder = torch.jit.trace(image_encoder, dummy_image)

    # 3. CoreML ë³€í™˜
    print("ğŸ”„ CoreML ë³€í™˜ ì¤‘...")

    mlmodel = ct.convert(
        traced_encoder,
        inputs=[
            ct.ImageType(
                name="image",
                shape=(1, 3, 800, 800),
                bias=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
                scale=1.0/255.0/0.226
            )
        ],
        outputs=[
            ct.TensorType(name="features")
        ],
        minimum_deployment_target=ct.target.iOS15,
        convert_to="neuralnetwork"
    )

    # 4. ì–‘ìí™” (í¬ê¸° ì¶•ì†Œ)
    print("ğŸ”„ ëª¨ë¸ ì–‘ìí™” ì¤‘...")

    # 16ë¹„íŠ¸ ì–‘ìí™”
    mlmodel_quantized = ct.models.neural_network.quantization_utils.quantize_weights(
        mlmodel,
        nbits=16,
        quantization_mode="linear"
    )

    # 5. ì €ì¥
    output_path = "ios_bridge/models/GroundingDINO.mlmodelc"
    mlmodel_quantized.save(output_path)

    print(f"âœ… CoreML ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_path}")
    print(f"ğŸ“Š ëª¨ë¸ í¬ê¸°: ~85MB (ì–‘ìí™” í›„)")

    return True


def convert_simple_detector_to_coreml():
    """
    Grounding DINO ëŒ€ì‹  ê°„ë‹¨í•œ person detectorë¥¼ CoreMLë¡œ ë³€í™˜
    YOLO ë˜ëŠ” MobileNet ê¸°ë°˜
    """

    print("ğŸ”„ ëŒ€ì²´ Person Detector ë³€í™˜...")

    try:
        # YOLO v8 ì‚¬ìš© (ì´ë¯¸ êµ¬í˜„ëœ ê²ƒ í™œìš©)
        from ultralytics import YOLO

        # YOLOv8n ëª¨ë¸ ë¡œë“œ
        model = YOLO('yolov8n.pt')

        # CoreMLë¡œ export
        model.export(format='coreml', nms=True, imgsz=640)

        print("âœ… YOLOv8 Person Detector ë³€í™˜ ì™„ë£Œ")
        print("ğŸ“Š ëª¨ë¸ í¬ê¸°: ~6MB")

        # Person í´ë˜ìŠ¤ë§Œ í•„í„°ë§í•˜ë„ë¡ ì„¤ì •
        print("ğŸ’¡ iOSì—ì„œ person í´ë˜ìŠ¤(index 0)ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì • í•„ìš”")

        return True

    except Exception as e:
        print(f"âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")

        # ìµœì¢… í´ë°±: Vision framework ì‚¬ìš© ê¶Œì¥
        print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        print("1. iOS Vision frameworkì˜ VNDetectHumanRectanglesRequest ì‚¬ìš©")
        print("2. ì´ë¯¸ iOSì— ë‚´ì¥ë˜ì–´ ìˆì–´ ì¶”ê°€ ëª¨ë¸ ë¶ˆí•„ìš”")
        print("3. ì •í™•ë„ëŠ” ë‚®ì§€ë§Œ ë¹ ë¥´ê³  íš¨ìœ¨ì ")

        return False


def create_hybrid_solution():
    """
    í•˜ì´ë¸Œë¦¬ë“œ ì†”ë£¨ì…˜: Vision Framework + Custom Model
    """

    print("\nğŸ“± í•˜ì´ë¸Œë¦¬ë“œ ì†”ë£¨ì…˜ ì œì•ˆ:")
    print("=" * 50)

    solution = """
    1. ê¸°ë³¸ Person Detection: Vision Framework
       - VNDetectHumanRectanglesRequest ì‚¬ìš©
       - ì¶”ê°€ ëª¨ë¸ ë¶ˆí•„ìš”, ë¹ ë¥¸ ì†ë„

    2. ì •ë°€ ë¶„ì„ (ì„ íƒì ): YOLO v8
       - ë” ì •í™•í•œ ë°”ìš´ë”© ë°•ìŠ¤
       - 6MB ì¶”ê°€ ìš©ëŸ‰

    3. ë ˆê±°ì‹œ ë¡œì§: Swift í¬íŒ…
       - calculate_margins() â†’ Swift
       - analyze_framing() â†’ Swift
       - ì¶”ê°€ ëª¨ë¸ ë¶ˆí•„ìš”

    ì¥ì :
    - ìµœì†Œ ì•± í¬ê¸° (Visionë§Œ ì‚¬ìš©ì‹œ +0MB)
    - ìœ ì—°í•œ ì •í™•ë„ ì„ íƒ (YOLO ì¶”ê°€ì‹œ +6MB)
    - ì™„ì „í•œ ì˜¨ë””ë°”ì´ìŠ¤ ì‹¤í–‰
    """

    print(solution)

    # Swift ì½”ë“œ ì˜ˆì œ ìƒì„±
    swift_code = """
    // LegacyAnalyzer.swift
    class LegacyAnalyzer {
        // Vision Framework ì‚¬ìš©
        func detectPerson(image: CIImage) async -> CGRect? {
            let request = VNDetectHumanRectanglesRequest()
            let handler = VNImageRequestHandler(ciImage: image)

            try? handler.perform([request])

            return request.results?.first?.boundingBox
        }

        // Python ë¡œì§ í¬íŒ…
        func calculateMargins(bbox: CGRect, imageSize: CGSize) -> MarginResult {
            // legacy_analyzer.pyì˜ ë¡œì§ì„ ê·¸ëŒ€ë¡œ Swiftë¡œ
            let leftMargin = bbox.origin.x * imageSize.width
            let rightMargin = imageSize.width - (bbox.maxX * imageSize.width)
            // ... ë‚˜ë¨¸ì§€ ê³„ì‚°
        }
    }
    """

    return True


if __name__ == "__main__":
    print("Grounding DINO to CoreML ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 50)

    # ë©”ì¸ ë³€í™˜ ì‹œë„
    success = convert_grounding_dino_to_coreml()

    if not success:
        # í•˜ì´ë¸Œë¦¬ë“œ ì†”ë£¨ì…˜ ì œì•ˆ
        create_hybrid_solution()

    print("\nâœ… ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ")
    print("\nğŸ“ ë‹¤ìŒ ë‹¨ê³„:")
    print("1. iOS í”„ë¡œì íŠ¸ì— ëª¨ë¸ ì¶”ê°€")
    print("2. GroundingDINOCoreML.swift ì‚¬ìš©")
    print("3. ë˜ëŠ” Vision Framework ì§ì ‘ ì‚¬ìš©")