import Foundation
import UIKit
import CoreImage
import Combine
import CoreML
import AVFoundation

// MARK: - Analysis State (Grouped for Performance)
struct AnalysisState: Equatable {
    var instantFeedback: [FeedbackItem] = []
    var isPerfect: Bool = false
    var perfectScore: Double = 0.0
    var categoryStatuses: [CategoryStatus] = []
    var completedFeedbacks: [CompletedFeedback] = []
    var gateEvaluation: GateEvaluation?
    var v15Feedback: String = ""
    var unifiedFeedback: UnifiedFeedback?
}

// MARK: - ì‹¤ì‹œê°„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° êµ¬ì¡°
struct FrameAnalysis {
    let faceRect: CGRect?                           // ì–¼êµ´ ìœ„ì¹˜ (ì •ê·œí™”ëœ ì¢Œí‘œ)
    let bodyRect: CGRect?                           // ì „ì‹  ì¶”ì • ì˜ì—­
    let brightness: Float                           // í‰ê·  ë°ê¸°
    let tiltAngle: Float                            // ê¸°ìš¸ê¸° ê°ë„
    let faceYaw: Float?                             // ì–¼êµ´ ì¢Œìš° íšŒì „ (ì •ë©´=0)
    let facePitch: Float?                           // ì–¼êµ´ ìƒí•˜ ê°ë„
    let cameraAngle: CameraAngle                    // ì¹´ë©”ë¼ ê°ë„
    let poseKeypoints: [(point: CGPoint, confidence: Float)]?  // ì‹ ë¢°ë„ í¬í•¨ í‚¤í¬ì¸íŠ¸
    let compositionType: CompositionType?           // êµ¬ë„ íƒ€ì…
    // ğŸ—‘ï¸ VNFaceObservation ì œê±° (RTMPoseë¡œ ëŒ€ì²´)
    let gaze: GazeResult?                           // ğŸ†• ì‹œì„  ì¶”ì  ê²°ê³¼
    let depth: V15DepthResult?                      // ğŸ”¥ Depth Anything ML ê¸°ë°˜ ê¹Šì´ ì¶”ì •
    let aspectRatio: CameraAspectRatio              // ğŸ†• ì¹´ë©”ë¼ ë¹„ìœ¨
    let imagePadding: ImagePadding?                 // ğŸ†• ì—¬ë°± ì •ë³´
}

// ğŸ†• ì´ë¯¸ì§€ ì—¬ë°± ì •ë³´
struct ImagePadding {
    let top: CGFloat        // ìƒë‹¨ ì—¬ë°± (0.0 ~ 1.0)
    let bottom: CGFloat     // í•˜ë‹¨ ì—¬ë°±
    let left: CGFloat       // ì¢Œì¸¡ ì—¬ë°±
    let right: CGFloat      // ìš°ì¸¡ ì—¬ë°±

    var total: CGFloat {
        return top + bottom + left + right
    }

    var hasExcessivePadding: Bool {
        // ì–´ëŠ í•œ ìª½ì´ 15% ì´ìƒ ì—¬ë°±ì´ë©´ ê³¼ë„í•¨
        return top > 0.15 || bottom > 0.15 || left > 0.15 || right > 0.15
    }
}

// MARK: - ì‹¤ì‹œê°„ í”¼ë“œë°± ìƒì„±ê¸°
class RealtimeAnalyzer: ObservableObject {
    // MARK: - Published State
    @Published var state = AnalysisState()
    
    // ğŸ’¡ Wrapper properties for backward compatibility (read-only)
    var instantFeedback: [FeedbackItem] { state.instantFeedback }
    var isPerfect: Bool { state.isPerfect }
    var perfectScore: Double { state.perfectScore }
    var categoryStatuses: [CategoryStatus] { state.categoryStatuses }
    var completedFeedbacks: [CompletedFeedback] { state.completedFeedbacks }
    var gateEvaluation: GateEvaluation? { state.gateEvaluation }
    var v15Feedback: String { state.v15Feedback }
    var unifiedFeedback: UnifiedFeedback? { state.unifiedFeedback }

    // ğŸ› ContentViewì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ internalë¡œ ë³€ê²½
    var referenceAnalysis: FrameAnalysis?
    var referenceFramingResult: PhotographyFramingResult?  // ğŸ†• ë ˆí¼ëŸ°ìŠ¤ ì‚¬ì§„í•™ í”„ë ˆì´ë° ë¶„ì„ ê²°ê³¼

    // ğŸ†• v1.5 ìºì‹œëœ ë ˆí¼ëŸ°ìŠ¤
    var cachedReference: CachedReference?

    private var lastAnalysisTime = Date()
    private let analysisInterval: TimeInterval = 0.05  // 50msë§ˆë‹¤ ë¶„ì„ - ë°˜ì‘ì†ë„ ê°œì„ 

    // ğŸ”¥ ë¶„ì„ ì „ìš© ë°±ê·¸ë¼ìš´ë“œ í (UI ë¸”ë¡œí‚¹ ë°©ì§€)
    private let analysisQueue = DispatchQueue(label: "com.tryangle.analysis", qos: .userInitiated)
    private var isAnalyzing = false  // ë¶„ì„ ì¤‘ë³µ ë°©ì§€ í”Œë˜ê·¸
    private var isPaused = false     // ì¼ì‹œ ì¤‘ì§€ í”Œë˜ê·¸ (íƒ­ ì „í™˜ ì‹œ)

    // íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ë¥¼ ìœ„í•œ ìƒíƒœ ì¶”ì 
    private var feedbackHistory: [String: Int] = [:]  // ì¹´í…Œê³ ë¦¬ë³„ ì—°ì† ê°ì§€ íšŸìˆ˜
    private let historyThreshold = 3  // ğŸ”„ 3ë²ˆ ì—°ì† ê°ì§€ë˜ì–´ì•¼ í‘œì‹œ (ì•½ 0.3ì´ˆ) - ë°˜ì‘ì†ë„ ê°œì„ 
    private var perfectFrameCount = 0  // ì™„ë²½í•œ í”„ë ˆì„ ì—°ì† íšŸìˆ˜
    private let perfectThreshold = 5  // 5í”„ë ˆì„(ì•½ 0.5ì´ˆ) ì—°ì† ì™„ë²½í•´ì•¼ ê°ì§€ - ë°˜ì‘ì†ë„ ê°œì„ 

    // ğŸ†• ê³ ì • í”¼ë“œë°± (í•œ ë²ˆ í‘œì‹œë˜ë©´ í•´ê²°ë  ë•Œê¹Œì§€ ìœ ì§€)
    private var stickyFeedbacks: [String: FeedbackItem] = [:]  // ì¹´í…Œê³ ë¦¬ë³„ ê³ ì • í”¼ë“œë°±

    // ğŸ†• ì´ì „ í”„ë ˆì„ì˜ í”¼ë“œë°± (ì™„ë£Œ ê°ì§€ìš©)
    private var previousFeedbackIds = Set<String>()
    // ğŸ†• ì™„ë£Œ ê°ì§€ë¥¼ ìœ„í•œ íˆìŠ¤í…Œë¦¬ì‹œìŠ¤
    private var disappearedFeedbackHistory: [String: Int] = [:]  // ì‚¬ë¼ì§„ í”¼ë“œë°±ì˜ ì—°ì† íšŸìˆ˜
    private let disappearedThreshold = 2  // 2ë²ˆ ì—°ì† ì‚¬ë¼ì ¸ì•¼ ì™„ë£Œë¡œ íŒë‹¨ - ë°˜ì‘ì†ë„ ê°œì„ 

    // ğŸ†• ê³ ì • í”¼ë“œë°± ì¹´í…Œê³ ë¦¬ (í¬ì¦ˆ ê´€ë ¨ì€ ê³„ì† í‘œì‹œ)
    // pose_missing_partsëŠ” ì´ì œ ë ˆí¼ëŸ°ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ì œëŒ€ë¡œ ê°ì§€ë˜ë¯€ë¡œ sticky ì²˜ë¦¬
    private let stickyCategories: Set<String> = [
        "pose_left_arm",
        "pose_right_arm",
        "pose_left_leg",
        "pose_right_leg",
        "pose_missing_parts"
    ]

    // ğŸ”¥ RTMPose ë¶„ì„ê¸° (ONNX Runtime with CoreML EP)
    private var poseMLAnalyzer: PoseMLAnalyzer!
    private let compositionAnalyzer = CompositionAnalyzer()
    private let cameraAngleDetector = CameraAngleDetector()
    
    // ğŸ†• Image Processing Context
    private let ciContext = CIContext(options: [.cacheIntermediates: false])
    
    private var cancellables = Set<AnyCancellable>()
    
    // MARK: - Subscription Setup
    func setupSubscription(framePublisher: AnyPublisher<CMSampleBuffer, Never>, cameraManager: CameraManager) {
        framePublisher
            .sink { [weak self] buffer in
                guard let self = self else { return }
                self.process(
                    buffer: buffer,
                    isFrontCamera: cameraManager.isFrontCamera,
                    currentAspectRatio: cameraManager.aspectRatio, // Note: Accessed on background thread?
                    zoomFactor: cameraManager.virtualZoom
                )
            }
            .store(in: &cancellables)
    }
    
    // Note: accessing cameraManager properties (published) from background sink might be racey if not thread safe.
    // However, CameraManager @Published props are updated on Main Thread.
    // Reading them from background thread is generally TSan unsafe but widely done.
    // Ideally, we should receive these values as a combined stream.
    // But for now, since they change rarely compared to frames, reading current value is acceptable risk or we can assume `process` usage.
    // Actually, `process` does `analysisQueue.async`.
    // So we are capturing `cameraManager` instance.
    // Better approach: combineLatest? 
    // Frame comes at 60fps. Changes in zoom/ratio are rare.
    // `cameraManager` is ObservableObject.
    // We can just read properties.
    
    // MARK: - Buffer Processing (Combine Bridge)
    func process(buffer: CMSampleBuffer, isFrontCamera: Bool, currentAspectRatio: CameraAspectRatio, zoomFactor: CGFloat) {
        // Drop frame if analyzing
        guard !isAnalyzing, !isPaused else { return }
        
        // Throttling
        guard Date().timeIntervalSince(lastAnalysisTime) >= thermalManager.recommendedAnalysisInterval else { return }
        
        isAnalyzing = true
        lastAnalysisTime = Date()
        self.currentZoomFactor = zoomFactor // Update zoom
        
        // Async conversion
        analysisQueue.async { [weak self] in
            guard let self = self else { return }
            
            guard let pixelBuffer = CMSampleBufferGetImageBuffer(buffer) else {
                self.resetAnalyzingFlag()
                return
            }
            
            let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
            guard let cgImage = self.ciContext.createCGImage(ciImage, from: ciImage.extent) else {
                self.resetAnalyzingFlag()
                return
            }
            
            // Handle Orientation
            let _ = isFrontCamera ? UIImage.Orientation.upMirrored : .up 
            // Note: CameraManager handles orientation better. For analysis, .up is usually fine if keypoints are relative, 
            // but for correct display/aspect ratio logic, we might need correct orientation.
            // Let's assume standard portrait for now or use the one from CameraManager if passed. 
            // Actually, for analysis, we often want the raw image orientation. 
            // Let's stick to .up for backend analysis to avoid rotation overhead, unless strictly needed.
            // If the model expects specific orientation, we should rotate. 
            // Existing code used `image.imageOrientation`. 
            // Let's create UIImage with .up for simplicity as standard for ML models typically.
            
            let image = UIImage(cgImage: cgImage, scale: 1.0, orientation: .up)
            
            // Proceed to analyze
            self.analyzeFrameInternal(image, isFrontCamera: isFrontCamera, currentAspectRatio: currentAspectRatio)
        }
    }
    
    private func resetAnalyzingFlag() {
        DispatchQueue.main.async { self.isAnalyzing = false }
    }
    private let gazeTracker = GazeTracker()
    private let depthAnything = DepthAnythingCoreML.shared  // ğŸ”¥ ì‹±ê¸€í†¤ ì‚¬ìš© (ë©”ëª¨ë¦¬ ìµœì í™”)
    private let poseComparator = AdaptivePoseComparator()
    private let gapAnalyzer = GapAnalyzer()
    private let feedbackGenerator = FeedbackGenerator()  // ğŸ—‘ï¸ êµ¬ì‹ (ë ˆê±°ì‹œ í˜¸í™˜ìš©)
    private let framingAnalyzer = FramingAnalyzer()  // ê¸°ì¡´ í”„ë ˆì´ë° ë¶„ì„ê¸°
    private let photographyFramingAnalyzer = PhotographyFramingAnalyzer()  // ì‚¬ì§„í•™ ê¸°ë°˜ í”„ë ˆì´ë° ë¶„ì„ê¸°
    // ğŸ—‘ï¸ stagedFeedbackGenerator ì‚­ì œ - Gate Systemìœ¼ë¡œ í†µí•©ë¨

    // ğŸ†• v1.5 í†µí•© Gate System (5ë‹¨ê³„)
    private let gateSystem = GateSystem.shared
    private let marginAnalyzer = MarginAnalyzer()
    private let personDetector = PersonDetector()  // ì •ë°€ BBox (30í”„ë ˆì„ë§ˆë‹¤) - YOLOX ì¬ì‚¬ìš©
    private let focalLengthEstimator = FocalLengthEstimator.shared  // ğŸ†• 35mm í™˜ì‚° ì´ˆì ê±°ë¦¬

    // ğŸ†• v1.5 í”„ë ˆì„ ì¹´ìš´í„° (Level ì²˜ë¦¬ìš©)
    private var frameCount = 0
    private var lastGroundingDINOBBox: CGRect?  // ë§ˆì§€ë§‰ YOLOX ê²°ê³¼ ìºì‹œ
    private var lastCompressionIndex: CGFloat?  // ë§ˆì§€ë§‰ ì••ì¶•ê° ìºì‹œ
    private var lastDepthResult: V15DepthResult?   // ğŸ”¥ Depth Anything ê²°ê³¼ ìºì‹œ

    // ğŸ†• 35mm í™˜ì‚° ì´ˆì ê±°ë¦¬ ê´€ë ¨
    private var referenceImageData: Data?       // ë ˆí¼ëŸ°ìŠ¤ EXIF ì¶”ì¶œìš©
    private var referenceDepthMap: MLMultiArray?  // ë ˆí¼ëŸ°ìŠ¤ ëìŠ¤ë§µ (EXIF ì—†ì„ ë•Œ fallback)
    private var referenceFocalLength: FocalLengthInfo?  // ìºì‹œëœ ë ˆí¼ëŸ°ìŠ¤ ì´ˆì ê±°ë¦¬
    var currentZoomFactor: CGFloat = 1.0        // í˜„ì¬ ì¤Œ ë°°ìœ¨ (CameraManagerì—ì„œ ì—…ë°ì´íŠ¸)

    // ğŸ”¥ ì„±ëŠ¥ ìµœì í™”
    private let thermalManager = ThermalStateManager()
    private let frameSkipper = AdaptiveFrameSkipper()
    private var lastPerformanceLog = Date()

    // ğŸ†• ì´ˆê¸°í™”
    init() {
        print("ğŸ¬ğŸ¬ğŸ¬ RealtimeAnalyzer init() í˜¸ì¶œë¨ ğŸ¬ğŸ¬ğŸ¬")

        // ğŸ”¥ PoseMLAnalyzerë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¯¸ë¦¬ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ 17ì´ˆ ì§€ì—° ë°©ì§€)
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            print("ğŸ”¥ RealtimeAnalyzer: PoseMLAnalyzer ë°±ê·¸ë¼ìš´ë“œ ì´ˆê¸°í™” ì‹œì‘")
            let startTime = CACurrentMediaTime()
            let analyzer = PoseMLAnalyzer()
            let loadTime = (CACurrentMediaTime() - startTime) * 1000
            print("âœ… RealtimeAnalyzer: PoseMLAnalyzer ì´ˆê¸°í™” ì™„ë£Œ (\(String(format: "%.0f", loadTime))ms)")

            DispatchQueue.main.async {
                self?.poseMLAnalyzer = analyzer

                // ğŸ”¥ PersonDetectorì— RTMPoseRunner ì—°ê²° (YOLOX ì¬ì‚¬ìš©)
                if let rtmRunner = analyzer.rtmPoseRunner {
                    self?.personDetector.setRTMPoseRunner(rtmRunner)
                }
            }
        }
    }


    // MARK: - Helper Methods

    /// ì—¬ë°± ê³„ì‚° (RTMPose êµ¬ì¡°ì  í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
    private func calculatePaddingFromKeypoints(
        keypoints: [(point: CGPoint, confidence: Float)]
    ) -> ImagePadding? {
        // êµ¬ì¡°ì  í‚¤í¬ì¸íŠ¸ë§Œ ì‚¬ìš© (0-16: ëª¸í†µ í‚¤í¬ì¸íŠ¸, ì†ê°€ë½/ì–¼êµ´ ëœë“œë§ˆí¬ ì œì™¸)
        let structuralIndices = PhotographyFramingAnalyzer.StructuralKeypoints.all

        // ì‹ ë¢°ë„ 0.3 ì´ìƒì¸ í‚¤í¬ì¸íŠ¸ë§Œ í•„í„°ë§
        let validPoints = structuralIndices.compactMap { idx -> CGPoint? in
            guard idx < keypoints.count else { return nil }
            return keypoints[idx].confidence > 0.3 ? keypoints[idx].point : nil
        }

        // ìµœì†Œ 3ê°œ ì´ìƒì˜ í‚¤í¬ì¸íŠ¸ê°€ í•„ìš”
        guard validPoints.count >= 3 else { return nil }

        // ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° (ì •ê·œí™”ëœ ì¢Œí‘œ: 0.0 ~ 1.0)
        let minX = validPoints.map { $0.x }.min() ?? 0
        let maxX = validPoints.map { $0.x }.max() ?? 1
        let minY = validPoints.map { $0.y }.min() ?? 0
        let maxY = validPoints.map { $0.y }.max() ?? 1

        // ì—¬ë°± ê³„ì‚° (ì •ê·œí™”ëœ ì¢Œí‘œê³„)
        let top = 1.0 - maxY     // ìƒë‹¨ ì—¬ë°±
        let bottom = minY        // í•˜ë‹¨ ì—¬ë°±
        let left = minX          // ì¢Œì¸¡ ì—¬ë°±
        let right = 1.0 - maxX   // ìš°ì¸¡ ì—¬ë°±

        return ImagePadding(
            top: top,
            bottom: bottom,
            left: left,
            right: right
        )
    }

    /// ğŸ—‘ï¸ êµ¬ì‹ ì—¬ë°± ê³„ì‚° (ì–¼êµ´ ìœ„ì¹˜ ê¸°ë°˜ bodyRect ì¶”ì •) - ë” ì´ìƒ ì‚¬ìš© ì•ˆí•¨
    @available(*, deprecated, message: "Use calculatePaddingFromKeypoints instead")
    private func calculatePadding(bodyRect: CGRect?, imageSize: CGSize) -> ImagePadding? {
        guard let body = bodyRect else { return nil }

        // ğŸ”¥ Vision ì¢Œí‘œê³„: Y=0(í™”ë©´ í•˜ë‹¨), Y=1(í™”ë©´ ìƒë‹¨)
        // body.minY = ì¸ë¬¼ì˜ ì•„ë˜ìª½ ê²½ê³„ (Y ì‘ì€ ê°’)
        // body.maxY = ì¸ë¬¼ì˜ ìœ„ìª½ ê²½ê³„ (Y í° ê°’)

        let top = 1.0 - body.maxY  // í™”ë©´ ìƒë‹¨ ì—¬ë°± (ì¸ë¬¼ ìœ„ ê³µê°„)
        let bottom = body.minY     // í™”ë©´ í•˜ë‹¨ ì—¬ë°± (ì¸ë¬¼ ì•„ë˜ ê³µê°„)
        let left = body.minX       // ì¢Œì¸¡ ì—¬ë°±
        let right = 1.0 - body.maxX  // ìš°ì¸¡ ì—¬ë°±

        return ImagePadding(
            top: top,
            bottom: bottom,
            left: left,
            right: right
        )
    }

    /// ğŸ†• v6: í‚¤í¬ì¸íŠ¸ì—ì„œ ì¸ë¬¼ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° (Python improved_margin_analyzer._calculate_person_bbox ì´ì‹)
    /// - Returns: ì •ê·œí™”ëœ ì¢Œí‘œ (0.0 ~ 1.0)ì˜ ë°”ìš´ë”© ë°•ìŠ¤
    private func calculateBodyRectFromKeypoints(_ keypoints: [(point: CGPoint, confidence: Float)], imageSize: CGSize) -> CGRect? {
        // ì‹ ë¢°ë„ 0.3 ì´ìƒì¸ êµ¬ì¡°ì  í‚¤í¬ì¸íŠ¸(0-16)ë§Œ í•„í„°ë§
        let structuralIndices = PhotographyFramingAnalyzer.StructuralKeypoints.all

        let validPoints = structuralIndices.compactMap { idx -> CGPoint? in
            guard idx < keypoints.count else { return nil }
            return keypoints[idx].confidence > 0.3 ? keypoints[idx].point : nil
        }

        // ìµœì†Œ 3ê°œ ì´ìƒì˜ í‚¤í¬ì¸íŠ¸ê°€ í•„ìš”
        guard validPoints.count >= 3 else { return nil }

        // ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° (í”½ì…€ ì¢Œí‘œ)
        let minX = validPoints.map { $0.x }.min() ?? 0
        let maxX = validPoints.map { $0.x }.max() ?? 1
        let minY = validPoints.map { $0.y }.min() ?? 0
        let maxY = validPoints.map { $0.y }.max() ?? 1

        // ğŸ†• ì •ê·œí™” (0.0 ~ 1.0)
        let normalizedX = minX / imageSize.width
        let normalizedY = minY / imageSize.height
        let normalizedWidth = (maxX - minX) / imageSize.width
        let normalizedHeight = (maxY - minY) / imageSize.height

        return CGRect(x: normalizedX, y: normalizedY, width: normalizedWidth, height: normalizedHeight)
    }

    // MARK: - ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ë¶„ì„
    func analyzeReference(_ image: UIImage, imageData: Data? = nil) {
        print("========================================")
        print("ğŸ¯ğŸ¯ğŸ¯ ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘ ğŸ¯ğŸ¯ğŸ¯")
        print("========================================")

        // ğŸ†• EXIF ì¶”ì¶œìš© ì´ë¯¸ì§€ ë°ì´í„° ì €ì¥
        self.referenceImageData = imageData ?? image.jpegData(compressionQuality: 1.0)

        guard let cgImage = image.cgImage else {
            print("âŒ cgImage ì—†ìŒ")
            return
        }

        // ğŸ†• ëª¨ë¸ ë¡œë”© ëŒ€ê¸°
        guard let analyzer = poseMLAnalyzer else {
            print("â³ PoseMLAnalyzer ë¡œë”© ì¤‘... ë ˆí¼ëŸ°ìŠ¤ ë¶„ì„ ëŒ€ê¸°")
            // 0.5ì´ˆ í›„ ì¬ì‹œë„
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
                self?.analyzeReference(image)
            }
            return
        }

        print("ğŸ¯ ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ í¬ê¸°: \(cgImage.width) x \(cgImage.height)")
        print("ğŸ¯ ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ orientation: \(image.imageOrientation.rawValue)")

        // ğŸ”¥ RTMPoseë¡œ ì–¼êµ´+í¬ì¦ˆ ë™ì‹œ ë¶„ì„ (ONNX Runtime with CoreML EP)
        print("ğŸ¯ PoseMLAnalyzer.analyzeFaceAndPose() í˜¸ì¶œ ì¤‘...")
        let (faceResult, poseResult) = analyzer.analyzeFaceAndPose(from: image)
        print("ğŸ¯ ë¶„ì„ ì™„ë£Œ:")
        print("   - ì–¼êµ´: \(faceResult != nil ? "âœ… ê²€ì¶œë¨" : "âŒ ê²€ì¶œ ì•ˆë¨")")
        print("   - í¬ì¦ˆ: \(poseResult != nil ? "âœ… ê²€ì¶œë¨ (\(poseResult!.keypoints.count)ê°œ í‚¤í¬ì¸íŠ¸)" : "âŒ ê²€ì¶œ ì•ˆë¨")")

        if let pose = poseResult {
            let visibleCount = pose.keypoints.filter { $0.confidence >= 0.5 }.count
            print("   - í¬ì¦ˆ ì‹ ë¢°ë„ â‰¥ 0.5: \(visibleCount)/\(pose.keypoints.count)ê°œ")
        }

        // ğŸ”¥ ë””ë²„ê·¸: í¬ì¦ˆ ê²€ì¶œ ì‹¤íŒ¨ ì‹œ ì´ë¯¸ì§€ ì €ì¥
        if poseResult == nil {
            saveDebugImage(image, reason: "pose_detection_failed")
        }

        let faceRect = faceResult?.faceRect
        let faceYaw = faceResult?.yaw
        let facePitch = faceResult?.pitch
        let poseKeypoints = poseResult?.keypoints

        // ë°ê¸° ê³„ì‚°
        let brightness = poseMLAnalyzer.calculateBrightness(from: cgImage)

        // ğŸ†• ë”ì¹˜ í‹¸íŠ¸ ê°ì§€ (RTMPose í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
        let tiltAngle = cameraAngleDetector.detectDutchTilt(faceObservation: nil) ?? 0.0

        // ì „ì‹  ì˜ì—­ ì¶”ì •
        let bodyRect = poseMLAnalyzer.estimateBodyRect(from: faceRect)

        // ì¹´ë©”ë¼ ì•µê¸€ ê°ì§€ (RTMPose í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
        let cameraAngle = cameraAngleDetector.detectCameraAngle(
            faceRect: faceRect,
            facePitch: facePitch,
            faceObservation: nil
        )

        // êµ¬ë„ íƒ€ì… ë¶„ë¥˜
        var compositionType: CompositionType? = nil
        if let faceRect = faceRect {
            let subjectPosition = CGPoint(x: faceRect.midX, y: faceRect.midY)
            compositionType = compositionAnalyzer.classifyComposition(subjectPosition: subjectPosition)
        }

        // ğŸ—‘ï¸ ì‹œì„  ì¶”ì  ë¹„í™œì„±í™” (VNFaceObservation ì œê±°)
        let gaze: GazeResult? = nil

        // ğŸ”¥ Depth Anything ML ê¸°ë°˜ ê¹Šì´ ì¶”ì • (ì™„ì „ ë¹„ë™ê¸° ì²˜ë¦¬)
        // âœ… ì„¸ë§ˆí¬ì–´ ì œê±°: ë°±ê·¸ë¼ìš´ë“œ íì—ì„œ ë¹„ë™ê¸° ì²´ì¸ìœ¼ë¡œ ì²˜ë¦¬
        // âš ï¸ ë©”ëª¨ë¦¬ ìµœì í™”: autoreleasepoolë¡œ ì„ì‹œ ë©”ëª¨ë¦¬ ì¦‰ì‹œ í•´ì œ

        // ğŸ†• ë¹„ìœ¨ ê°ì§€ (ë¨¼ì € ê³„ì‚°)
        let imageSize = CGSize(width: cgImage.width, height: cgImage.height)
        let aspectRatio = CameraAspectRatio.detect(from: imageSize)

        // ğŸ†• ì—¬ë°± ê³„ì‚° (RTMPose í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
        var padding: ImagePadding? = nil
        if let keypoints = poseKeypoints, keypoints.count >= 17 {
            // í‚¤í¬ì¸íŠ¸ë¥¼ ì •ê·œí™”ëœ ì¢Œí‘œë¡œ ë³€í™˜ (0.0 ~ 1.0)
            let normalizedKeypoints = keypoints.map { kp -> (point: CGPoint, confidence: Float) in
                let normalizedPoint = CGPoint(
                    x: kp.point.x / imageSize.width,
                    y: kp.point.y / imageSize.height
                )
                return (point: normalizedPoint, confidence: kp.confidence)
            }
            // êµ¬ì¡°ì  í‚¤í¬ì¸íŠ¸(0-16)ë¡œ ì—¬ë°± ê³„ì‚°
            padding = calculatePaddingFromKeypoints(keypoints: normalizedKeypoints)
        }

        // ğŸ†• ì‚¬ì§„í•™ ê¸°ë°˜ í”„ë ˆì´ë° ë¶„ì„ (RTMPose 133ê°œ í‚¤í¬ì¸íŠ¸)
        if let keypoints = poseKeypoints, keypoints.count >= 133 {
            let normalizedKeypoints = keypoints.map { kp -> (point: CGPoint, confidence: Float) in
                let normalizedPoint = CGPoint(
                    x: kp.point.x / imageSize.width,
                    y: kp.point.y / imageSize.height
                )
                return (point: normalizedPoint, confidence: kp.confidence)
            }
            referenceFramingResult = photographyFramingAnalyzer.analyze(
                keypoints: normalizedKeypoints,
                imageSize: imageSize
            )
            if let refFraming = referenceFramingResult {
                print("   - ğŸ“¸ ë ˆí¼ëŸ°ìŠ¤ ìƒ· íƒ€ì…: \(refFraming.shotType.rawValue)")
                print("   - ğŸ“¸ ë ˆí¼ëŸ°ìŠ¤ í—¤ë“œë£¸: \(String(format: "%.1f%%", refFraming.headroom * 100))")
                print("   - ğŸ“¸ ë ˆí¼ëŸ°ìŠ¤ ì¹´ë©”ë¼ ì•µê¸€: \(refFraming.cameraAngle.rawValue)")
            }
        } else {
            referenceFramingResult = nil
            print("   - âš ï¸ ì‚¬ì§„í•™ í”„ë ˆì´ë° ë¶„ì„ ë¶ˆê°€ (í‚¤í¬ì¸íŠ¸ ë¶€ì¡±)")
        }

        // ğŸ”¥ ë¹„ë™ê¸° ì²´ì¸ ì‹œì‘: Depth ì¶”ì • â†’ PersonDetector â†’ ìµœì¢… ë¶„ì„ ì™„ë£Œ
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let self = self else { return }

            autoreleasepool {
                // Step 1: Depth ì¶”ì • (ë¹„ë™ê¸°)
                self.depthAnything.estimateDepth(from: image) { [weak self] result in
                    guard let self = self else { return }

                    let depth: V15DepthResult?
                    switch result {
                    case .success(let depthResult):
                        depth = depthResult
                        print("âœ… Depth Anything ë¶„ì„ ì™„ë£Œ: ì••ì¶•ê° \(String(format: "%.2f", depthResult.compressionIndex))")
                    case .failure(let error):
                        print("âš ï¸ Depth Anything ë¶„ì„ ì‹¤íŒ¨: \(error.localizedDescription)")
                        depth = nil
                    }

                    // Step 2: PersonDetector (ë¹„ë™ê¸°)
                    if let ciImage = CIImage(image: image) {
                        self.personDetector.detectPerson(in: ciImage) { [weak self] preciseBBox in
                            guard let self = self else { return }

                            // Step 3: ìµœì¢… ë¶„ì„ ì™„ë£Œ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
                            self.finalizeReferenceAnalysis(
                                faceRect: faceRect,
                                bodyRect: bodyRect,
                                brightness: Double(brightness),
                                tiltAngle: Double(tiltAngle),
                                faceYaw: faceYaw.map { Double($0) },
                                facePitch: facePitch.map { Double($0) },
                                cameraAngle: cameraAngle,
                                poseKeypoints: poseKeypoints,
                                compositionType: compositionType,
                                gaze: gaze,
                                depth: depth,
                                aspectRatio: aspectRatio,
                                padding: padding,
                                preciseBBox: preciseBBox,
                                image: image,
                                imageSize: imageSize
                            )
                        }
                    } else {
                        // PersonDetector ì‹¤í–‰ ë¶ˆê°€ ì‹œ ë°”ë¡œ ì™„ë£Œ
                        self.finalizeReferenceAnalysis(
                            faceRect: faceRect,
                            bodyRect: bodyRect,
                            brightness: Double(brightness),
                            tiltAngle: Double(tiltAngle),
                            faceYaw: faceYaw.map { Double($0) },
                            facePitch: facePitch.map { Double($0) },
                            cameraAngle: cameraAngle,
                            poseKeypoints: poseKeypoints,
                            compositionType: compositionType,
                            gaze: gaze,
                            depth: depth,
                            aspectRatio: aspectRatio,
                            padding: padding,
                            preciseBBox: nil,
                            image: image,
                            imageSize: imageSize
                        )
                    }
                }
            }
        }
    }

    // MARK: - ë ˆí¼ëŸ°ìŠ¤ ë¶„ì„ ìµœì¢… ì²˜ë¦¬ (ë¹„ë™ê¸° ì™„ë£Œ í›„)
    private func finalizeReferenceAnalysis(
        faceRect: CGRect?,
        bodyRect: CGRect?,
        brightness: Double,
        tiltAngle: Double,
        faceYaw: Double?,
        facePitch: Double?,
        cameraAngle: CameraAngle,
        poseKeypoints: [(point: CGPoint, confidence: Float)]?,
        compositionType: CompositionType?,
        gaze: GazeResult?,
        depth: V15DepthResult?,
        aspectRatio: CameraAspectRatio,
        padding: ImagePadding?,
        preciseBBox: CGRect?,
        image: UIImage,
        imageSize: CGSize
    ) {
        // ë°±ê·¸ë¼ìš´ë“œ íì—ì„œ ì‹¤í–‰ë¨

        // ğŸ†• v1.5: ì—¬ë°± ë¶„ì„ ë° ìºì‹±
        if let bbox = preciseBBox ?? bodyRect {
            let marginResult = marginAnalyzer.analyze(
                bbox: bbox,
                imageSize: imageSize,
                isNormalized: true
            )

            // ìºì‹œ ì €ì¥
            let refId = UUID().uuidString
            let cachedRef = CacheManager.shared.cacheReference(
                id: refId,
                image: image,
                bbox: bbox,
                margins: marginResult,
                compressionIndex: depth.map { CGFloat($0.compressionIndex) }
            )

            // ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ìºì‹œ ì—…ë°ì´íŠ¸
            DispatchQueue.main.async { [weak self] in
                self?.cachedReference = cachedRef
            }

            print("ğŸ“¦ v1.5 ë ˆí¼ëŸ°ìŠ¤ ìºì‹œ ì™„ë£Œ: \(refId)")
        }

        // ğŸ†• 35mm í™˜ì‚° ì´ˆì ê±°ë¦¬ ì¶”ì • (EXIF â†’ ëìŠ¤ë§µ ìˆœì„œ)
        let refFL = focalLengthEstimator.estimateReferenceFocalLength(
            imageData: referenceImageData,
            depthMap: referenceDepthMap,
            fallback: 50
        )

        print("ğŸ“ ë ˆí¼ëŸ°ìŠ¤ ì´ˆì ê±°ë¦¬: \(refFL.focalLength35mm)mm (\(refFL.lensType.displayName)) - \(refFL.source.description)")

        print("========================================")
        print("ğŸ“¸ ë ˆí¼ëŸ°ìŠ¤ ë¶„ì„ ìµœì¢… ê²°ê³¼:")
        print("========================================")
        print("   - ë¹„ìœ¨: \(aspectRatio.displayName)")
        print("   - ì–¼êµ´: \(faceRect != nil ? "âœ… ê°ì§€ë¨" : "âŒ ì—†ìŒ")")
        print("   - ì–¼êµ´ ê°ë„: yaw=\(faceYaw ?? 0), pitch=\(facePitch ?? 0)")
        print("   - ì¹´ë©”ë¼ ì•µê¸€: \(cameraAngle.description)")
        print("   - êµ¬ë„: \(compositionType?.description ?? "ì•Œ ìˆ˜ ì—†ìŒ")")
        print("   - ì‹œì„ : \(gaze?.direction.description ?? "ì•Œ ìˆ˜ ì—†ìŒ")")
        print("   - ğŸ”¥ ì••ì¶•ê°: \(depth.map { String(format: "%.2f (\($0.cameraType.description))", $0.compressionIndex) } ?? "ì•Œ ìˆ˜ ì—†ìŒ")")
        print("   - ğŸ†• YOLOX BBox: \(preciseBBox != nil ? "âœ…" : "âŒ (RTMPose ì‚¬ìš©)")")

        if let keypoints = poseKeypoints {
            let visibleCount = keypoints.filter { $0.confidence >= 0.5 }.count
            print("   - í¬ì¦ˆ í‚¤í¬ì¸íŠ¸: \(keypoints.count)ê°œ (ì‹ ë¢°ë„ â‰¥ 0.5: \(visibleCount)ê°œ)")
            if visibleCount >= 5 {
                print("   - âœ… í¬ì¦ˆ ê²€ì¶œ ì„±ê³µ! UIì— í‘œì‹œë  ê²ƒì„")
            } else {
                print("   - âš ï¸ í¬ì¦ˆ ì‹ ë¢°ë„ ë‚®ìŒ - í¬ì¦ˆ ë¹„êµ ë¶ˆê°€ëŠ¥")
            }
        } else {
            print("   - âŒ í¬ì¦ˆ í‚¤í¬ì¸íŠ¸: ì—†ìŒ")
            print("   - âš ï¸ RTMPose í¬ì¦ˆ ê²€ì¶œ ì‹¤íŒ¨")
        }

        print("   - ë°ê¸°: \(brightness)")
        print("   - ê¸°ìš¸ê¸°: \(tiltAngle)ë„")
        print("========================================")

        // ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ referenceAnalysis ë° referenceFocalLength ì—…ë°ì´íŠ¸
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }

            self.referenceAnalysis = FrameAnalysis(
                faceRect: faceRect,
                bodyRect: bodyRect,
                brightness: Float(brightness),
                tiltAngle: Float(tiltAngle),
                faceYaw: faceYaw.map { Float($0) },
                facePitch: facePitch.map { Float($0) },
                cameraAngle: cameraAngle,
                poseKeypoints: poseKeypoints,
                compositionType: compositionType,
                gaze: gaze,
                depth: depth,
                aspectRatio: aspectRatio,
                imagePadding: padding
            )

            self.referenceFocalLength = refFL

            print("âœ… ë ˆí¼ëŸ°ìŠ¤ ë¶„ì„ ì™„ë£Œ: UI ì—…ë°ì´íŠ¸ë¨")
        }
    }

    // MARK: - Pause/Resume (íƒ­ ì „í™˜ìš©)
    func pauseAnalysis() {
        print("â¸ï¸ RealtimeAnalyzer: ë¶„ì„ ì¼ì‹œ ì¤‘ì§€ (íƒ­ ì „í™˜)")
        isPaused = true

        // í”¼ë“œë°± ì´ˆê¸°í™”
        DispatchQueue.main.async {
            var newState = self.state
            newState.instantFeedback = []
            newState.isPerfect = false
            newState.perfectScore = 0.0
            newState.unifiedFeedback = nil
            self.state = newState
        }
    }

    func resumeAnalysis() {
        print("â–¶ï¸ RealtimeAnalyzer: ë¶„ì„ ì¬ê°œ (íƒ­ ë³µê·€)")
        isPaused = false

        // ìƒíƒœ ì´ˆê¸°í™” (ìƒˆë¡­ê²Œ ì‹œì‘)
        lastAnalysisTime = Date()
        feedbackHistory.removeAll()
        disappearedFeedbackHistory.removeAll()
        perfectFrameCount = 0
    }

    // MARK: - ì‹¤ì‹œê°„ í”„ë ˆì„ ë¶„ì„

    
    // MARK: - Internal Analysis Logic
    // Renamed from analyzeFrame to separate public/private concerns if needed.
    // Kept public analyzeFrame for legacy calls if any, but logic moved here.
    func analyzeFrame(_ image: UIImage, isFrontCamera: Bool = false, currentAspectRatio: CameraAspectRatio = .ratio4_3) {
        // Adapter for old timer-based calls (will be removed, but kept for safety during refactor)
        guard !isAnalyzing, !isPaused else { return }
        guard Date().timeIntervalSince(lastAnalysisTime) >= thermalManager.recommendedAnalysisInterval else { return }
        isAnalyzing = true
        lastAnalysisTime = Date()
        
        analysisQueue.async { [weak self] in
            self?.analyzeFrameInternal(image, isFrontCamera: isFrontCamera, currentAspectRatio: currentAspectRatio)
        }
    }

    private func analyzeFrameInternal(_ image: UIImage, isFrontCamera: Bool, currentAspectRatio: CameraAspectRatio) {
        // Safe check for reference
        guard let reference = referenceAnalysis else {
            DispatchQueue.main.async {
                var newState = self.state
                newState.instantFeedback = []
                newState.perfectScore = 0.0
                newState.isPerfect = false
                self.state = newState
                self.isAnalyzing = false
            }
            return
        }
        
         guard let cgImage = image.cgImage else {
             resetAnalyzingFlag()
             return 
         }

        // ğŸ†• ëª¨ë¸ ë¡œë”© ëŒ€ê¸° (ì•± ì‹œì‘ ì§í›„)
        guard let analyzer = self.poseMLAnalyzer else {
            print("â³ PoseMLAnalyzer ë¡œë”© ì¤‘... ë¶„ì„ ìŠ¤í‚µ")
            resetAnalyzingFlag()
            return
        }

        let analysisStart = CACurrentMediaTime()  // ğŸ” í”„ë¡œíŒŒì¼ë§

        // RTMPoseë¡œ ë¶„ì„ (ONNX Runtime with CoreML EP)
        let poseStart = CACurrentMediaTime()  // ğŸ”
        let (faceResult, poseResult) = analyzer.analyzeFaceAndPose(from: image)
        let poseEnd = CACurrentMediaTime()  // ğŸ”

        let analysisEnd = CACurrentMediaTime()  // ğŸ”

        // ğŸ” í”„ë¡œíŒŒì¼ë§ ë¡œê·¸ (ë§¤ ë¶„ì„ë§ˆë‹¤)
        let poseTime = (poseEnd - poseStart) * 1000
        let totalTime = (analysisEnd - analysisStart) * 1000
        print("ğŸ“Š [RealtimeAnalyzer] RTMPose: \(String(format: "%.1f", poseTime))ms, ì´ë¶„ì„: \(String(format: "%.1f", totalTime))ms")

        // ë¶„ì„ ì™„ë£Œ í›„ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ UI ì—…ë°ì´íŠ¸
        DispatchQueue.main.async {
            self.isAnalyzing = false
            self.processAnalysisResult(
                faceResult: faceResult,
                poseResult: poseResult,
                cgImage: cgImage,
                reference: reference, // Passed safely
                isFrontCamera: isFrontCamera,
                currentAspectRatio: currentAspectRatio
            )
        }
    }

    // MARK: - ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ (ë©”ì¸ ìŠ¤ë ˆë“œ)
    private func processAnalysisResult(
        faceResult: FaceAnalysisResult?,
        poseResult: PoseAnalysisResult?,
        cgImage: CGImage,
        reference: FrameAnalysis,
        isFrontCamera: Bool,
        currentAspectRatio: CameraAspectRatio
    ) {
        // ğŸ†• v1.5: í”„ë ˆì„ ì¹´ìš´í„° ì¦ê°€
        frameCount += 1

        // ğŸ”¥ ì„±ëŠ¥ ë¡œê·¸ (10ì´ˆë§ˆë‹¤)
        if Date().timeIntervalSince(lastPerformanceLog) >= 10 {
            lastPerformanceLog = Date()
            print(PerformanceOptimizer.shared.getPerformanceReport())
            print("ğŸŒ¡ï¸ ë°œì—´ ìƒíƒœ: \(thermalManager.currentThermalState.rawValue), ë¶„ì„ ê°„ê²©: \(Int(thermalManager.recommendedAnalysisInterval * 1000))ms")
        }

        // ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ì™„ì„±ë„ 0ìœ¼ë¡œ ì„¤ì •
        guard faceResult != nil else {
            // Update grouped state
            var newState = self.state
            newState.instantFeedback = [FeedbackItem(
                priority: 1,
                icon: "ğŸ‘¤",
                message: "ì–¼êµ´ì„ í™”ë©´ì— ë³´ì—¬ì£¼ì„¸ìš”",
                category: "no_face",
                currentValue: nil,
                targetValue: nil,
                tolerance: nil,
                unit: nil
            )]
            newState.perfectScore = 0.0
            newState.isPerfect = false
            
            if self.state != newState {
                self.state = newState
            }
            return
        }

        // ë°ê¸° ë° ê¸°ìš¸ê¸°
        let brightness = poseMLAnalyzer.calculateBrightness(from: cgImage)
        let tilt = cameraAngleDetector.detectDutchTilt(faceObservation: nil) ?? 0.0

        // ğŸ†• ì´ë¯¸ì§€ í¬ê¸° (ì •ê·œí™”ì— í•„ìš”)
        let imageSize = CGSize(width: cgImage.width, height: cgImage.height)

        // ğŸ†• ì „ì‹  ì˜ì—­ - RTMPose í‚¤í¬ì¸íŠ¸ì—ì„œ ì •í™•í•˜ê²Œ ê³„ì‚° (ì •ê·œí™”ëœ ì¢Œí‘œ)
        let bodyRect: CGRect? = {
            if let keypoints = poseResult?.keypoints, !keypoints.isEmpty {
                return calculateBodyRectFromKeypoints(keypoints, imageSize: imageSize)
            }
            // RTMPose í‚¤í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ì–¼êµ´ ê¸°ë°˜ ì¶”ì • (fallback) - ì´ë¯¸ ì •ê·œí™”ë¨
            return poseMLAnalyzer.estimateBodyRect(from: faceResult?.faceRect)
        }()

        // ì¹´ë©”ë¼ ì•µê¸€ (RTMPose í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
        let cameraAngle = cameraAngleDetector.detectCameraAngle(
            faceRect: faceResult?.faceRect,
            facePitch: faceResult?.pitch,
            faceObservation: nil
        )

        // êµ¬ë„
        var compositionType: CompositionType? = nil
        if let faceRect = faceResult?.faceRect {
            let subjectPosition = CGPoint(x: faceRect.midX, y: faceRect.midY)
            compositionType = compositionAnalyzer.classifyComposition(subjectPosition: subjectPosition)
        }

        // ğŸ—‘ï¸ ì‹œì„  ë¹„í™œì„±í™” (VNFaceObservation ì œê±°)
        let gaze: GazeResult? = nil

        // ğŸ”¥ Level 2: Depth Anything ML ê¹Šì´ ì¶”ì • (ë™ì  í”„ë ˆì„ ìŠ¤í‚µ)
        let depth: V15DepthResult? = lastDepthResult  // ìºì‹œëœ ê°’ ì‚¬ìš©
        if frameSkipper.shouldExecute(level: 2, frameCount: frameCount) {
            // ë™ì  ê°„ê²©ìœ¼ë¡œ ìƒˆë¡œ ê³„ì‚° (ë¹„ë™ê¸° â†’ ë°±ê·¸ë¼ìš´ë“œ)
            let uiImage = UIImage(cgImage: cgImage)
            depthAnything.estimateDepth(from: uiImage) { [weak self] result in
                DispatchQueue.main.async {
                    switch result {
                    case .success(let depthResult):
                        self?.lastDepthResult = depthResult  // ìºì‹œ ì—…ë°ì´íŠ¸
                    case .failure:
                        break  // ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ìºì‹œ ìœ ì§€
                    }
                }
            }
        }

        // ğŸ†• í˜„ì¬ ì´ë¯¸ì§€ í¬ê¸° (ìœ„ì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨)
        let currentImageSize = imageSize

        // ğŸ†• ì—¬ë°± ê³„ì‚° (RTMPose í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
        var currentPadding: ImagePadding? = nil
        if let keypoints = poseResult?.keypoints, keypoints.count >= 17 {
            // í‚¤í¬ì¸íŠ¸ë¥¼ ì •ê·œí™”ëœ ì¢Œí‘œë¡œ ë³€í™˜ (0.0 ~ 1.0)
            let normalizedKeypoints = keypoints.map { kp -> (point: CGPoint, confidence: Float) in
                let normalizedPoint = CGPoint(
                    x: kp.point.x / currentImageSize.width,
                    y: kp.point.y / currentImageSize.height
                )
                return (point: normalizedPoint, confidence: kp.confidence)
            }
            // êµ¬ì¡°ì  í‚¤í¬ì¸íŠ¸(0-16)ë¡œ ì—¬ë°± ê³„ì‚°
            currentPadding = calculatePaddingFromKeypoints(keypoints: normalizedKeypoints)
        }

        // ğŸ†• í”„ë ˆì´ë° ë¶„ì„ ì¶”ê°€ (ìµœìš°ì„ )
        let _ = FrameAnalysis(
            faceRect: faceResult?.faceRect,
            bodyRect: bodyRect,
            brightness: brightness,
            tiltAngle: tilt,
            faceYaw: faceResult?.yaw,
            facePitch: faceResult?.pitch,
            cameraAngle: cameraAngle,
            poseKeypoints: poseResult?.keypoints,
            compositionType: compositionType,
            gaze: gaze,
            depth: depth,
            aspectRatio: currentAspectRatio,
            imagePadding: currentPadding
        )

        // ğŸ—‘ï¸ ë¹„ìœ¨ ë¶ˆì¼ì¹˜ ì²´í¬ëŠ” ì´ì œ StagedFeedbackGeneratorê°€ ì²˜ë¦¬ (Phase 3)

        // ============================================
        // ğŸ†• v1.5 í†µí•© Gate System í‰ê°€ (5ë‹¨ê³„)
        // ============================================

        // Level 3: Grounding DINOë¡œ ì •ë°€ BBox ê°±ì‹  (ë™ì  í”„ë ˆì„ ìŠ¤í‚µ)
        if frameSkipper.shouldExecute(level: 3, frameCount: frameCount) {
            let uiImage = UIImage(cgImage: cgImage)
            if let ciImage = CIImage(image: uiImage) {
                // ğŸ”¥ Level 3 ì „ìš© ë°±ê·¸ë¼ìš´ë“œ íì—ì„œ ë¹„ë™ê¸° ì‹¤í–‰
                PerformanceOptimizer.shared.level3Queue.async { [weak self] in
                    self?.personDetector.detectPerson(in: ciImage) { bbox in
                        DispatchQueue.main.async {
                            // ğŸ”§ FIX: nilì¼ ë•Œë„ ì—…ë°ì´íŠ¸í•˜ì—¬ ìºì‹œ stale ë°©ì§€
                            self?.lastGroundingDINOBBox = bbox
                            if bbox == nil {
                                print("âš ï¸ Grounding DINO: ì¸ë¬¼ ë¯¸ê°ì§€ - BBox ì´ˆê¸°í™”")
                            }
                        }
                    }
                }
            }
        }

        // ğŸ”§ FIX: í˜„ì¬ BBox ê²°ì • - ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬ ì¶”ê°€
        // bodyRectê°€ ìˆìœ¼ë©´ í˜„ì¬ í”„ë ˆì„ì— ì¸ë¬¼ì´ ìˆë‹¤ëŠ” ì˜ë¯¸ â†’ ê·¸ê²ƒ ì‚¬ìš©
        // bodyRectê°€ ì—†ê³  ìºì‹œëœ BBoxë„ ì—†ìœ¼ë©´ â†’ ê¸°ë³¸ê°’ (ê±°ì˜ ì—†ëŠ” ì¸ë¬¼)
        let currentBBox: CGRect
        if let body = bodyRect {
            // Visionì—ì„œ í˜„ì¬ í”„ë ˆì„ì— ì¸ë¬¼ ê°ì§€ë¨ â†’ bodyRect ë˜ëŠ” DINO ê²°ê³¼ ì‚¬ìš©
            currentBBox = lastGroundingDINOBBox ?? body
        } else if lastGroundingDINOBBox != nil {
            // Visionì€ ì¸ë¬¼ ëª» ì°¾ì•˜ì§€ë§Œ DINOì—ì„œëŠ” ì°¾ìŒ â†’ DINO ê²°ê³¼ ì‚¬ìš©
            currentBBox = lastGroundingDINOBBox!
        } else {
            // ë‘˜ ë‹¤ ì¸ë¬¼ ì—†ìŒ â†’ ì‘ì€ ê¸°ë³¸ê°’ (ì¸ë¬¼ ë¯¸ê²€ì¶œë¡œ ì²˜ë¦¬ë¨)
            currentBBox = CGRect(x: 0.45, y: 0.45, width: 0.01, height: 0.01)
        }

        // ğŸ”§ FIX: ì••ì¶•ê°ì€ í˜„ì¬ í”„ë ˆì„ ê°’ ì‚¬ìš© (ìºì‹œ ì˜ì¡´ ì œê±°)
        // depthê°€ nilì´ë©´ ì••ì¶•ê°ë„ nilë¡œ ì „ë‹¬ â†’ Gateì—ì„œ "ë¶„ì„ ì¤‘" í‘œì‹œ
        let currentCompressionIndex: CGFloat?
        if let depthResult = depth {
            currentCompressionIndex = CGFloat(depthResult.compressionIndex)
            lastCompressionIndex = currentCompressionIndex  // ìºì‹œë„ ì—…ë°ì´íŠ¸
        } else {
            // ğŸ”§ ìºì‹œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - í˜„ì¬ í”„ë ˆì„ì— depth ì—†ìœ¼ë©´ nil
            currentCompressionIndex = nil
        }

        // í¬ì¦ˆ ë¹„êµ (Gate 4ìš©)
        var poseComparison: PoseComparisonResult? = nil
        if let refKeypoints = reference.poseKeypoints,
           let curKeypoints = poseResult?.keypoints,
           refKeypoints.count >= 133 && curKeypoints.count >= 133 {

            poseComparison = poseComparator.comparePoses(
                referenceKeypoints: refKeypoints,
                currentKeypoints: curKeypoints
            )
        }

        // âœ… ë¬´ê±°ìš´ ì—°ì‚°ì„ ë°±ê·¸ë¼ìš´ë“œë¡œ ì´ë™
        // ë°±ê·¸ë¼ìš´ë“œì—ì„œ Gate System í‰ê°€ ë° í”¼ë“œë°± ìƒì„±
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let self = self else { return }

            // ğŸ†• v6: í‚¤í¬ì¸íŠ¸ ë³€í™˜ (tuple â†’ PoseKeypoint)
            let currentPoseKeypoints: [PoseKeypoint]? = poseResult?.keypoints.map { kp in
                PoseKeypoint(location: kp.point, confidence: kp.confidence)
            }
            let referencePoseKeypoints: [PoseKeypoint]? = reference.poseKeypoints?.map { kp in
                PoseKeypoint(location: kp.point, confidence: kp.confidence)
            }

            var stableFeedback: [FeedbackItem] = []
            var evaluation: GateEvaluation?
            var unifiedFeedback: UnifiedFeedback?

            if let cached = self.cachedReference {
                // ğŸ†• 35mm í™˜ì‚° ì´ˆì ê±°ë¦¬ ê³„ì‚°
                let currentFocalLength = self.focalLengthEstimator.focalLengthFromZoom(self.currentZoomFactor)

                // ğŸ”¥ ë¬´ê±°ìš´ ì—°ì‚°: Gate System í‰ê°€ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
                evaluation = self.gateSystem.evaluate(
                    currentBBox: currentBBox,
                    referenceBBox: cached.bbox,
                    currentImageSize: currentImageSize,
                    referenceImageSize: cached.imageSize,
                    compressionIndex: currentCompressionIndex,
                    referenceCompressionIndex: cached.compressionIndex,
                    currentAspectRatio: currentAspectRatio,
                    referenceAspectRatio: reference.aspectRatio,
                    poseComparison: poseComparison,
                    isFrontCamera: isFrontCamera,
                    currentKeypoints: currentPoseKeypoints,
                    referenceKeypoints: referencePoseKeypoints,
                    currentFocalLength: currentFocalLength,
                    referenceFocalLength: self.referenceFocalLength
                )

                // ğŸ”¥ ë¬´ê±°ìš´ ì—°ì‚°: UnifiedFeedback ìƒì„± (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
                if let eval = evaluation {
                    let targetZoomValue = self.referenceFocalLength.map {
                        CGFloat($0.focalLength35mm) / CGFloat(FocalLengthEstimator.iPhoneBaseFocalLength)
                    }

                    unifiedFeedback = UnifiedFeedbackGenerator.shared.generateUnifiedFeedback(
                        from: eval,
                        isFrontCamera: isFrontCamera,
                        currentZoom: self.currentZoomFactor,
                        targetZoom: targetZoomValue,
                        currentSubjectSize: currentBBox.width * currentBBox.height,
                        targetSubjectSize: cached.bbox.width * cached.bbox.height
                    )

                    // Gate System í”¼ë“œë°± ìƒì„±
                    let gateFeedbacks = V15FeedbackGenerator.shared.generateFeedbackItems(from: eval)

                    // íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ì ìš©
                    for fb in gateFeedbacks {
                        self.feedbackHistory[fb.category, default: 0] += 1

                        if self.feedbackHistory[fb.category]! >= self.historyThreshold {
                            stableFeedback.append(fb)
                        }
                    }

                    // ì‚¬ë¼ì§„ ì¹´í…Œê³ ë¦¬ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
                    let currentCategories = Set(gateFeedbacks.map { $0.category })
                    for (category, _) in self.feedbackHistory {
                        if !currentCategories.contains(category) {
                            self.feedbackHistory[category] = 0
                        }
                    }

                    print("ğŸ¯ v1.5 Gate: \(eval.passedCount)/5 í†µê³¼, ì ìˆ˜: \(String(format: "%.0f%%", Double(eval.overallScore) * 100))")
                }
            }

            // ì™„ë²½ ìƒíƒœ ê°ì§€ (Gate System ê¸°ì¤€)
            let isCurrentlyPerfect = evaluation?.allPassed ?? false
            let score = evaluation.map { Double($0.overallScore) } ?? 0.0

            // ì™„ë£Œëœ í”¼ë“œë°± ê°ì§€ (íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ì ìš©)
            let currentFeedbackIds = Set(stableFeedback.map { $0.id })
            let disappeared = self.previousFeedbackIds.subtracting(currentFeedbackIds)

            var completedToAdd: [CompletedFeedback] = []

            // ì‚¬ë¼ì§„ í”¼ë“œë°±ì˜ ì—°ì† íšŸìˆ˜ ì¶”ì 
            for disappearedId in disappeared {
                self.disappearedFeedbackHistory[disappearedId, default: 0] += 1

                // 5ë²ˆ ì—°ì† ì‚¬ë¼ì§€ë©´ ì™„ë£Œë¡œ íŒë‹¨
                if self.disappearedFeedbackHistory[disappearedId]! >= self.disappearedThreshold {
                    if let completedItem = self.instantFeedback.first(where: { $0.id == disappearedId }) {
                        let completed = CompletedFeedback(item: completedItem, completedAt: Date())
                        completedToAdd.append(completed)
                    }
                    // ì™„ë£Œ ì²˜ë¦¬ í›„ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
                    self.disappearedFeedbackHistory[disappearedId] = 0
                }
            }

            // ë‹¤ì‹œ ë‚˜íƒ€ë‚œ í”¼ë“œë°±ì€ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
            for (feedbackId, _) in self.disappearedFeedbackHistory {
                if currentFeedbackIds.contains(feedbackId) {
                    self.disappearedFeedbackHistory[feedbackId] = 0
                }
            }

            // ì¹´í…Œê³ ë¦¬ë³„ ìƒíƒœ ê³„ì‚°
            let categoryStatuses = self.calculateCategoryStatuses(from: stableFeedback)

            // ë©”ì¸ ìŠ¤ë ˆë“œë¡œ UI ì—…ë°ì´íŠ¸ë§Œ ì „ë‹¬
            DispatchQueue.main.async { [weak self] in
                guard let self = self else { return }
                
                var newState = self.state

                // âœ… Phase 1 ìµœì í™”: ì¡°ê±´ ì—†ì´ í• ë‹¹ (Equatableì´ ë§ˆì§€ë§‰ì— ë¹„êµ)
                if let eval = evaluation {
                    newState.gateEvaluation = eval
                    newState.v15Feedback = eval.primaryFeedback
                }

                if let unified = unifiedFeedback {
                    newState.unifiedFeedback = unified
                }

                newState.instantFeedback = stableFeedback
                newState.perfectScore = score  // ì¡°ê±´ ì œê±° (Equatableì´ ì•Œì•„ì„œ ë¹„êµ)
                newState.categoryStatuses = categoryStatuses

                // ì™„ë²½ í”„ë ˆì„ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
                if isCurrentlyPerfect {
                    self.perfectFrameCount += 1
                } else {
                    self.perfectFrameCount = 0
                }

                newState.isPerfect = self.perfectFrameCount >= self.perfectThreshold

                // ì™„ë£Œëœ í”¼ë“œë°±: ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ ì—…ë°ì´íŠ¸
                var updatedCompletedFeedbacks = newState.completedFeedbacks
                if !completedToAdd.isEmpty {
                    updatedCompletedFeedbacks.append(contentsOf: completedToAdd)
                }
                updatedCompletedFeedbacks.removeAll { !$0.shouldDisplay }
                newState.completedFeedbacks = updatedCompletedFeedbacks

                // ì´ì „ í”¼ë“œë°± ì—…ë°ì´íŠ¸ (Internal state, not published)
                self.previousFeedbackIds = currentFeedbackIds

                // âœ… Final State Update: Equatable í•œ ë²ˆë§Œ ë¹„êµ
                if self.state != newState {
                    self.state = newState
                }
            }
        }
    }

    // MARK: - Category Status Calculation

    /// ì¹´í…Œê³ ë¦¬ë³„ ìƒíƒœ ê³„ì‚°
    private func calculateCategoryStatuses(from feedbacks: [FeedbackItem]) -> [CategoryStatus] {
        // ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ìƒíƒœ ìƒì„±
        var statusMap: [FeedbackCategory: CategoryStatus] = [:]

        // ê° ì¹´í…Œê³ ë¦¬ ì´ˆê¸°í™” (ëª¨ë‘ ë§Œì¡± ìƒíƒœë¡œ ì‹œì‘)
        for category in FeedbackCategory.allCases {
            statusMap[category] = CategoryStatus(
                category: category,
                isSatisfied: true,
                activeFeedbacks: []
            )
        }

        // í”¼ë“œë°±ì´ ìˆëŠ” ì¹´í…Œê³ ë¦¬ëŠ” ë¶ˆë§Œì¡± ìƒíƒœë¡œ ë³€ê²½
        for feedback in feedbacks {
            if let category = FeedbackCategory.from(categoryString: feedback.category) {
                var activeFeedbacks = statusMap[category]?.activeFeedbacks ?? []
                activeFeedbacks.append(feedback)

                statusMap[category] = CategoryStatus(
                    category: category,
                    isSatisfied: false,
                    activeFeedbacks: activeFeedbacks.sorted { $0.priority < $1.priority }
                )
            }
        }

        // ìš°ì„ ìˆœìœ„ ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
        return Array(statusMap.values).sorted { $0.priority < $1.priority }
    }

    // ğŸ—‘ï¸ êµ¬ì‹ í•¨ìˆ˜ë“¤ ì œê±°ë¨ (ìƒˆ ì»´í¬ë„ŒíŠ¸ë¡œ ëŒ€ì²´)
    // - calculatePerfectScore() â†’ GapAnalyzer.calculateCompletionScore() ì‚¬ìš©
    // - calculateBrightness() â†’ VisionAnalyzer.calculateBrightness() ì‚¬ìš©
    // - calculateTilt() â†’ CameraAngleDetector.detectDutchTilt() ì‚¬ìš©
    // - estimateBodyRect() â†’ VisionAnalyzer.estimateBodyRect() ì‚¬ìš©
    // - extractPoseKeypoints() â†’ VisionAnalyzer ë‚´ë¶€ ì‚¬ìš©
    // - estimateCameraAngle() â†’ CameraAngleDetector ì‚¬ìš©
    // - comparePoseKeypoints() â†’ AdaptivePoseComparator ì‚¬ìš©
    // - calculateAngle() â†’ AdaptivePoseComparator ë‚´ë¶€ ì‚¬ìš©

    // MARK: - ë””ë²„ê·¸ í—¬í¼
    private func saveDebugImage(_ image: UIImage, reason: String) {
        guard let data = image.jpegData(compressionQuality: 0.8) else { return }

        let timestamp = DateFormatter.localizedString(from: Date(), dateStyle: .none, timeStyle: .medium)
            .replacingOccurrences(of: ":", with: "-")
        let filename = "debug_\(reason)_\(timestamp).jpg"

        if let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first {
            let fileURL = documentsPath.appendingPathComponent(filename)
            try? data.write(to: fileURL)
            print("ğŸ” ë””ë²„ê·¸ ì´ë¯¸ì§€ ì €ì¥: \(fileURL.path)")
        }
    }
}