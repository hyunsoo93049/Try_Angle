import Foundation
import Vision
import UIKit
import CoreImage
import Combine

// MARK: - v1.5 Extension for DepthResult
extension DepthResult {
    /// v1.5 í˜¸í™˜: ì••ì¶•ê° ì§€ìˆ˜ ê³„ì‚° (ê±°ë¦¬ ë° ì¤Œ ê¸°ë°˜)
    var compressionIndex: Float {
        // ì¤Œ ë°°ìœ¨ì— ë”°ë¥¸ ì••ì¶•ê° ì¶”ì •
        let zoom = zoomFactor ?? 1.0
        if zoom >= 3.0 {
            return 0.8  // ë§ì›
        } else if zoom >= 2.0 {
            return 0.6  // ì¤€ë§ì›
        } else if zoom <= 0.6 {
            return 0.2  // ê´‘ê°
        } else {
            return 0.4  // í‘œì¤€
        }
    }
}

// MARK: - ì‹¤ì‹œê°„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° êµ¬ì¡°
struct FrameAnalysis {
    let faceRect: CGRect?                           // ì–¼êµ´ ìœ„ì¹˜ (ì •ê·œí™”ëœ ì¢Œí‘œ)
    let bodyRect: CGRect?                           // ì „ì‹  ì¶”ì • ì˜ì—­
    let brightness: Float                           // í‰ê·  ë°ê¸°
    let tiltAngle: Float                            // ê¸°ìš¸ê¸° ê°ë„
    let faceYaw: Float?                             // ì–¼êµ´ ì¢Œìš° íšŒì „ (ì •ë©´=0)
    let facePitch: Float?                           // ì–¼êµ´ ìƒí•˜ ê°ë„
    let cameraAngle: CameraAngle                    // ì¹´ë©”ë¼ ê°ë„
    let poseKeypoints: [(point: CGPoint, confidence: Float)]?  // ì‹ ë¢°ë„ í¬í•¨ í‚¤í¬ì¸íŠ¸
    let compositionType: CompositionType?           // êµ¬ë„ íƒ€ì…
    let faceObservation: VNFaceObservation?         // ì–¼êµ´ ê´€ì°° ê²°ê³¼
    let gaze: GazeResult?                           // ğŸ†• ì‹œì„  ì¶”ì  ê²°ê³¼
    let depth: DepthResult?                         // ğŸ†• ê¹Šì´ ì¶”ì • ê²°ê³¼
    let aspectRatio: CameraAspectRatio              // ğŸ†• ì¹´ë©”ë¼ ë¹„ìœ¨
    let imagePadding: ImagePadding?                 // ğŸ†• ì—¬ë°± ì •ë³´
}

// ğŸ†• ì´ë¯¸ì§€ ì—¬ë°± ì •ë³´
struct ImagePadding {
    let top: CGFloat        // ìƒë‹¨ ì—¬ë°± (0.0 ~ 1.0)
    let bottom: CGFloat     // í•˜ë‹¨ ì—¬ë°±
    let left: CGFloat       // ì¢Œì¸¡ ì—¬ë°±
    let right: CGFloat      // ìš°ì¸¡ ì—¬ë°±

    var total: CGFloat {
        return top + bottom + left + right
    }

    var hasExcessivePadding: Bool {
        // ì–´ëŠ í•œ ìª½ì´ 15% ì´ìƒ ì—¬ë°±ì´ë©´ ê³¼ë„í•¨
        return top > 0.15 || bottom > 0.15 || left > 0.15 || right > 0.15
    }
}

// MARK: - ì‹¤ì‹œê°„ í”¼ë“œë°± ìƒì„±ê¸°
class RealtimeAnalyzer: ObservableObject {
    @Published var instantFeedback: [FeedbackItem] = []
    @Published var isPerfect: Bool = false  // ì™„ë²½í•œ ìƒíƒœ ê°ì§€
    @Published var perfectScore: Double = 0.0  // ì™„ì„±ë„ ì ìˆ˜ (0~1)
    @Published var categoryStatuses: [CategoryStatus] = []  // ğŸ†• ì¹´í…Œê³ ë¦¬ë³„ ìƒíƒœ
    @Published var completedFeedbacks: [CompletedFeedback] = []  // ğŸ†• ì™„ë£Œëœ í”¼ë“œë°±ë“¤

    // ğŸ†• v1.5 Gate System ê²°ê³¼
    @Published var gateEvaluation: GateEvaluation?
    @Published var v15Feedback: String = ""  // v1.5 í”¼ë“œë°± ë©”ì‹œì§€

    // ğŸ†• v1.5 í†µí•© í”¼ë“œë°± (í•˜ë‚˜ì˜ ë™ì‘ â†’ ì—¬ëŸ¬ Gate í•´ê²°)
    @Published var unifiedFeedback: UnifiedFeedback?

    // ğŸ› ContentViewì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ internalë¡œ ë³€ê²½
    var referenceAnalysis: FrameAnalysis?
    var referenceFramingResult: PhotographyFramingResult?  // ğŸ†• ë ˆí¼ëŸ°ìŠ¤ ì‚¬ì§„í•™ í”„ë ˆì´ë° ë¶„ì„ ê²°ê³¼

    // ğŸ†• v1.5 ìºì‹œëœ ë ˆí¼ëŸ°ìŠ¤
    var cachedReference: CachedReference?

    private var lastAnalysisTime = Date()
    private let analysisInterval: TimeInterval = 0.05  // 50msë§ˆë‹¤ ë¶„ì„ - ë°˜ì‘ì†ë„ ê°œì„ 

    // ğŸ”¥ ë¶„ì„ ì „ìš© ë°±ê·¸ë¼ìš´ë“œ í (UI ë¸”ë¡œí‚¹ ë°©ì§€)
    private let analysisQueue = DispatchQueue(label: "com.tryangle.analysis", qos: .userInitiated)
    private var isAnalyzing = false  // ë¶„ì„ ì¤‘ë³µ ë°©ì§€ í”Œë˜ê·¸

    // íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ë¥¼ ìœ„í•œ ìƒíƒœ ì¶”ì 
    private var feedbackHistory: [String: Int] = [:]  // ì¹´í…Œê³ ë¦¬ë³„ ì—°ì† ê°ì§€ íšŸìˆ˜
    private let historyThreshold = 3  // ğŸ”„ 3ë²ˆ ì—°ì† ê°ì§€ë˜ì–´ì•¼ í‘œì‹œ (ì•½ 0.3ì´ˆ) - ë°˜ì‘ì†ë„ ê°œì„ 
    private var perfectFrameCount = 0  // ì™„ë²½í•œ í”„ë ˆì„ ì—°ì† íšŸìˆ˜
    private let perfectThreshold = 5  // 5í”„ë ˆì„(ì•½ 0.5ì´ˆ) ì—°ì† ì™„ë²½í•´ì•¼ ê°ì§€ - ë°˜ì‘ì†ë„ ê°œì„ 

    // ğŸ†• ê³ ì • í”¼ë“œë°± (í•œ ë²ˆ í‘œì‹œë˜ë©´ í•´ê²°ë  ë•Œê¹Œì§€ ìœ ì§€)
    private var stickyFeedbacks: [String: FeedbackItem] = [:]  // ì¹´í…Œê³ ë¦¬ë³„ ê³ ì • í”¼ë“œë°±

    // ğŸ†• ì´ì „ í”„ë ˆì„ì˜ í”¼ë“œë°± (ì™„ë£Œ ê°ì§€ìš©)
    private var previousFeedbackIds = Set<String>()
    // ğŸ†• ì™„ë£Œ ê°ì§€ë¥¼ ìœ„í•œ íˆìŠ¤í…Œë¦¬ì‹œìŠ¤
    private var disappearedFeedbackHistory: [String: Int] = [:]  // ì‚¬ë¼ì§„ í”¼ë“œë°±ì˜ ì—°ì† íšŸìˆ˜
    private let disappearedThreshold = 2  // 2ë²ˆ ì—°ì† ì‚¬ë¼ì ¸ì•¼ ì™„ë£Œë¡œ íŒë‹¨ - ë°˜ì‘ì†ë„ ê°œì„ 

    // ğŸ†• ê³ ì • í”¼ë“œë°± ì¹´í…Œê³ ë¦¬ (í¬ì¦ˆ ê´€ë ¨ì€ ê³„ì† í‘œì‹œ)
    // pose_missing_partsëŠ” ì´ì œ ë ˆí¼ëŸ°ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ì œëŒ€ë¡œ ê°ì§€ë˜ë¯€ë¡œ sticky ì²˜ë¦¬
    private let stickyCategories: Set<String> = [
        "pose_left_arm",
        "pose_right_arm",
        "pose_left_leg",
        "pose_right_leg",
        "pose_missing_parts"
    ]

    // ğŸ”¥ RTMPose ë¶„ì„ê¸° (ONNX Runtime with CoreML EP)
    private var poseMLAnalyzer: PoseMLAnalyzer!
    private let compositionAnalyzer = CompositionAnalyzer()
    private let cameraAngleDetector = CameraAngleDetector()
    private let gazeTracker = GazeTracker()
    private let depthEstimator = DepthEstimator()
    private let poseComparator = AdaptivePoseComparator()
    private let gapAnalyzer = GapAnalyzer()
    private let feedbackGenerator = FeedbackGenerator()  // ğŸ—‘ï¸ êµ¬ì‹ (ë ˆê±°ì‹œ í˜¸í™˜ìš©)
    private let framingAnalyzer = FramingAnalyzer()  // ê¸°ì¡´ í”„ë ˆì´ë° ë¶„ì„ê¸°
    private let photographyFramingAnalyzer = PhotographyFramingAnalyzer()  // ì‚¬ì§„í•™ ê¸°ë°˜ í”„ë ˆì´ë° ë¶„ì„ê¸°
    // ğŸ—‘ï¸ stagedFeedbackGenerator ì‚­ì œ - Gate Systemìœ¼ë¡œ í†µí•©ë¨

    // ğŸ†• v1.5 í†µí•© Gate System (5ë‹¨ê³„)
    private let gateSystem = GateSystem.shared
    private let marginAnalyzer = MarginAnalyzer()
    private let personDetector = PersonDetector()  // ì •ë°€ BBox (30í”„ë ˆì„ë§ˆë‹¤)
    private let focalLengthEstimator = FocalLengthEstimator.shared  // ğŸ†• 35mm í™˜ì‚° ì´ˆì ê±°ë¦¬

    // ğŸ†• v1.5 í”„ë ˆì„ ì¹´ìš´í„° (Level ì²˜ë¦¬ìš©)
    private var frameCount = 0
    private var lastGroundingDINOBBox: CGRect?  // ë§ˆì§€ë§‰ Grounding DINO ê²°ê³¼ ìºì‹œ
    private var lastCompressionIndex: CGFloat?  // ë§ˆì§€ë§‰ ì••ì¶•ê° ìºì‹œ
    private var lastDepthResult: DepthResult?   // ë§ˆì§€ë§‰ Depth ê²°ê³¼ ìºì‹œ (Level 2)

    // ğŸ†• 35mm í™˜ì‚° ì´ˆì ê±°ë¦¬ ê´€ë ¨
    private var referenceImageData: Data?       // ë ˆí¼ëŸ°ìŠ¤ EXIF ì¶”ì¶œìš©
    private var referenceDepthMap: MLMultiArray?  // ë ˆí¼ëŸ°ìŠ¤ ëìŠ¤ë§µ (EXIF ì—†ì„ ë•Œ fallback)
    private var referenceFocalLength: FocalLengthInfo?  // ìºì‹œëœ ë ˆí¼ëŸ°ìŠ¤ ì´ˆì ê±°ë¦¬
    var currentZoomFactor: CGFloat = 1.0        // í˜„ì¬ ì¤Œ ë°°ìœ¨ (CameraManagerì—ì„œ ì—…ë°ì´íŠ¸)

    // ğŸ”¥ ì„±ëŠ¥ ìµœì í™”
    private let thermalManager = ThermalStateManager()
    private let frameSkipper = AdaptiveFrameSkipper()
    private var lastPerformanceLog = Date()

    // ğŸ†• ì´ˆê¸°í™”
    init() {
        print("ğŸ¬ğŸ¬ğŸ¬ RealtimeAnalyzer init() í˜¸ì¶œë¨ ğŸ¬ğŸ¬ğŸ¬")

        // ğŸ”¥ PoseMLAnalyzerë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¯¸ë¦¬ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ 17ì´ˆ ì§€ì—° ë°©ì§€)
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            print("ğŸ”¥ RealtimeAnalyzer: PoseMLAnalyzer ë°±ê·¸ë¼ìš´ë“œ ì´ˆê¸°í™” ì‹œì‘")
            let startTime = CACurrentMediaTime()
            let analyzer = PoseMLAnalyzer()
            let loadTime = (CACurrentMediaTime() - startTime) * 1000
            print("âœ… RealtimeAnalyzer: PoseMLAnalyzer ì´ˆê¸°í™” ì™„ë£Œ (\(String(format: "%.0f", loadTime))ms)")

            DispatchQueue.main.async {
                self?.poseMLAnalyzer = analyzer
            }
        }
    }

    // Vision ìš”ì²­ ìºì‹±
    private lazy var faceDetectionRequest: VNDetectFaceLandmarksRequest = {
        let request = VNDetectFaceLandmarksRequest()
        request.revision = VNDetectFaceLandmarksRequestRevision3
        return request
    }()

    private lazy var poseDetectionRequest: VNDetectHumanBodyPoseRequest = {
        let request = VNDetectHumanBodyPoseRequest()
        return request
    }()

    // MARK: - Helper Methods

    /// ì—¬ë°± ê³„ì‚° (RTMPose êµ¬ì¡°ì  í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
    private func calculatePaddingFromKeypoints(
        keypoints: [(point: CGPoint, confidence: Float)]
    ) -> ImagePadding? {
        // êµ¬ì¡°ì  í‚¤í¬ì¸íŠ¸ë§Œ ì‚¬ìš© (0-16: ëª¸í†µ í‚¤í¬ì¸íŠ¸, ì†ê°€ë½/ì–¼êµ´ ëœë“œë§ˆí¬ ì œì™¸)
        let structuralIndices = PhotographyFramingAnalyzer.StructuralKeypoints.all

        // ì‹ ë¢°ë„ 0.3 ì´ìƒì¸ í‚¤í¬ì¸íŠ¸ë§Œ í•„í„°ë§
        let validPoints = structuralIndices.compactMap { idx -> CGPoint? in
            guard idx < keypoints.count else { return nil }
            return keypoints[idx].confidence > 0.3 ? keypoints[idx].point : nil
        }

        // ìµœì†Œ 3ê°œ ì´ìƒì˜ í‚¤í¬ì¸íŠ¸ê°€ í•„ìš”
        guard validPoints.count >= 3 else { return nil }

        // ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° (ì •ê·œí™”ëœ ì¢Œí‘œ: 0.0 ~ 1.0)
        let minX = validPoints.map { $0.x }.min() ?? 0
        let maxX = validPoints.map { $0.x }.max() ?? 1
        let minY = validPoints.map { $0.y }.min() ?? 0
        let maxY = validPoints.map { $0.y }.max() ?? 1

        // ì—¬ë°± ê³„ì‚° (ì •ê·œí™”ëœ ì¢Œí‘œê³„)
        let top = 1.0 - maxY     // ìƒë‹¨ ì—¬ë°±
        let bottom = minY        // í•˜ë‹¨ ì—¬ë°±
        let left = minX          // ì¢Œì¸¡ ì—¬ë°±
        let right = 1.0 - maxX   // ìš°ì¸¡ ì—¬ë°±

        return ImagePadding(
            top: top,
            bottom: bottom,
            left: left,
            right: right
        )
    }

    /// ğŸ—‘ï¸ êµ¬ì‹ ì—¬ë°± ê³„ì‚° (ì–¼êµ´ ìœ„ì¹˜ ê¸°ë°˜ bodyRect ì¶”ì •) - ë” ì´ìƒ ì‚¬ìš© ì•ˆí•¨
    @available(*, deprecated, message: "Use calculatePaddingFromKeypoints instead")
    private func calculatePadding(bodyRect: CGRect?, imageSize: CGSize) -> ImagePadding? {
        guard let body = bodyRect else { return nil }

        // ğŸ”¥ Vision ì¢Œí‘œê³„: Y=0(í™”ë©´ í•˜ë‹¨), Y=1(í™”ë©´ ìƒë‹¨)
        // body.minY = ì¸ë¬¼ì˜ ì•„ë˜ìª½ ê²½ê³„ (Y ì‘ì€ ê°’)
        // body.maxY = ì¸ë¬¼ì˜ ìœ„ìª½ ê²½ê³„ (Y í° ê°’)

        let top = 1.0 - body.maxY  // í™”ë©´ ìƒë‹¨ ì—¬ë°± (ì¸ë¬¼ ìœ„ ê³µê°„)
        let bottom = body.minY     // í™”ë©´ í•˜ë‹¨ ì—¬ë°± (ì¸ë¬¼ ì•„ë˜ ê³µê°„)
        let left = body.minX       // ì¢Œì¸¡ ì—¬ë°±
        let right = 1.0 - body.maxX  // ìš°ì¸¡ ì—¬ë°±

        return ImagePadding(
            top: top,
            bottom: bottom,
            left: left,
            right: right
        )
    }

    /// ğŸ†• v6: í‚¤í¬ì¸íŠ¸ì—ì„œ ì¸ë¬¼ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° (Python improved_margin_analyzer._calculate_person_bbox ì´ì‹)
    /// - Returns: ì •ê·œí™”ëœ ì¢Œí‘œ (0.0 ~ 1.0)ì˜ ë°”ìš´ë”© ë°•ìŠ¤
    private func calculateBodyRectFromKeypoints(_ keypoints: [(point: CGPoint, confidence: Float)], imageSize: CGSize) -> CGRect? {
        // ì‹ ë¢°ë„ 0.3 ì´ìƒì¸ êµ¬ì¡°ì  í‚¤í¬ì¸íŠ¸(0-16)ë§Œ í•„í„°ë§
        let structuralIndices = PhotographyFramingAnalyzer.StructuralKeypoints.all

        let validPoints = structuralIndices.compactMap { idx -> CGPoint? in
            guard idx < keypoints.count else { return nil }
            return keypoints[idx].confidence > 0.3 ? keypoints[idx].point : nil
        }

        // ìµœì†Œ 3ê°œ ì´ìƒì˜ í‚¤í¬ì¸íŠ¸ê°€ í•„ìš”
        guard validPoints.count >= 3 else { return nil }

        // ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° (í”½ì…€ ì¢Œí‘œ)
        let minX = validPoints.map { $0.x }.min() ?? 0
        let maxX = validPoints.map { $0.x }.max() ?? 1
        let minY = validPoints.map { $0.y }.min() ?? 0
        let maxY = validPoints.map { $0.y }.max() ?? 1

        // ğŸ†• ì •ê·œí™” (0.0 ~ 1.0)
        let normalizedX = minX / imageSize.width
        let normalizedY = minY / imageSize.height
        let normalizedWidth = (maxX - minX) / imageSize.width
        let normalizedHeight = (maxY - minY) / imageSize.height

        return CGRect(x: normalizedX, y: normalizedY, width: normalizedWidth, height: normalizedHeight)
    }

    // MARK: - ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ë¶„ì„
    func analyzeReference(_ image: UIImage, imageData: Data? = nil) {
        print("========================================")
        print("ğŸ¯ğŸ¯ğŸ¯ ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘ ğŸ¯ğŸ¯ğŸ¯")
        print("========================================")

        // ğŸ†• EXIF ì¶”ì¶œìš© ì´ë¯¸ì§€ ë°ì´í„° ì €ì¥
        self.referenceImageData = imageData ?? image.jpegData(compressionQuality: 1.0)

        guard let cgImage = image.cgImage else {
            print("âŒ cgImage ì—†ìŒ")
            return
        }

        // ğŸ†• ëª¨ë¸ ë¡œë”© ëŒ€ê¸°
        guard let analyzer = poseMLAnalyzer else {
            print("â³ PoseMLAnalyzer ë¡œë”© ì¤‘... ë ˆí¼ëŸ°ìŠ¤ ë¶„ì„ ëŒ€ê¸°")
            // 0.5ì´ˆ í›„ ì¬ì‹œë„
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
                self?.analyzeReference(image)
            }
            return
        }

        print("ğŸ¯ ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ í¬ê¸°: \(cgImage.width) x \(cgImage.height)")
        print("ğŸ¯ ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ orientation: \(image.imageOrientation.rawValue)")

        // ğŸ”¥ RTMPoseë¡œ ì–¼êµ´+í¬ì¦ˆ ë™ì‹œ ë¶„ì„ (ONNX Runtime with CoreML EP)
        print("ğŸ¯ PoseMLAnalyzer.analyzeFaceAndPose() í˜¸ì¶œ ì¤‘...")
        let (faceResult, poseResult) = analyzer.analyzeFaceAndPose(from: image)
        print("ğŸ¯ ë¶„ì„ ì™„ë£Œ:")
        print("   - ì–¼êµ´: \(faceResult != nil ? "âœ… ê²€ì¶œë¨" : "âŒ ê²€ì¶œ ì•ˆë¨")")
        print("   - í¬ì¦ˆ: \(poseResult != nil ? "âœ… ê²€ì¶œë¨ (\(poseResult!.keypoints.count)ê°œ í‚¤í¬ì¸íŠ¸)" : "âŒ ê²€ì¶œ ì•ˆë¨")")

        if let pose = poseResult {
            let visibleCount = pose.keypoints.filter { $0.confidence >= 0.5 }.count
            print("   - í¬ì¦ˆ ì‹ ë¢°ë„ â‰¥ 0.5: \(visibleCount)/\(pose.keypoints.count)ê°œ")
        }

        // ğŸ”¥ ë””ë²„ê·¸: í¬ì¦ˆ ê²€ì¶œ ì‹¤íŒ¨ ì‹œ ì´ë¯¸ì§€ ì €ì¥
        if poseResult == nil {
            saveDebugImage(image, reason: "pose_detection_failed")
        }

        let faceRect = faceResult?.faceRect
        let faceYaw = faceResult?.yaw
        let facePitch = faceResult?.pitch
        let poseKeypoints = poseResult?.keypoints

        // ë°ê¸° ê³„ì‚°
        let brightness = poseMLAnalyzer.calculateBrightness(from: cgImage)

        // ğŸ†• ë”ì¹˜ í‹¸íŠ¸ ê°ì§€
        let tiltAngle = cameraAngleDetector.detectDutchTilt(faceObservation: faceResult?.observation) ?? 0.0

        // ì „ì‹  ì˜ì—­ ì¶”ì •
        let bodyRect = poseMLAnalyzer.estimateBodyRect(from: faceRect)

        // ì¹´ë©”ë¼ ì•µê¸€ ê°ì§€
        let cameraAngle = cameraAngleDetector.detectCameraAngle(
            faceRect: faceRect,
            facePitch: facePitch,
            faceObservation: faceResult?.observation
        )

        // êµ¬ë„ íƒ€ì… ë¶„ë¥˜
        var compositionType: CompositionType? = nil
        if let faceRect = faceRect {
            let subjectPosition = CGPoint(x: faceRect.midX, y: faceRect.midY)
            compositionType = compositionAnalyzer.classifyComposition(subjectPosition: subjectPosition)
        }

        // ğŸ†• ì‹œì„  ì¶”ì 
        var gaze: GazeResult? = nil
        if let faceObservation = faceResult?.observation {
            gaze = gazeTracker.trackGaze(from: faceObservation)
        }

        // ğŸ†• ê¹Šì´ ì¶”ì •
        var depth: DepthResult? = nil
        if let faceRect = faceRect {
            depth = depthEstimator.estimateDistance(
                faceRect: faceRect,
                imageWidth: cgImage.width,
                zoomFactor: 1.0  // TODO: CameraManagerì—ì„œ ì‹¤ì œ ì¤Œ ê°’ ê°€ì ¸ì˜¤ê¸°
            )
        }

        // ğŸ†• ë¹„ìœ¨ ê°ì§€
        let imageSize = CGSize(width: cgImage.width, height: cgImage.height)
        let aspectRatio = CameraAspectRatio.detect(from: imageSize)

        // ğŸ†• ì—¬ë°± ê³„ì‚° (RTMPose í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
        var padding: ImagePadding? = nil
        if let keypoints = poseKeypoints, keypoints.count >= 17 {
            // í‚¤í¬ì¸íŠ¸ë¥¼ ì •ê·œí™”ëœ ì¢Œí‘œë¡œ ë³€í™˜ (0.0 ~ 1.0)
            let normalizedKeypoints = keypoints.map { kp -> (point: CGPoint, confidence: Float) in
                let normalizedPoint = CGPoint(
                    x: kp.point.x / imageSize.width,
                    y: kp.point.y / imageSize.height
                )
                return (point: normalizedPoint, confidence: kp.confidence)
            }
            // êµ¬ì¡°ì  í‚¤í¬ì¸íŠ¸(0-16)ë¡œ ì—¬ë°± ê³„ì‚°
            padding = calculatePaddingFromKeypoints(keypoints: normalizedKeypoints)
        }

        // ğŸ†• ì‚¬ì§„í•™ ê¸°ë°˜ í”„ë ˆì´ë° ë¶„ì„ (RTMPose 133ê°œ í‚¤í¬ì¸íŠ¸)
        if let keypoints = poseKeypoints, keypoints.count >= 133 {
            let normalizedKeypoints = keypoints.map { kp -> (point: CGPoint, confidence: Float) in
                let normalizedPoint = CGPoint(
                    x: kp.point.x / imageSize.width,
                    y: kp.point.y / imageSize.height
                )
                return (point: normalizedPoint, confidence: kp.confidence)
            }
            referenceFramingResult = photographyFramingAnalyzer.analyze(
                keypoints: normalizedKeypoints,
                imageSize: imageSize
            )
            if let refFraming = referenceFramingResult {
                print("   - ğŸ“¸ ë ˆí¼ëŸ°ìŠ¤ ìƒ· íƒ€ì…: \(refFraming.shotType.rawValue)")
                print("   - ğŸ“¸ ë ˆí¼ëŸ°ìŠ¤ í—¤ë“œë£¸: \(String(format: "%.1f%%", refFraming.headroom * 100))")
                print("   - ğŸ“¸ ë ˆí¼ëŸ°ìŠ¤ ì¹´ë©”ë¼ ì•µê¸€: \(refFraming.cameraAngle.rawValue)")
            }
        } else {
            referenceFramingResult = nil
            print("   - âš ï¸ ì‚¬ì§„í•™ í”„ë ˆì´ë° ë¶„ì„ ë¶ˆê°€ (í‚¤í¬ì¸íŠ¸ ë¶€ì¡±)")
        }

        referenceAnalysis = FrameAnalysis(
            faceRect: faceRect,
            bodyRect: bodyRect,
            brightness: brightness,
            tiltAngle: tiltAngle,
            faceYaw: faceYaw,
            facePitch: facePitch,
            cameraAngle: cameraAngle,
            poseKeypoints: poseKeypoints,
            compositionType: compositionType,
            faceObservation: faceResult?.observation,
            gaze: gaze,
            depth: depth,
            aspectRatio: aspectRatio,
            imagePadding: padding
        )

        // ğŸ†• v1.5: Grounding DINOë¡œ ì •ë°€ BBox ë¶„ì„
        var preciseBBox: CGRect?
        if let ciImage = CIImage(image: image) {
            let semaphore = DispatchSemaphore(value: 0)
            personDetector.detectPerson(in: ciImage) { bbox in
                preciseBBox = bbox
                semaphore.signal()
            }
            semaphore.wait()
        }

        // ğŸ†• v1.5: ì—¬ë°± ë¶„ì„ ë° ìºì‹±
        if let bbox = preciseBBox ?? bodyRect {
            let marginResult = marginAnalyzer.analyze(
                bbox: bbox,
                imageSize: imageSize,
                isNormalized: true
            )

            // ìºì‹œ ì €ì¥
            let refId = UUID().uuidString
            cachedReference = CacheManager.shared.cacheReference(
                id: refId,
                image: image,
                bbox: bbox,
                margins: marginResult,
                compressionIndex: depth.map { CGFloat($0.compressionIndex) }
            )
            print("ğŸ“¦ v1.5 ë ˆí¼ëŸ°ìŠ¤ ìºì‹œ ì™„ë£Œ: \(refId)")
        }

        // ğŸ†• 35mm í™˜ì‚° ì´ˆì ê±°ë¦¬ ì¶”ì • (EXIF â†’ ëìŠ¤ë§µ ìˆœì„œ)
        // TODO: Depth Anything V2 CoreMLë¡œ ëìŠ¤ë§µ ìƒì„± í›„ ì €ì¥
        // í˜„ì¬ëŠ” EXIF ìš°ì„ , fallbackìœ¼ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©
        self.referenceFocalLength = focalLengthEstimator.estimateReferenceFocalLength(
            imageData: referenceImageData,
            depthMap: referenceDepthMap,  // TODO: ì‹¤ì œ ëìŠ¤ë§µ ì—°ë™
            fallback: 50  // í•¸ë“œí° ì‚¬ì§„ ê¸°ë³¸ê°’ (í‘œì¤€ ë Œì¦ˆ ì¶”ì •)
        )

        if let refFL = referenceFocalLength {
            print("ğŸ“ ë ˆí¼ëŸ°ìŠ¤ ì´ˆì ê±°ë¦¬: \(refFL.focalLength35mm)mm (\(refFL.lensType.displayName)) - \(refFL.source.description)")
        }

        print("========================================")
        print("ğŸ“¸ ë ˆí¼ëŸ°ìŠ¤ ë¶„ì„ ìµœì¢… ê²°ê³¼:")
        print("========================================")
        print("   - ë¹„ìœ¨: \(aspectRatio.displayName)")
        print("   - ì–¼êµ´: \(faceRect != nil ? "âœ… ê°ì§€ë¨" : "âŒ ì—†ìŒ")")
        print("   - ì–¼êµ´ ê°ë„: yaw=\(faceYaw ?? 0), pitch=\(facePitch ?? 0)")
        print("   - ì¹´ë©”ë¼ ì•µê¸€: \(cameraAngle.description)")
        print("   - êµ¬ë„: \(compositionType?.description ?? "ì•Œ ìˆ˜ ì—†ìŒ")")
        print("   - ì‹œì„ : \(gaze?.direction.description ?? "ì•Œ ìˆ˜ ì—†ìŒ")")
        print("   - ê±°ë¦¬: \(depth?.distance.map { String(format: "%.2fm", $0) } ?? "ì•Œ ìˆ˜ ì—†ìŒ")")
        print("   - ğŸ†• Grounding DINO BBox: \(preciseBBox != nil ? "âœ…" : "âŒ (RTMPose ì‚¬ìš©)")")

        if let keypoints = poseKeypoints {
            let visibleCount = keypoints.filter { $0.confidence >= 0.5 }.count
            print("   - í¬ì¦ˆ í‚¤í¬ì¸íŠ¸: \(keypoints.count)ê°œ (ì‹ ë¢°ë„ â‰¥ 0.5: \(visibleCount)ê°œ)")
            if visibleCount >= 5 {
                print("   - âœ… í¬ì¦ˆ ê²€ì¶œ ì„±ê³µ! UIì— í‘œì‹œë  ê²ƒì„")
            } else {
                print("   - âš ï¸ í¬ì¦ˆ ì‹ ë¢°ë„ ë‚®ìŒ - í¬ì¦ˆ ë¹„êµ ë¶ˆê°€ëŠ¥")
            }
        } else {
            print("   - âŒ í¬ì¦ˆ í‚¤í¬ì¸íŠ¸: ì—†ìŒ")
            print("   - âš ï¸ RTMPose í¬ì¦ˆ ê²€ì¶œ ì‹¤íŒ¨")
        }

        print("   - ë°ê¸°: \(brightness)")
        print("   - ê¸°ìš¸ê¸°: \(tiltAngle)ë„")
        print("========================================")
    }

    // MARK: - ì‹¤ì‹œê°„ í”„ë ˆì„ ë¶„ì„
    func analyzeFrame(_ image: UIImage, isFrontCamera: Bool = false, currentAspectRatio: CameraAspectRatio = .ratio4_3) {
        // ğŸ”¥ ë™ì  ë¶„ì„ ê°„ê²© (ë°œì—´ ìƒíƒœì— ë”°ë¼ ì¡°ì ˆ)
        let dynamicInterval = thermalManager.recommendedAnalysisInterval
        guard Date().timeIntervalSince(lastAnalysisTime) >= dynamicInterval else { return }

        // ì´ë¯¸ ë¶„ì„ ì¤‘ì´ë©´ ìŠ¤í‚µ (UI ë¸”ë¡œí‚¹ ë°©ì§€)
        guard !isAnalyzing else { return }

        // ë ˆí¼ëŸ°ìŠ¤ê°€ ì—†ìœ¼ë©´ ë¶„ì„í•˜ì§€ ì•ŠìŒ
        guard let reference = referenceAnalysis else {
            DispatchQueue.main.async {
                self.instantFeedback = []
                self.perfectScore = 0.0
                self.isPerfect = false
            }
            return
        }

        guard let cgImage = image.cgImage else { return }
        lastAnalysisTime = Date()
        isAnalyzing = true

        // ğŸ”¥ ë°±ê·¸ë¼ìš´ë“œ íì—ì„œ ë¶„ì„ ì‹¤í–‰ (UI ë¸”ë¡œí‚¹ ë°©ì§€)
        analysisQueue.async { [weak self] in
            guard let self = self else { return }

            // ğŸ†• ëª¨ë¸ ë¡œë”© ëŒ€ê¸° (ì•± ì‹œì‘ ì§í›„)
            guard let analyzer = self.poseMLAnalyzer else {
                print("â³ PoseMLAnalyzer ë¡œë”© ì¤‘... ë¶„ì„ ìŠ¤í‚µ")
                DispatchQueue.main.async {
                    self.isAnalyzing = false
                }
                return
            }

            let analysisStart = CACurrentMediaTime()  // ğŸ” í”„ë¡œíŒŒì¼ë§

            // RTMPoseë¡œ ë¶„ì„ (ONNX Runtime with CoreML EP)
            let poseStart = CACurrentMediaTime()  // ğŸ”
            let (faceResult, poseResult) = analyzer.analyzeFaceAndPose(from: image)
            let poseEnd = CACurrentMediaTime()  // ğŸ”

            let analysisEnd = CACurrentMediaTime()  // ğŸ”

            // ğŸ” í”„ë¡œíŒŒì¼ë§ ë¡œê·¸ (ë§¤ ë¶„ì„ë§ˆë‹¤)
            let poseTime = (poseEnd - poseStart) * 1000
            let totalTime = (analysisEnd - analysisStart) * 1000
            print("ğŸ“Š [RealtimeAnalyzer] RTMPose: \(String(format: "%.1f", poseTime))ms, ì´ë¶„ì„: \(String(format: "%.1f", totalTime))ms")

            // ë¶„ì„ ì™„ë£Œ í›„ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ UI ì—…ë°ì´íŠ¸
            DispatchQueue.main.async {
                self.isAnalyzing = false
                self.processAnalysisResult(
                    faceResult: faceResult,
                    poseResult: poseResult,
                    cgImage: cgImage,
                    reference: reference,
                    isFrontCamera: isFrontCamera,
                    currentAspectRatio: currentAspectRatio
                )
            }
        }
    }

    // MARK: - ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ (ë©”ì¸ ìŠ¤ë ˆë“œ)
    private func processAnalysisResult(
        faceResult: FaceAnalysisResult?,
        poseResult: PoseAnalysisResult?,
        cgImage: CGImage,
        reference: FrameAnalysis,
        isFrontCamera: Bool,
        currentAspectRatio: CameraAspectRatio
    ) {
        // ğŸ†• v1.5: í”„ë ˆì„ ì¹´ìš´í„° ì¦ê°€
        frameCount += 1

        // ğŸ”¥ ì„±ëŠ¥ ë¡œê·¸ (10ì´ˆë§ˆë‹¤)
        if Date().timeIntervalSince(lastPerformanceLog) >= 10 {
            lastPerformanceLog = Date()
            print(PerformanceOptimizer.shared.getPerformanceReport())
            print("ğŸŒ¡ï¸ ë°œì—´ ìƒíƒœ: \(thermalManager.currentThermalState.rawValue), ë¶„ì„ ê°„ê²©: \(Int(thermalManager.recommendedAnalysisInterval * 1000))ms")
        }

        // ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ì™„ì„±ë„ 0ìœ¼ë¡œ ì„¤ì •
        guard faceResult != nil else {
            self.instantFeedback = [FeedbackItem(
                priority: 1,
                icon: "ğŸ‘¤",
                message: "ì–¼êµ´ì„ í™”ë©´ì— ë³´ì—¬ì£¼ì„¸ìš”",
                category: "no_face",
                currentValue: nil,
                targetValue: nil,
                tolerance: nil,
                unit: nil
            )]
            self.perfectScore = 0.0
            self.isPerfect = false
            return
        }

        // ë°ê¸° ë° ê¸°ìš¸ê¸°
        let brightness = poseMLAnalyzer.calculateBrightness(from: cgImage)
        let tilt = cameraAngleDetector.detectDutchTilt(faceObservation: faceResult?.observation) ?? 0.0

        // ğŸ†• ì´ë¯¸ì§€ í¬ê¸° (ì •ê·œí™”ì— í•„ìš”)
        let imageSize = CGSize(width: cgImage.width, height: cgImage.height)

        // ğŸ†• ì „ì‹  ì˜ì—­ - RTMPose í‚¤í¬ì¸íŠ¸ì—ì„œ ì •í™•í•˜ê²Œ ê³„ì‚° (ì •ê·œí™”ëœ ì¢Œí‘œ)
        let bodyRect: CGRect? = {
            if let keypoints = poseResult?.keypoints, !keypoints.isEmpty {
                return calculateBodyRectFromKeypoints(keypoints, imageSize: imageSize)
            }
            // RTMPose í‚¤í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ì–¼êµ´ ê¸°ë°˜ ì¶”ì • (fallback) - ì´ë¯¸ ì •ê·œí™”ë¨
            return poseMLAnalyzer.estimateBodyRect(from: faceResult?.faceRect)
        }()

        // ì¹´ë©”ë¼ ì•µê¸€
        let cameraAngle = cameraAngleDetector.detectCameraAngle(
            faceRect: faceResult?.faceRect,
            facePitch: faceResult?.pitch,
            faceObservation: faceResult?.observation
        )

        // êµ¬ë„
        var compositionType: CompositionType? = nil
        if let faceRect = faceResult?.faceRect {
            let subjectPosition = CGPoint(x: faceRect.midX, y: faceRect.midY)
            compositionType = compositionAnalyzer.classifyComposition(subjectPosition: subjectPosition)
        }

        // ì‹œì„ 
        var gaze: GazeResult? = nil
        if let faceObservation = faceResult?.observation {
            gaze = gazeTracker.trackGaze(from: faceObservation)
        }

        // ğŸ†• Level 2: ê¹Šì´ ì¶”ì • (ë™ì  í”„ë ˆì„ ìŠ¤í‚µ)
        var depth: DepthResult? = lastDepthResult  // ìºì‹œëœ ê°’ ì‚¬ìš©
        if frameSkipper.shouldExecute(level: 2, frameCount: frameCount) {
            // ë™ì  ê°„ê²©ìœ¼ë¡œ ìƒˆë¡œ ê³„ì‚°
            if let faceRect = faceResult?.faceRect {
                depth = depthEstimator.estimateDistance(
                    faceRect: faceRect,
                    imageWidth: cgImage.width,
                    zoomFactor: 1.0  // TODO: ì‹¤ì œ ì¤Œ ê°’
                )
                lastDepthResult = depth  // ìºì‹œ ì—…ë°ì´íŠ¸
            }
        }

        // ğŸ†• í˜„ì¬ ì´ë¯¸ì§€ í¬ê¸° (ìœ„ì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨)
        let currentImageSize = imageSize

        // ğŸ†• ì—¬ë°± ê³„ì‚° (RTMPose í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
        var currentPadding: ImagePadding? = nil
        if let keypoints = poseResult?.keypoints, keypoints.count >= 17 {
            // í‚¤í¬ì¸íŠ¸ë¥¼ ì •ê·œí™”ëœ ì¢Œí‘œë¡œ ë³€í™˜ (0.0 ~ 1.0)
            let normalizedKeypoints = keypoints.map { kp -> (point: CGPoint, confidence: Float) in
                let normalizedPoint = CGPoint(
                    x: kp.point.x / currentImageSize.width,
                    y: kp.point.y / currentImageSize.height
                )
                return (point: normalizedPoint, confidence: kp.confidence)
            }
            // êµ¬ì¡°ì  í‚¤í¬ì¸íŠ¸(0-16)ë¡œ ì—¬ë°± ê³„ì‚°
            currentPadding = calculatePaddingFromKeypoints(keypoints: normalizedKeypoints)
        }

        // ğŸ†• í”„ë ˆì´ë° ë¶„ì„ ì¶”ê°€ (ìµœìš°ì„ )
        let currentFrame = FrameAnalysis(
            faceRect: faceResult?.faceRect,
            bodyRect: bodyRect,
            brightness: brightness,
            tiltAngle: tilt,
            faceYaw: faceResult?.yaw,
            facePitch: faceResult?.pitch,
            cameraAngle: cameraAngle,
            poseKeypoints: poseResult?.keypoints,
            compositionType: compositionType,
            faceObservation: faceResult?.observation,
            gaze: gaze,
            depth: depth,
            aspectRatio: currentAspectRatio,
            imagePadding: currentPadding
        )

        // ğŸ—‘ï¸ ë¹„ìœ¨ ë¶ˆì¼ì¹˜ ì²´í¬ëŠ” ì´ì œ StagedFeedbackGeneratorê°€ ì²˜ë¦¬ (Phase 3)

        // ============================================
        // ğŸ†• v1.5 í†µí•© Gate System í‰ê°€ (5ë‹¨ê³„)
        // ============================================

        // Level 3: Grounding DINOë¡œ ì •ë°€ BBox ê°±ì‹  (ë™ì  í”„ë ˆì„ ìŠ¤í‚µ)
        if frameSkipper.shouldExecute(level: 3, frameCount: frameCount) {
            let uiImage = UIImage(cgImage: cgImage)
            if let ciImage = CIImage(image: uiImage) {
                // ğŸ”¥ Level 3 ì „ìš© ë°±ê·¸ë¼ìš´ë“œ íì—ì„œ ë¹„ë™ê¸° ì‹¤í–‰
                PerformanceOptimizer.shared.level3Queue.async { [weak self] in
                    self?.personDetector.detectPerson(in: ciImage) { bbox in
                        DispatchQueue.main.async {
                            // ğŸ”§ FIX: nilì¼ ë•Œë„ ì—…ë°ì´íŠ¸í•˜ì—¬ ìºì‹œ stale ë°©ì§€
                            self?.lastGroundingDINOBBox = bbox
                            if bbox == nil {
                                print("âš ï¸ Grounding DINO: ì¸ë¬¼ ë¯¸ê°ì§€ - BBox ì´ˆê¸°í™”")
                            }
                        }
                    }
                }
            }
        }

        // ğŸ”§ FIX: í˜„ì¬ BBox ê²°ì • - ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬ ì¶”ê°€
        // bodyRectê°€ ìˆìœ¼ë©´ í˜„ì¬ í”„ë ˆì„ì— ì¸ë¬¼ì´ ìˆë‹¤ëŠ” ì˜ë¯¸ â†’ ê·¸ê²ƒ ì‚¬ìš©
        // bodyRectê°€ ì—†ê³  ìºì‹œëœ BBoxë„ ì—†ìœ¼ë©´ â†’ ê¸°ë³¸ê°’ (ê±°ì˜ ì—†ëŠ” ì¸ë¬¼)
        let currentBBox: CGRect
        if let body = bodyRect {
            // Visionì—ì„œ í˜„ì¬ í”„ë ˆì„ì— ì¸ë¬¼ ê°ì§€ë¨ â†’ bodyRect ë˜ëŠ” DINO ê²°ê³¼ ì‚¬ìš©
            currentBBox = lastGroundingDINOBBox ?? body
        } else if lastGroundingDINOBBox != nil {
            // Visionì€ ì¸ë¬¼ ëª» ì°¾ì•˜ì§€ë§Œ DINOì—ì„œëŠ” ì°¾ìŒ â†’ DINO ê²°ê³¼ ì‚¬ìš©
            currentBBox = lastGroundingDINOBBox!
        } else {
            // ë‘˜ ë‹¤ ì¸ë¬¼ ì—†ìŒ â†’ ì‘ì€ ê¸°ë³¸ê°’ (ì¸ë¬¼ ë¯¸ê²€ì¶œë¡œ ì²˜ë¦¬ë¨)
            currentBBox = CGRect(x: 0.45, y: 0.45, width: 0.01, height: 0.01)
        }

        // ğŸ”§ FIX: ì••ì¶•ê°ì€ í˜„ì¬ í”„ë ˆì„ ê°’ ì‚¬ìš© (ìºì‹œ ì˜ì¡´ ì œê±°)
        // depthê°€ nilì´ë©´ ì••ì¶•ê°ë„ nilë¡œ ì „ë‹¬ â†’ Gateì—ì„œ "ë¶„ì„ ì¤‘" í‘œì‹œ
        let currentCompressionIndex: CGFloat?
        if let depthResult = depth {
            currentCompressionIndex = CGFloat(depthResult.compressionIndex)
            lastCompressionIndex = currentCompressionIndex  // ìºì‹œë„ ì—…ë°ì´íŠ¸
        } else {
            // ğŸ”§ ìºì‹œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - í˜„ì¬ í”„ë ˆì„ì— depth ì—†ìœ¼ë©´ nil
            currentCompressionIndex = nil
        }

        // í¬ì¦ˆ ë¹„êµ (Gate 4ìš©)
        var poseComparison: PoseComparisonResult? = nil
        if let refKeypoints = reference.poseKeypoints,
           let curKeypoints = poseResult?.keypoints,
           refKeypoints.count >= 133 && curKeypoints.count >= 133 {

            poseComparison = poseComparator.comparePoses(
                referenceKeypoints: refKeypoints,
                currentKeypoints: curKeypoints
            )
        }

        // í†µí•© Gate System í‰ê°€
        var stableFeedback: [FeedbackItem] = []

        // ğŸ†• v6: í‚¤í¬ì¸íŠ¸ ë³€í™˜ (tuple â†’ PoseKeypoint)
        let currentPoseKeypoints: [PoseKeypoint]? = poseResult?.keypoints.map { kp in
            PoseKeypoint(location: kp.point, confidence: kp.confidence)
        }
        let referencePoseKeypoints: [PoseKeypoint]? = reference.poseKeypoints?.map { kp in
            PoseKeypoint(location: kp.point, confidence: kp.confidence)
        }

        if let cached = cachedReference {
            // ğŸ†• 35mm í™˜ì‚° ì´ˆì ê±°ë¦¬ ê³„ì‚°
            let currentFocalLength = focalLengthEstimator.focalLengthFromZoom(currentZoomFactor)

            let evaluation = gateSystem.evaluate(
                currentBBox: currentBBox,
                referenceBBox: cached.bbox,
                currentImageSize: currentImageSize,
                referenceImageSize: cached.imageSize,
                compressionIndex: currentCompressionIndex,  // ğŸ”§ FIX: ìºì‹œ ëŒ€ì‹  í˜„ì¬ ê°’ ì‚¬ìš©
                referenceCompressionIndex: cached.compressionIndex,
                currentAspectRatio: currentAspectRatio,
                referenceAspectRatio: reference.aspectRatio,
                poseComparison: poseComparison,
                isFrontCamera: isFrontCamera,
                currentKeypoints: currentPoseKeypoints,          // ğŸ†• v6: í˜„ì¬ í‚¤í¬ì¸íŠ¸
                referenceKeypoints: referencePoseKeypoints,      // ğŸ†• v6: ë ˆí¼ëŸ°ìŠ¤ í‚¤í¬ì¸íŠ¸
                currentFocalLength: currentFocalLength,          // ğŸ†• í˜„ì¬ 35mm í™˜ì‚° ì´ˆì ê±°ë¦¬
                referenceFocalLength: referenceFocalLength       // ğŸ†• ë ˆí¼ëŸ°ìŠ¤ 35mm í™˜ì‚° ì´ˆì ê±°ë¦¬
            )

            // v1.5 ê²°ê³¼ ì €ì¥
            self.gateEvaluation = evaluation
            self.v15Feedback = evaluation.primaryFeedback

            // ğŸ†• v1.5 í†µí•© í”¼ë“œë°± ìƒì„± (í•˜ë‚˜ì˜ ë™ì‘ â†’ ì—¬ëŸ¬ Gate í•´ê²°)
            // ğŸ”§ ì••ì¶•ê° ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ í”¼ë“œë°±: ì¤Œ ì •ë³´ + ì¸ë¬¼ í¬ê¸° ì „ë‹¬
            let currentFocal = focalLengthEstimator.focalLengthFromZoom(currentZoomFactor)
            let targetZoomValue = referenceFocalLength.map {
                CGFloat($0.focalLength35mm) / CGFloat(FocalLengthEstimator.iPhoneBaseFocalLength)
            }

            self.unifiedFeedback = UnifiedFeedbackGenerator.shared.generateUnifiedFeedback(
                from: evaluation,
                isFrontCamera: isFrontCamera,
                currentZoom: currentZoomFactor,
                targetZoom: targetZoomValue,
                currentSubjectSize: currentBBox.width * currentBBox.height,  // ì ìœ ìœ¨ (ì •ê·œí™”ë¨)
                targetSubjectSize: cached.bbox.width * cached.bbox.height     // ë ˆí¼ëŸ°ìŠ¤ ì ìœ ìœ¨
            )

            // Gate System í”¼ë“œë°± ìƒì„±
            let gateFeedbacks = V15FeedbackGenerator.shared.generateFeedbackItems(from: evaluation)

            // íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ì ìš©
            for fb in gateFeedbacks {
                feedbackHistory[fb.category, default: 0] += 1

                if feedbackHistory[fb.category]! >= historyThreshold {
                    stableFeedback.append(fb)
                }
            }

            // ì‚¬ë¼ì§„ ì¹´í…Œê³ ë¦¬ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
            let currentCategories = Set(gateFeedbacks.map { $0.category })
            for (category, _) in feedbackHistory {
                if !currentCategories.contains(category) {
                    feedbackHistory[category] = 0
                }
            }

            print("ğŸ¯ v1.5 Gate: \(evaluation.passedCount)/5 í†µê³¼, ì ìˆ˜: \(String(format: "%.0f%%", Double(evaluation.overallScore) * 100))")
        }

        // ============================================

        // ì™„ë²½ ìƒíƒœ ê°ì§€ (Gate System ê¸°ì¤€)
        let isCurrentlyPerfect = gateEvaluation?.allPassed ?? false
        let score = gateEvaluation.map { Double($0.overallScore) } ?? 0.0

        if isCurrentlyPerfect {
            perfectFrameCount += 1
        } else {
            perfectFrameCount = 0
        }

        // ğŸ†• ì™„ë£Œëœ í”¼ë“œë°± ê°ì§€ (íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ì ìš©)
        let currentFeedbackIds = Set(stableFeedback.map { $0.id })
        let disappeared = previousFeedbackIds.subtracting(currentFeedbackIds)

        // ì‚¬ë¼ì§„ í”¼ë“œë°±ì˜ ì—°ì† íšŸìˆ˜ ì¶”ì 
        for disappearedId in disappeared {
            disappearedFeedbackHistory[disappearedId, default: 0] += 1

            // 5ë²ˆ ì—°ì† ì‚¬ë¼ì§€ë©´ ì™„ë£Œë¡œ íŒë‹¨
            if disappearedFeedbackHistory[disappearedId]! >= disappearedThreshold {
                if let completedItem = instantFeedback.first(where: { $0.id == disappearedId }) {
                    let completed = CompletedFeedback(item: completedItem, completedAt: Date())
                    completedFeedbacks.append(completed)
                }
                // ì™„ë£Œ ì²˜ë¦¬ í›„ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
                disappearedFeedbackHistory[disappearedId] = 0
            }
        }

        // ë‹¤ì‹œ ë‚˜íƒ€ë‚œ í”¼ë“œë°±ì€ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        for (feedbackId, _) in disappearedFeedbackHistory {
            if currentFeedbackIds.contains(feedbackId) {
                disappearedFeedbackHistory[feedbackId] = 0
            }
        }

        // 2ì´ˆ ì§€ë‚œ ì™„ë£Œ í”¼ë“œë°± ì œê±°
        completedFeedbacks.removeAll { !$0.shouldDisplay }

        // ì´ì „ í”¼ë“œë°± ì—…ë°ì´íŠ¸
        previousFeedbackIds = currentFeedbackIds

        // ğŸ†• ì¹´í…Œê³ ë¦¬ë³„ ìƒíƒœ ê³„ì‚°
        let categoryStatuses = calculateCategoryStatuses(from: stableFeedback)

        // ì¦‰ì‹œ í”¼ë“œë°± ì—…ë°ì´íŠ¸ (ë©”ì¸ ì“°ë ˆë“œì—ì„œ ì§ì ‘ ì‹¤í–‰ ì¤‘ì´ë¯€ë¡œ async ë¶ˆí•„ìš”)
        self.instantFeedback = stableFeedback
        self.perfectScore = score
        self.isPerfect = perfectFrameCount >= perfectThreshold
        self.categoryStatuses = categoryStatuses
    }

    // MARK: - Category Status Calculation

    /// ì¹´í…Œê³ ë¦¬ë³„ ìƒíƒœ ê³„ì‚°
    private func calculateCategoryStatuses(from feedbacks: [FeedbackItem]) -> [CategoryStatus] {
        // ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ìƒíƒœ ìƒì„±
        var statusMap: [FeedbackCategory: CategoryStatus] = [:]

        // ê° ì¹´í…Œê³ ë¦¬ ì´ˆê¸°í™” (ëª¨ë‘ ë§Œì¡± ìƒíƒœë¡œ ì‹œì‘)
        for category in FeedbackCategory.allCases {
            statusMap[category] = CategoryStatus(
                category: category,
                isSatisfied: true,
                activeFeedbacks: []
            )
        }

        // í”¼ë“œë°±ì´ ìˆëŠ” ì¹´í…Œê³ ë¦¬ëŠ” ë¶ˆë§Œì¡± ìƒíƒœë¡œ ë³€ê²½
        for feedback in feedbacks {
            if let category = FeedbackCategory.from(categoryString: feedback.category) {
                var activeFeedbacks = statusMap[category]?.activeFeedbacks ?? []
                activeFeedbacks.append(feedback)

                statusMap[category] = CategoryStatus(
                    category: category,
                    isSatisfied: false,
                    activeFeedbacks: activeFeedbacks.sorted { $0.priority < $1.priority }
                )
            }
        }

        // ìš°ì„ ìˆœìœ„ ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
        return Array(statusMap.values).sorted { $0.priority < $1.priority }
    }

    // ğŸ—‘ï¸ êµ¬ì‹ í•¨ìˆ˜ë“¤ ì œê±°ë¨ (ìƒˆ ì»´í¬ë„ŒíŠ¸ë¡œ ëŒ€ì²´)
    // - calculatePerfectScore() â†’ GapAnalyzer.calculateCompletionScore() ì‚¬ìš©
    // - calculateBrightness() â†’ VisionAnalyzer.calculateBrightness() ì‚¬ìš©
    // - calculateTilt() â†’ CameraAngleDetector.detectDutchTilt() ì‚¬ìš©
    // - estimateBodyRect() â†’ VisionAnalyzer.estimateBodyRect() ì‚¬ìš©
    // - extractPoseKeypoints() â†’ VisionAnalyzer ë‚´ë¶€ ì‚¬ìš©
    // - estimateCameraAngle() â†’ CameraAngleDetector ì‚¬ìš©
    // - comparePoseKeypoints() â†’ AdaptivePoseComparator ì‚¬ìš©
    // - calculateAngle() â†’ AdaptivePoseComparator ë‚´ë¶€ ì‚¬ìš©

    // MARK: - ë””ë²„ê·¸ í—¬í¼
    private func saveDebugImage(_ image: UIImage, reason: String) {
        guard let data = image.jpegData(compressionQuality: 0.8) else { return }

        let timestamp = DateFormatter.localizedString(from: Date(), dateStyle: .none, timeStyle: .medium)
            .replacingOccurrences(of: ":", with: "-")
        let filename = "debug_\(reason)_\(timestamp).jpg"

        if let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first {
            let fileURL = documentsPath.appendingPathComponent(filename)
            try? data.write(to: fileURL)
            print("ğŸ” ë””ë²„ê·¸ ì´ë¯¸ì§€ ì €ì¥: \(fileURL.path)")
        }
    }
}