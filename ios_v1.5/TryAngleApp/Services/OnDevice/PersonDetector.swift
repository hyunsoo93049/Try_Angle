// PersonDetector.swift
// ì‚¬ëŒ ê²€ì¶œ í†µí•© ì¸í„°í˜ì´ìŠ¤
// Grounding DINO (ONNX Runtime) + Vision Framework í´ë°±
// ì‘ì„±ì¼: 2025-12-05
// ìˆ˜ì •ì¼: 2025-12-09 - íŒŒì¼ëª… ë³€ê²½ (GroundingDINOCoreML â†’ PersonDetector)

import CoreML
import Vision
import CoreImage

class PersonDetector {

    // ONNX ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    private var onnxModel: GroundingDINOONNX?
    private var useONNX: Bool = false
    private var isLoading: Bool = true

    // MARK: - Initialization
    init() {
        print("ğŸ”„ Grounding DINO ONNX ëª¨ë¸ ë¡œë”© ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)...")

        // ONNX ëª¨ë¸ ì´ˆê¸°í™” (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¡œë”©ë¨)
        let onnx = GroundingDINOONNX { [weak self] success in
            guard let self = self else { return }

            self.isLoading = false

            if success {
                self.useONNX = true
                print("âœ… Grounding DINO ì´ˆê¸°í™” ì™„ë£Œ (ONNX Runtime)")
            } else {
                self.useONNX = false
                print("âš ï¸ ONNX ì„¸ì…˜ ë¡œë“œ ì‹¤íŒ¨, Vision Framework í´ë°± ì‚¬ìš©")
            }
        }

        onnxModel = onnx
    }

    // MARK: - Person Detection
    func detectPerson(in image: CIImage, completion: @escaping (CGRect?) -> Void) {
        // ë¡œë”© ì¤‘ì´ë©´ ONNX ëª¨ë¸ì˜ isSessionLoadedë¥¼ ì§ì ‘ ì²´í¬
        if let onnx = onnxModel, onnx.isSessionLoaded {
            // ONNX Runtime ì‚¬ìš©
            onnx.detectPerson(in: image, completion: completion)
        } else if isLoading {
            // ì•„ì§ ë¡œë”© ì¤‘ì´ë©´ Vision Frameworkë¡œ ì¼ë‹¨ ì²˜ë¦¬
            detectPersonWithVision(in: image, completion: completion)
        } else {
            // Vision Framework í´ë°±
            detectPersonWithVision(in: image, completion: completion)
        }
    }

    // MARK: - Vision Framework Fallback
    private func detectPersonWithVision(in image: CIImage, completion: @escaping (CGRect?) -> Void) {
        let request = VNDetectHumanRectanglesRequest { request, error in
            if let error = error {
                print("âŒ Vision person detection ì‹¤íŒ¨: \(error)")
                completion(nil)
                return
            }

            if let results = request.results as? [VNHumanObservation],
               let firstPerson = results.first {
                completion(firstPerson.boundingBox)
            } else {
                completion(nil)
            }
        }

        request.upperBodyOnly = false

        let handler = VNImageRequestHandler(ciImage: image, options: [:])
        DispatchQueue.global(qos: .userInitiated).async {
            do {
                try handler.perform([request])
            } catch {
                print("âŒ Vision ì²˜ë¦¬ ì‹¤íŒ¨: \(error)")
                DispatchQueue.main.async {
                    completion(nil)
                }
            }
        }
    }

    // MARK: - Multiple Person Detection
    func detectAllPersons(in image: CIImage, completion: @escaping ([Detection]) -> Void) {
        // ë¡œë”© ì¤‘ì´ë©´ ONNX ëª¨ë¸ì˜ isSessionLoadedë¥¼ ì§ì ‘ ì²´í¬
        if let onnx = onnxModel, onnx.isSessionLoaded {
            onnx.detectAllPersons(in: image, completion: completion)
        } else {
            detectAllPersonsWithVision(in: image, completion: completion)
        }
    }

    private func detectAllPersonsWithVision(in image: CIImage, completion: @escaping ([Detection]) -> Void) {
        let request = VNDetectHumanRectanglesRequest { request, error in
            if let error = error {
                print("âŒ Vision person detection ì‹¤íŒ¨: \(error)")
                completion([])
                return
            }

            if let results = request.results as? [VNHumanObservation] {
                let detections = results.map { observation in
                    Detection(
                        label: "person",
                        confidence: observation.confidence,
                        boundingBox: observation.boundingBox
                    )
                }
                completion(detections)
            } else {
                completion([])
            }
        }

        request.upperBodyOnly = false

        let handler = VNImageRequestHandler(ciImage: image, options: [:])
        DispatchQueue.global(qos: .userInitiated).async {
            do {
                try handler.perform([request])
            } catch {
                print("âŒ Vision ì²˜ë¦¬ ì‹¤íŒ¨: \(error)")
                DispatchQueue.main.async {
                    completion([])
                }
            }
        }
    }

    // MARK: - Text-Guided Detection
    func detectWithText(in image: CIImage, text: String, completion: @escaping ([Detection]) -> Void) {
        if text.lowercased().contains("person") || text.lowercased().contains("ì‚¬ëŒ") {
            detectAllPersons(in: image, completion: completion)
        } else {
            print("âš ï¸ í˜„ì¬ 'person' ê²€ì¶œë§Œ ì§€ì›ë©ë‹ˆë‹¤")
            completion([])
        }
    }

    // MARK: - Model Info
    var isUsingONNX: Bool {
        // ì‹¤ì œ ë¡œë“œ ìƒíƒœ í™•ì¸
        return onnxModel?.isSessionLoaded ?? false
    }

    var modelDescription: String {
        if isUsingONNX {
            return "Grounding DINO (ONNX Runtime)"
        } else if isLoading {
            return "Grounding DINO (ë¡œë”© ì¤‘...)"
        } else {
            return "Vision Framework (VNDetectHumanRectanglesRequest)"
        }
    }
}

// MARK: - Detection Result
struct Detection {
    let label: String
    let confidence: Float
    let boundingBox: CGRect
}

// MARK: - Legacy System Port
extension PersonDetector {

    // legacy_analyzer.pyì˜ calculate_marginsë¥¼ Swiftë¡œ í¬íŒ…
    func calculateMargins(personBBox: CGRect, imageSize: CGSize) -> MarginAnalysis {
        let imageWidth = imageSize.width
        let imageHeight = imageSize.height

        // bboxë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜ (Visionì€ normalized coordinates ì‚¬ìš©)
        let x = personBBox.origin.x * imageWidth
        let y = personBBox.origin.y * imageHeight
        let w = personBBox.width * imageWidth
        let h = personBBox.height * imageHeight

        // ì—¬ë°± ê³„ì‚°
        let leftMargin = x
        let rightMargin = imageWidth - (x + w)
        let topMargin = y
        let bottomMargin = imageHeight - (y + h)

        // ë¹„ìœ¨ ê³„ì‚°
        let leftRatio = leftMargin / imageWidth
        let rightRatio = rightMargin / imageWidth
        let topRatio = topMargin / imageHeight
        let bottomRatio = bottomMargin / imageHeight

        // ê· í˜• ì ìˆ˜ ê³„ì‚°
        let horizontalBalance = 1.0 - abs(leftRatio - rightRatio)
        let verticalBalance = 1.0 - abs(topRatio - bottomRatio * 0.5) // í•˜ë‹¨ 2:1 ë¹„ìœ¨ ì„ í˜¸
        let balanceScore = (horizontalBalance + verticalBalance) / 2.0

        return MarginAnalysis(
            left: leftMargin,
            right: rightMargin,
            top: topMargin,
            bottom: bottomMargin,
            leftRatio: leftRatio,
            rightRatio: rightRatio,
            topRatio: topRatio,
            bottomRatio: bottomRatio,
            balanceScore: balanceScore
        )
    }

    // í”„ë ˆì´ë° ë¶„ì„
    func analyzeFraming(personBBox: CGRect, imageSize: CGSize) -> V15FramingAnalysis {
        let margins = calculateMargins(personBBox: personBBox, imageSize: imageSize)

        // í”„ë ˆì´ë° íƒ€ì… ê²°ì •
        let framingType: V15FramingType
        let bboxHeightRatio = personBBox.height

        if bboxHeightRatio > 0.8 {
            framingType = .tooTight
        } else if bboxHeightRatio < 0.3 {
            framingType = .tooLoose
        } else if bboxHeightRatio > 0.6 {
            framingType = .closeUp
        } else if bboxHeightRatio > 0.4 {
            framingType = .medium
        } else {
            framingType = .wide
        }

        // í¬ë¡­ ì´ìŠˆ ì²´í¬
        let hasCropIssue = margins.left < 10 || margins.right < 10 ||
                           margins.top < 10 || margins.bottom < 10

        return V15FramingAnalysis(
            type: framingType,
            score: margins.balanceScore,
            hasCropIssue: hasCropIssue,
            margins: margins
        )
    }
}

// MARK: - Data Structures
struct MarginAnalysis {
    let left: CGFloat
    let right: CGFloat
    let top: CGFloat
    let bottom: CGFloat
    let leftRatio: CGFloat
    let rightRatio: CGFloat
    let topRatio: CGFloat
    let bottomRatio: CGFloat
    let balanceScore: CGFloat
}

struct V15FramingAnalysis {
    let type: V15FramingType
    let score: CGFloat
    let hasCropIssue: Bool
    let margins: MarginAnalysis
}

enum V15FramingType {
    case tooTight
    case tooLoose
    case closeUp
    case medium
    case wide
}