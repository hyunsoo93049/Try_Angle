//
//  TryAngleOnDevice.swift
//  ì™„ì „í•œ ì˜¨ë””ë°”ì´ìŠ¤ ì‹¤í–‰ ì‹œìŠ¤í…œ
//  ì‘ì„±ì¼: 2025-12-05
//  API ì„œë²„ ì—†ì´ iOSì—ì„œ ì§ì ‘ ì‹¤í–‰
//

import Foundation
import UIKit
import AVFoundation
import Vision
import CoreML

// MARK: - ì˜¨ë””ë°”ì´ìŠ¤ í†µí•© ë¶„ì„ê¸°
class TryAngleOnDeviceAnalyzer {

    // ëª¨ë¸ë“¤
    private let rtmposeRunner: RTMPoseRunner
    private let depthEstimator: DepthAnythingCoreML
    private let personDetector: PersonDetector?  // ì‚¬ëŒ ê²€ì¶œê¸° (ì„ íƒì )

    // í”¼ë“œë°± ìƒì„±ê¸°
    private let feedbackGenerator: OnDeviceFeedbackGenerator

    // ì„±ëŠ¥ ì¶”ì 
    private var performanceStats = PerformanceStats()

    // ì„¤ì •
    private let useLegacySystem: Bool

    init(enableLegacySystem: Bool = false) {
        print("ğŸš€ TryAngle ì˜¨ë””ë°”ì´ìŠ¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")

        self.useLegacySystem = enableLegacySystem

        // RTMPose (ONNX)
        if let rtmpose = RTMPoseRunner() {
            self.rtmposeRunner = rtmpose
            print("âœ… RTMPose ONNX ë¡œë“œ ì™„ë£Œ")
        } else {
            fatalError("âŒ RTMPose ì´ˆê¸°í™” ì‹¤íŒ¨")
        }

        // Depth Anything (CoreML)
        self.depthEstimator = DepthAnythingCoreML(modelType: .small)
        print("âœ… Depth Anything CoreML ë¡œë“œ ì™„ë£Œ")

        // Person Detector (ì„ íƒì  - ë ˆê±°ì‹œ ì‹œìŠ¤í…œ)
        if enableLegacySystem {
            self.personDetector = PersonDetector()
            print("âœ… Person Detector ë¡œë“œ ì™„ë£Œ (ë ˆê±°ì‹œ ëª¨ë“œ)")
        } else {
            self.personDetector = nil
            print("â„¹ï¸ ë ˆê±°ì‹œ ì‹œìŠ¤í…œ ë¹„í™œì„±í™” (RTMPoseë§Œ ì‚¬ìš©)")
        }

        // í”¼ë“œë°± ìƒì„±ê¸°
        self.feedbackGenerator = OnDeviceFeedbackGenerator(useLegacySystem: enableLegacySystem)
        print("âœ… í”¼ë“œë°± ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

        print("ğŸ¯ ì˜¨ë””ë°”ì´ìŠ¤ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    }

    // MARK: - ë©”ì¸ ë¶„ì„ í•¨ìˆ˜
    func analyzeFrame(_ image: UIImage, completion: @escaping (TryAngleFeedback) -> Void) {
        let startTime = CFAbsoluteTimeGetCurrent()

        // ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ DispatchGroup
        let group = DispatchGroup()

        var poseResult: RTMPoseResult?
        var depthResult: V15DepthResult?
        var legacyBBox: CGRect?

        // 1. RTMPose ì²˜ë¦¬ (ë¹„ë™ê¸°)
        group.enter()
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            poseResult = self?.rtmposeRunner.detectPose(from: image)
            group.leave()
        }

        // 2. Depth Anything ì²˜ë¦¬ (ë¹„ë™ê¸°)
        group.enter()
        depthEstimator.estimateDepth(from: image) { result in
            if case .success(let depth) = result {
                depthResult = depth
            }
            group.leave()
        }

        // 3. Person Detector ì²˜ë¦¬ (ë ˆê±°ì‹œ ëª¨ë“œì¼ ë•Œë§Œ)
        if useLegacySystem, let personDetector = personDetector {
            group.enter()
            let ciImage = CIImage(image: image)!
            personDetector.detectPerson(in: ciImage) { bbox in
                legacyBBox = bbox
                group.leave()
            }
        }

        // 4. ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ í›„ í”¼ë“œë°± ìƒì„±
        group.notify(queue: .main) { [weak self] in
            guard let self = self else { return }

            // ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            let processingTime = CFAbsoluteTimeGetCurrent() - startTime
            self.performanceStats.update(processingTime: processingTime)

            // í”¼ë“œë°± ìƒì„± (ë ˆê±°ì‹œ bbox í¬í•¨)
            let feedback = self.feedbackGenerator.generateFeedback(
                pose: poseResult,
                depth: depthResult,
                legacyBBox: legacyBBox,
                imageSize: image.size,
                processingTime: processingTime
            )

            completion(feedback)
        }
    }

    // MARK: - ë ˆí¼ëŸ°ìŠ¤ ë¶„ì„
    func analyzeReference(_ image: UIImage) -> ReferenceAnalysis {
        // ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ì •ë°€ ë¶„ì„
        let pose = rtmposeRunner.detectPose(from: image)

        var depth: V15DepthResult?
        let semaphore = DispatchSemaphore(value: 0)

        depthEstimator.estimateDepth(from: image) { result in
            if case .success(let d) = result {
                depth = d
            }
            semaphore.signal()
        }
        semaphore.wait()

        return ReferenceAnalysis(
            pose: pose,
            depth: depth,
            timestamp: Date()
        )
    }

    // MARK: - ì„±ëŠ¥ í†µê³„
    func getPerformanceStats() -> PerformanceStats {
        return performanceStats
    }
}

// MARK: - ì˜¨ë””ë°”ì´ìŠ¤ í”¼ë“œë°± ìƒì„±ê¸°
class OnDeviceFeedbackGenerator {

    // í•œêµ­ì–´ ë©”ì‹œì§€
    private let messages = FeedbackMessages()
    private let useLegacySystem: Bool

    init(useLegacySystem: Bool = false) {
        self.useLegacySystem = useLegacySystem
    }

    func generateFeedback(pose: RTMPoseResult?,
                         depth: V15DepthResult?,
                         legacyBBox: CGRect? = nil,
                         imageSize: CGSize? = nil,
                         processingTime: TimeInterval) -> TryAngleFeedback {

        var primary = ""
        var suggestions = [String]()
        var movement: MovementGuide?
        var marginInfo: MarginInfo?

        // 1. BBox ì„ íƒ (ë ˆê±°ì‹œ ìš°ì„ , ì—†ìœ¼ë©´ RTMPose)
        let effectiveBBox: CGRect?
        if let legacyBBox = legacyBBox {
            effectiveBBox = legacyBBox
            print("ğŸ“ ë ˆê±°ì‹œ BBox ì‚¬ìš©")
        } else if let pose = pose, let poseBBox = pose.boundingBox {
            // RTMPose bboxë¥¼ normalized coordinatesë¡œ ë³€í™˜
            effectiveBBox = CGRect(
                x: poseBBox.origin.x / UIScreen.main.bounds.width,
                y: poseBBox.origin.y / UIScreen.main.bounds.height,
                width: poseBBox.width / UIScreen.main.bounds.width,
                height: poseBBox.height / UIScreen.main.bounds.height
            )
            print("ğŸ¤– RTMPose BBox ì‚¬ìš©")
        } else {
            effectiveBBox = nil
            primary = "ì¸ë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        }

        // 2. ì—¬ë°± ë¶„ì„ (ë ˆê±°ì‹œ ìŠ¤íƒ€ì¼)
        if let bbox = effectiveBBox, let size = imageSize {
            let margins = calculateLegacyMargins(bbox: bbox, imageSize: size)
            marginInfo = margins

            // ì—¬ë°± í”¼ë“œë°± ìƒì„±
            if margins.leftRatio < 0.05 {
                suggestions.append("ì™¼ìª½ ì—¬ë°±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤")
            } else if margins.leftRatio > 0.4 {
                suggestions.append("ì™¼ìª½ ì—¬ë°±ì´ ë„ˆë¬´ í½ë‹ˆë‹¤")
            }

            if margins.rightRatio < 0.05 {
                suggestions.append("ì˜¤ë¥¸ìª½ ì—¬ë°±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤")
            } else if margins.rightRatio > 0.4 {
                suggestions.append("ì˜¤ë¥¸ìª½ ì—¬ë°±ì´ ë„ˆë¬´ í½ë‹ˆë‹¤")
            }

            if margins.topRatio < 0.05 {
                suggestions.append("ìƒë‹¨ ì—¬ë°±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤")
            } else if margins.topRatio > 0.3 {
                suggestions.append("ìƒë‹¨ ì—¬ë°±ì´ ë„ˆë¬´ í½ë‹ˆë‹¤")
            }

            // ê· í˜• ì²´í¬
            if margins.balanceScore < 0.6 {
                primary = "êµ¬ë„ ê· í˜•ì„ ë§ì¶°ì£¼ì„¸ìš”"
            } else if margins.balanceScore > 0.85 {
                primary = "ì¢‹ì€ êµ¬ë„ì…ë‹ˆë‹¤!"
            }
        }

        // 3. í¬ì¦ˆ ê¸°ë°˜ í”¼ë“œë°± (RTMPose í‚¤í¬ì¸íŠ¸)
        if let pose = pose {
            let poseFeedback = analyzePose(pose)
            if primary.isEmpty, let primaryPose = poseFeedback.primary {
                primary = primaryPose
            }
            suggestions.append(contentsOf: poseFeedback.suggestions)
            movement = poseFeedback.movement
        }

        // 4. ê¹Šì´ ê¸°ë°˜ í”¼ë“œë°±
        if let depth = depth {
            let depthFeedback = analyzeDepth(depth)
            suggestions.append(contentsOf: depthFeedback)
        }

        // 5. ìš°ì„ ìˆœìœ„ ì •ë ¬
        if suggestions.count > 3 {
            suggestions = Array(suggestions.prefix(3))
        }

        return TryAngleFeedback(
            primary: primary.isEmpty ? "ì¹´ë©”ë¼ ìœ„ì¹˜ ì¡°ì • ì¤‘..." : primary,
            suggestions: suggestions,
            movement: movement,
            compressionInfo: depth.map { CompressionInfo(
                index: $0.compressionIndex,
                cameraType: $0.cameraType.description,
                suggestion: $0.cameraType.recommendation
            )},
            marginInfo: marginInfo,
            processingTime: processingTime,
            isOnDevice: true,
            usedLegacySystem: legacyBBox != nil
        )
    }

    // MARK: - ë ˆê±°ì‹œ ì—¬ë°± ê³„ì‚°
    private func calculateLegacyMargins(bbox: CGRect, imageSize: CGSize) -> MarginInfo {
        // legacy_analyzer.pyì˜ ë¡œì§ì„ Swiftë¡œ í¬íŒ…

        let x = bbox.origin.x * imageSize.width
        let y = bbox.origin.y * imageSize.height
        let w = bbox.width * imageSize.width
        let h = bbox.height * imageSize.height

        let leftMargin = x
        let rightMargin = imageSize.width - (x + w)
        let topMargin = y
        let bottomMargin = imageSize.height - (y + h)

        let leftRatio = leftMargin / imageSize.width
        let rightRatio = rightMargin / imageSize.width
        let topRatio = topMargin / imageSize.height
        let bottomRatio = bottomMargin / imageSize.height

        // ê· í˜• ì ìˆ˜ ê³„ì‚° (ë ˆê±°ì‹œ ìŠ¤íƒ€ì¼)
        let horizontalBalance = 1.0 - abs(leftRatio - rightRatio)
        let verticalBalance = 1.0 - abs(topRatio - bottomRatio * 0.5)  // í•˜ë‹¨ 2:1 ë¹„ìœ¨
        let balanceScore = (horizontalBalance + verticalBalance) / 2.0

        return MarginInfo(
            left: leftMargin,
            right: rightMargin,
            top: topMargin,
            bottom: bottomMargin,
            leftRatio: leftRatio,
            rightRatio: rightRatio,
            topRatio: topRatio,
            bottomRatio: bottomRatio,
            balanceScore: balanceScore
        )
    }

    // MARK: - í¬ì¦ˆ ë¶„ì„
    private func analyzePose(_ pose: RTMPoseResult) -> (primary: String?, suggestions: [String], movement: MovementGuide?) {
        guard let bbox = pose.boundingBox else {
            return (nil, [], nil)
        }

        // í™”ë©´ ì¤‘ì•™ê³¼ì˜ ê±°ë¦¬ ê³„ì‚°
        let screenCenter = CGPoint(x: 0.5, y: 0.5)
        let personCenter = CGPoint(
            x: bbox.midX / UIScreen.main.bounds.width,
            y: bbox.midY / UIScreen.main.bounds.height
        )

        let dx = personCenter.x - screenCenter.x
        let dy = personCenter.y - screenCenter.y

        var primary: String?
        var suggestions = [String]()
        var movement: MovementGuide?

        // ìœ„ì¹˜ í”¼ë“œë°±
        if abs(dx) > 0.1 || abs(dy) > 0.1 {
            let direction: String
            let arrow: String

            if abs(dx) > abs(dy) {
                // ìˆ˜í‰ ì´ë™
                if dx > 0 {
                    direction = "ì™¼ìª½ìœ¼ë¡œ"
                    arrow = "â†"
                } else {
                    direction = "ì˜¤ë¥¸ìª½ìœ¼ë¡œ"
                    arrow = "â†’"
                }
            } else {
                // ìˆ˜ì§ ì´ë™
                if dy > 0 {
                    direction = "ìœ„ë¡œ"
                    arrow = "â†‘"
                } else {
                    direction = "ì•„ë˜ë¡œ"
                    arrow = "â†“"
                }
            }

            let amount = String(format: "%.0f%%", max(abs(dx), abs(dy)) * 100)
            primary = "ì¹´ë©”ë¼ë¥¼ \(direction) ì´ë™í•˜ì„¸ìš”"

            movement = MovementGuide(
                direction: direction,
                arrow: arrow,
                amount: amount
            )
        }

        // í¬ê¸° í”¼ë“œë°±
        let bboxArea = bbox.width * bbox.height
        let screenArea = UIScreen.main.bounds.width * UIScreen.main.bounds.height
        let sizeRatio = bboxArea / screenArea

        if sizeRatio < 0.15 {
            suggestions.append("ë” ê°€ê¹Œì´ ì ‘ê·¼í•˜ì„¸ìš”")
        } else if sizeRatio > 0.5 {
            suggestions.append("ì¡°ê¸ˆ ë’¤ë¡œ ë¬¼ëŸ¬ë‚˜ì„¸ìš”")
        }

        // 133 í‚¤í¬ì¸íŠ¸ ë¶„ì„
        let visibleKeypoints = pose.keypoints.filter { $0.confidence > 0.5 }
        if visibleKeypoints.count < 50 {
            suggestions.append("ì „ì‹ ì´ ë³´ì´ë„ë¡ ì¡°ì •í•˜ì„¸ìš”")
        }

        return (primary, suggestions, movement)
    }

    // MARK: - ê¹Šì´ ë¶„ì„
    private func analyzeDepth(_ depth: V15DepthResult) -> [String] {
        var suggestions = [String]()

        switch depth.cameraType {
        case .wide:
            suggestions.append("ê´‘ê° ë Œì¦ˆ - ë” ê°€ê¹Œì´ ì ‘ê·¼í•˜ê±°ë‚˜ ë§ì› ë Œì¦ˆ ì‚¬ìš©")
        case .telephoto:
            suggestions.append("ë§ì› ë Œì¦ˆ - ë°°ê²½ê³¼ì˜ ë¶„ë¦¬ê°ì´ ì¢‹ìŠµë‹ˆë‹¤")
        case .normal, .semiTele:
            // ì ì ˆí•¨
            break
        }

        if depth.compressionIndex < 0.3 {
            suggestions.append("ë°°ê²½ì´ ë„ˆë¬´ ë„“ê²Œ ë³´ì…ë‹ˆë‹¤")
        } else if depth.compressionIndex > 0.8 {
            suggestions.append("ë°°ê²½ì´ ë„ˆë¬´ ì••ì¶•ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        }

        return suggestions
    }
}

// MARK: - ë°ì´í„° êµ¬ì¡°ì²´
struct TryAngleFeedback {
    let primary: String
    let suggestions: [String]
    let movement: MovementGuide?
    let compressionInfo: CompressionInfo?
    let marginInfo: MarginInfo?  // ë ˆê±°ì‹œ ì—¬ë°± ì •ë³´
    let processingTime: TimeInterval
    let isOnDevice: Bool
    let usedLegacySystem: Bool  // ë ˆê±°ì‹œ ì‹œìŠ¤í…œ ì‚¬ìš© ì—¬ë¶€
}

struct MovementGuide {
    let direction: String
    let arrow: String
    let amount: String
}

struct CompressionInfo {
    let index: Float
    let cameraType: String
    let suggestion: String?
}

struct MarginInfo {
    let left: CGFloat
    let right: CGFloat
    let top: CGFloat
    let bottom: CGFloat
    let leftRatio: CGFloat
    let rightRatio: CGFloat
    let topRatio: CGFloat
    let bottomRatio: CGFloat
    let balanceScore: CGFloat
}

struct ReferenceAnalysis {
    let pose: RTMPoseResult?
    let depth: V15DepthResult?
    let timestamp: Date
}

struct PerformanceStats {
    var totalFrames: Int = 0
    var averageTime: TimeInterval = 0
    var minTime: TimeInterval = Double.greatestFiniteMagnitude
    var maxTime: TimeInterval = 0

    mutating func update(processingTime: TimeInterval) {
        totalFrames += 1
        averageTime = ((averageTime * Double(totalFrames - 1)) + processingTime) / Double(totalFrames)
        minTime = min(minTime, processingTime)
        maxTime = max(maxTime, processingTime)
    }

    var fps: Double {
        return averageTime > 0 ? 1.0 / averageTime : 0
    }
}

// MARK: - í”¼ë“œë°± ë©”ì‹œì§€
struct FeedbackMessages {
    let movement = [
        "up": "ìœ„ë¡œ ì´ë™í•˜ì„¸ìš”",
        "down": "ì•„ë˜ë¡œ ì´ë™í•˜ì„¸ìš”",
        "left": "ì™¼ìª½ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”",
        "right": "ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”"
    ]

    let composition = [
        "center": "ì¤‘ì•™ì— ìœ„ì¹˜ì‹œí‚¤ì„¸ìš”",
        "rule_of_thirds": "3ë¶„í•  ì„ ì— ë§ì¶”ì„¸ìš”",
        "leading_space": "ì‹œì„  ë°©í–¥ì— ê³µê°„ì„ ë‘ì„¸ìš”"
    ]

    let framing = [
        "too_tight": "í”„ë ˆì„ì´ ë„ˆë¬´ íƒ€ì´íŠ¸í•©ë‹ˆë‹¤",
        "too_loose": "í”„ë ˆì„ì´ ë„ˆë¬´ ëŠìŠ¨í•©ë‹ˆë‹¤",
        "good": "ì¢‹ì€ í”„ë ˆì´ë°ì…ë‹ˆë‹¤"
    ]
}

// MARK: - ì¹´ë©”ë¼ ë·°ì»¨íŠ¸ë¡¤ëŸ¬ (ì˜¨ë””ë°”ì´ìŠ¤)
class TryAngleOnDeviceCameraViewController: UIViewController {

    // ë¶„ì„ê¸°
    private let analyzer = TryAngleOnDeviceAnalyzer()

    // ì¹´ë©”ë¼
    private var captureSession: AVCaptureSession!
    private var videoOutput: AVCaptureVideoDataOutput!
    private var previewLayer: AVCaptureVideoPreviewLayer!

    // UI
    private let feedbackLabel = UILabel()
    private let movementArrow = UILabel()
    private let performanceLabel = UILabel()
    private let compressionLabel = UILabel()

    // í”„ë ˆì„ ìŠ¤í‚µ
    private var frameCounter = 0
    private let processEveryNFrames = 3  // 3í”„ë ˆì„ë§ˆë‹¤ ì²˜ë¦¬

    override func viewDidLoad() {
        super.viewDidLoad()
        setupCamera()
        setupUI()
    }

    private func setupCamera() {
        captureSession = AVCaptureSession()
        captureSession.sessionPreset = .hd1280x720

        guard let camera = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back),
              let input = try? AVCaptureDeviceInput(device: camera) else { return }

        captureSession.addInput(input)

        videoOutput = AVCaptureVideoDataOutput()
        videoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "video.queue"))
        videoOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA]

        captureSession.addOutput(videoOutput)

        previewLayer = AVCaptureVideoPreviewLayer(session: captureSession)
        previewLayer.videoGravity = .resizeAspectFill
        previewLayer.frame = view.bounds
        view.layer.insertSublayer(previewLayer, at: 0)

        DispatchQueue.global(qos: .background).async { [weak self] in
            self?.captureSession.startRunning()
        }
    }

    private func setupUI() {
        // í”¼ë“œë°± ë ˆì´ë¸”
        feedbackLabel.translatesAutoresizingMaskIntoConstraints = false
        feedbackLabel.textColor = .white
        feedbackLabel.backgroundColor = UIColor.black.withAlphaComponent(0.7)
        feedbackLabel.textAlignment = .center
        feedbackLabel.numberOfLines = 0
        feedbackLabel.layer.cornerRadius = 10
        feedbackLabel.clipsToBounds = true
        feedbackLabel.font = .systemFont(ofSize: 16, weight: .medium)
        view.addSubview(feedbackLabel)

        // ì›€ì§ì„ í™”ì‚´í‘œ
        movementArrow.translatesAutoresizingMaskIntoConstraints = false
        movementArrow.textColor = .systemYellow
        movementArrow.font = .systemFont(ofSize: 48)
        movementArrow.textAlignment = .center
        view.addSubview(movementArrow)

        // ì„±ëŠ¥ ë ˆì´ë¸”
        performanceLabel.translatesAutoresizingMaskIntoConstraints = false
        performanceLabel.textColor = .systemGreen
        performanceLabel.font = .monospacedSystemFont(ofSize: 12, weight: .regular)
        performanceLabel.backgroundColor = UIColor.black.withAlphaComponent(0.5)
        view.addSubview(performanceLabel)

        // ì••ì¶•ê° ë ˆì´ë¸”
        compressionLabel.translatesAutoresizingMaskIntoConstraints = false
        compressionLabel.textColor = .white
        compressionLabel.font = .systemFont(ofSize: 14)
        compressionLabel.backgroundColor = UIColor.black.withAlphaComponent(0.5)
        compressionLabel.layer.cornerRadius = 5
        compressionLabel.clipsToBounds = true
        view.addSubview(compressionLabel)

        NSLayoutConstraint.activate([
            // í”¼ë“œë°±
            feedbackLabel.bottomAnchor.constraint(equalTo: view.safeAreaLayoutGuide.bottomAnchor, constant: -20),
            feedbackLabel.leadingAnchor.constraint(equalTo: view.leadingAnchor, constant: 20),
            feedbackLabel.trailingAnchor.constraint(equalTo: view.trailingAnchor, constant: -20),
            feedbackLabel.heightAnchor.constraint(greaterThanOrEqualToConstant: 60),

            // í™”ì‚´í‘œ
            movementArrow.centerXAnchor.constraint(equalTo: view.centerXAnchor),
            movementArrow.centerYAnchor.constraint(equalTo: view.centerYAnchor),

            // ì„±ëŠ¥
            performanceLabel.topAnchor.constraint(equalTo: view.safeAreaLayoutGuide.topAnchor, constant: 10),
            performanceLabel.trailingAnchor.constraint(equalTo: view.trailingAnchor, constant: -10),

            // ì••ì¶•ê°
            compressionLabel.topAnchor.constraint(equalTo: performanceLabel.bottomAnchor, constant: 5),
            compressionLabel.trailingAnchor.constraint(equalTo: view.trailingAnchor, constant: -10)
        ])

        // ì˜¨ë””ë°”ì´ìŠ¤ í‘œì‹œ
        let onDeviceLabel = UILabel()
        onDeviceLabel.text = "ğŸ“± On-Device"
        onDeviceLabel.textColor = .systemBlue
        onDeviceLabel.font = .systemFont(ofSize: 12, weight: .bold)
        onDeviceLabel.translatesAutoresizingMaskIntoConstraints = false
        view.addSubview(onDeviceLabel)

        NSLayoutConstraint.activate([
            onDeviceLabel.topAnchor.constraint(equalTo: view.safeAreaLayoutGuide.topAnchor, constant: 10),
            onDeviceLabel.leadingAnchor.constraint(equalTo: view.leadingAnchor, constant: 10)
        ])
    }
}

// MARK: - AVCaptureVideoDataOutputSampleBufferDelegate
extension TryAngleOnDeviceCameraViewController: AVCaptureVideoDataOutputSampleBufferDelegate {

    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        // í”„ë ˆì„ ìŠ¤í‚µ
        frameCounter += 1
        if frameCounter % processEveryNFrames != 0 {
            return
        }

        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }

        // UIImage ë³€í™˜
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        let context = CIContext()
        guard let cgImage = context.createCGImage(ciImage, from: ciImage.extent) else { return }
        let uiImage = UIImage(cgImage: cgImage)

        // ë¶„ì„
        analyzer.analyzeFrame(uiImage) { [weak self] feedback in
            DispatchQueue.main.async {
                self?.updateUI(with: feedback)
            }
        }
    }

    private func updateUI(with feedback: TryAngleFeedback) {
        // í”¼ë“œë°± í…ìŠ¤íŠ¸
        var feedbackText = feedback.primary
        if !feedback.suggestions.isEmpty {
            feedbackText += "\n" + feedback.suggestions.joined(separator: " â€¢ ")
        }
        feedbackLabel.text = feedbackText

        // ì›€ì§ì„ í™”ì‚´í‘œ
        movementArrow.text = feedback.movement?.arrow ?? ""

        // ì„±ëŠ¥
        let fps = 1.0 / feedback.processingTime
        performanceLabel.text = String(format: "%.0f FPS | %.0fms", fps, feedback.processingTime * 1000)

        // ì••ì¶•ê°
        if let compression = feedback.compressionInfo {
            compressionLabel.text = "\(compression.cameraType) (\(String(format: "%.2f", compression.index)))"
        }
    }
}