import Foundation
import Vision
import UIKit
import CoreML
// TensorFlowLite wrapper is in TensorFlowLiteWrapper.swift

// MARK: - PoseML ë¶„ì„ê¸° (YOLO11s-pose + MoveNet Lightning)
class PoseMLAnalyzer {

    // YOLO11s-pose CoreML ëª¨ë¸
    private var yoloModel: MLModel?

    // MoveNet Lightning TFLite
    private var moveNetInterpreter: Interpreter?

    // Visionì€ ì–¼êµ´ ê°ì§€ìš©ìœ¼ë¡œë§Œ ê³„ì† ì‚¬ìš©
    private lazy var faceDetectionRequest: VNDetectFaceLandmarksRequest = {
        let request = VNDetectFaceLandmarksRequest()
        request.revision = VNDetectFaceLandmarksRequestRevision3
        return request
    }()

    // ğŸ› ë””ë²„ê·¸ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    private lazy var logFileURL: URL? = {
        if let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first {
            return documentsPath.appendingPathComponent("pose_debug.txt")
        }
        return nil
    }()

    init() {
        print("ğŸš€ PoseMLAnalyzer init() ì‹œì‘")
        logToFile("ğŸš€ PoseMLAnalyzer init() ì‹œì‘ - \(Date())")
        loadModels()
        print("ğŸš€ PoseMLAnalyzer init() ì™„ë£Œ")
        logToFile("ğŸš€ PoseMLAnalyzer init() ì™„ë£Œ")
    }

    // ğŸ› íŒŒì¼ì— ë¡œê·¸ ê¸°ë¡
    private func logToFile(_ message: String) {
        guard let logFileURL = logFileURL else { return }

        let timestamp = DateFormatter.localizedString(from: Date(), dateStyle: .none, timeStyle: .medium)
        let logMessage = "[\(timestamp)] \(message)\n"

        if let data = logMessage.data(using: .utf8) {
            if FileManager.default.fileExists(atPath: logFileURL.path) {
                if let fileHandle = try? FileHandle(forWritingTo: logFileURL) {
                    fileHandle.seekToEndOfFile()
                    fileHandle.write(data)
                    fileHandle.closeFile()
                }
            } else {
                try? data.write(to: logFileURL)
            }
        }

        // ì½˜ì†”ì—ë„ ì¶œë ¥
        print(message)
    }

    private func loadModels() {
        logToFile("ğŸ“¦ ëª¨ë¸ ë¡œë”© ì‹œì‘")

        // YOLO11s-pose ëª¨ë¸ ë¡œë“œ
        do {
            guard let modelURL = Bundle.main.url(forResource: "yolo11s-pose", withExtension: "mlpackage") else {
                logToFile("âŒ YOLO ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (yolo11s-pose.mlpackage)")
                return
            }
            logToFile("ğŸ“‚ YOLO ëª¨ë¸ íŒŒì¼ ì°¾ìŒ: \(modelURL.path)")
            let config = MLModelConfiguration()
            config.computeUnits = .all  // CPU + GPU + Neural Engine
            yoloModel = try MLModel(contentsOf: modelURL, configuration: config)
            logToFile("âœ… YOLO11s-pose ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        } catch {
            logToFile("âŒ YOLO ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: \(error)")
        }

        // MoveNet TFLite ëª¨ë¸ ë¡œë“œ
        do {
            guard let modelPath = Bundle.main.path(forResource: "movenet_lightning", ofType: "tflite") else {
                logToFile("âŒ MoveNet ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (movenet_lightning.tflite)")
                logToFile("âš ï¸ YOLOë§Œ ì‚¬ìš©í•˜ì—¬ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤")
                return
            }
            logToFile("ğŸ“‚ MoveNet ëª¨ë¸ íŒŒì¼ ì°¾ìŒ: \(modelPath)")
            var options = Interpreter.Options()
            options.threadCount = 4
            moveNetInterpreter = try Interpreter(modelPath: modelPath, options: options)
            try moveNetInterpreter?.allocateTensors()
            logToFile("âœ… MoveNet Lightning ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        } catch {
            logToFile("âŒ MoveNet ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: \(error)")
            logToFile("âš ï¸ YOLOë§Œ ì‚¬ìš©í•˜ì—¬ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤")
        }

        logToFile("ğŸ“¦ ëª¨ë¸ ë¡œë”© ì™„ë£Œ - YOLO: \(yoloModel != nil), MoveNet: \(moveNetInterpreter != nil)")
    }

    // MARK: - ì–¼êµ´ + í¬ì¦ˆ ë™ì‹œ ë¶„ì„ (VisionAnalyzerì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤)
    private var analysisCallCount = 0
    func analyzeFaceAndPose(from image: UIImage) -> (face: FaceAnalysisResult?, pose: PoseAnalysisResult?) {
        analysisCallCount += 1

        // 10ë²ˆë§ˆë‹¤ ë¡œê·¸ (ë„ˆë¬´ ë§ì€ ë¡œê·¸ ë°©ì§€)
        if analysisCallCount % 10 == 1 {
            logToFile("ğŸ” analyzeFaceAndPose() í˜¸ì¶œë¨ (í˜¸ì¶œ íšŸìˆ˜: \(analysisCallCount))")
        }

        guard let cgImage = image.cgImage else {
            logToFile("âŒ cgImage ì—†ìŒ")
            return (nil, nil)
        }

        // ì–¼êµ´ ê°ì§€ (Vision ê³„ì† ì‚¬ìš© - ê°€ì¥ ì •í™•í•¨)
        let faceResult = detectFace(from: image)

        // í¬ì¦ˆ ê°ì§€ (YOLO + MoveNet ìœµí•©)
        let yoloPose = detectPoseWithYOLO(from: cgImage)
        let moveNetPose = detectPoseWithMoveNet(from: cgImage)

        // YOLOì™€ MoveNet ê²°ê³¼ ìœµí•© (ë‘˜ ë‹¤ ìˆìœ¼ë©´ ìœµí•©, í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ê·¸ê²ƒ ì‚¬ìš©)
        let fusedPose = fusePoseResults(yolo: yoloPose, moveNet: moveNetPose)

        // 10ë²ˆë§ˆë‹¤ ìƒì„¸ ë¡œê·¸
        if analysisCallCount % 10 == 1 {
            logToFile("   ì–¼êµ´: \(faceResult != nil ? "âœ“" : "âœ—") | YOLO: \(yoloPose != nil ? "\(yoloPose!.keypoints.count)ê°œ" : "âœ—") | MoveNet: \(moveNetPose != nil ? "\(moveNetPose!.keypoints.count)ê°œ" : "âœ—") | ìœµí•©: \(fusedPose != nil ? "\(fusedPose!.keypoints.count)ê°œ" : "âœ—")")
        }

        return (faceResult, fusedPose)
    }

    // MARK: - ì–¼êµ´ ê°ì§€ (Vision)
    private func detectFace(from image: UIImage) -> FaceAnalysisResult? {
        guard let cgImage = image.cgImage else { return nil }

        let handler = VNImageRequestHandler(
            cgImage: cgImage,
            orientation: image.cgImageOrientation,
            options: [:]
        )
        try? handler.perform([faceDetectionRequest])

        guard let observation = faceDetectionRequest.results?.first else {
            return nil
        }

        return FaceAnalysisResult(
            faceRect: observation.boundingBox,
            landmarks: observation.landmarks,
            yaw: observation.yaw?.floatValue,
            pitch: observation.pitch?.floatValue,
            roll: observation.roll?.floatValue,
            observation: observation
        )
    }

    // MARK: - í¬ì¦ˆ ê°ì§€ (YOLO11s-pose)
    private func detectPoseWithYOLO(from cgImage: CGImage) -> PoseAnalysisResult? {
        guard let model = yoloModel else {
            print("âš ï¸ YOLO ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            return nil
        }

        // YOLO ì…ë ¥: 1280x1280 ì´ë¯¸ì§€ (ê³ í•´ìƒë„ë¡œ ì‘ì€ ê´€ì ˆë„ ê²€ì¶œ)
        let inputSize = CGSize(width: 1280, height: 1280)
        guard let resizedImage = resize(cgImage: cgImage, to: inputSize) else {
            return nil
        }

        guard let pixelBuffer = cgImageToPixelBuffer(resizedImage, size: inputSize) else {
            return nil
        }

        do {
            // YOLO ì¶”ë¡ 
            let input = try MLDictionaryFeatureProvider(dictionary: ["image": pixelBuffer])
            let output = try model.prediction(from: input)

            // YOLO ì¶œë ¥: (1, 56, 8400) í˜•íƒœ
            // 56 = 17 keypoints * 3 (x, y, confidence) + 5 (bbox + objectness)
            guard let outputArray = output.featureValue(for: "output")?.multiArrayValue else {
                return nil
            }

            // í‚¤í¬ì¸íŠ¸ íŒŒì‹±
            let keypoints = parseYOLOKeypoints(from: outputArray, originalSize: CGSize(width: cgImage.width, height: cgImage.height))

            return PoseAnalysisResult(
                keypoints: keypoints
            )
        } catch {
            print("âŒ YOLO ì¶”ë¡  ì‹¤íŒ¨: \(error)")
            return nil
        }
    }

    // MARK: - MoveNet í¬ì¦ˆ ê°ì§€
    private func detectPoseWithMoveNet(from cgImage: CGImage) -> PoseAnalysisResult? {
        guard let interpreter = moveNetInterpreter else {
            logToFile("âš ï¸ MoveNet interpreter ì—†ìŒ - ì´ˆê¸°í™” ì‹¤íŒ¨í–ˆì„ ê°€ëŠ¥ì„±")
            return nil
        }

        // MoveNet ì…ë ¥: 192x192 ì´ë¯¸ì§€
        let inputSize = CGSize(width: 192, height: 192)
        guard let resizedImage = resize(cgImage: cgImage, to: inputSize) else {
            logToFile("âŒ MoveNet: ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨")
            return nil
        }

        guard let inputData = preprocessForMoveNet(resizedImage) else {
            logToFile("âŒ MoveNet: ì „ì²˜ë¦¬ ì‹¤íŒ¨")
            return nil
        }

        do {
            try interpreter.copy(inputData, toInputAt: 0)
            try interpreter.invoke()

            // MoveNet ì¶œë ¥: (1, 1, 17, 3) - [y, x, confidence]
            let outputTensor = try interpreter.output(at: 0)
            let keypoints = parseMoveNetKeypoints(from: outputTensor.data,
                                                  originalSize: CGSize(width: cgImage.width, height: cgImage.height))

            return PoseAnalysisResult(keypoints: keypoints)
        } catch let error as InterpreterError {
            logToFile("âŒ MoveNet ì¶”ë¡  ì‹¤íŒ¨ (Interpreter): \(error.localizedDescription)")
            return nil
        } catch {
            logToFile("âŒ MoveNet ì¶”ë¡  ì‹¤íŒ¨ (Unknown): \(error)")
            return nil
        }
    }

    // MARK: - YOLO í‚¤í¬ì¸íŠ¸ íŒŒì‹±
    private func parseYOLOKeypoints(from output: MLMultiArray, originalSize: CGSize) -> [(point: CGPoint, confidence: Float)] {
        // YOLO pose ì¶œë ¥ í˜•ì‹: (1, 56, 8400)
        // 56 = bbox(4) + objectness(1) + 17 keypoints * 3 (x, y, conf)

        var keypoints: [(point: CGPoint, confidence: Float)] = []

        // ê°€ì¥ ë†’ì€ objectnessë¥¼ ê°€ì§„ detection ì°¾ê¸°
        var maxObjectness: Float = 0
        var maxIndex = 0

        let detectionCount = output.shape[2].intValue
        for i in 0..<detectionCount {
            let objectness = output[[0, 4, i] as [NSNumber]].floatValue
            if objectness > maxObjectness {
                maxObjectness = objectness
                maxIndex = i
            }
        }

        // ğŸ”¥ Objectness threshold: ë„ˆë¬´ ë‚®ìœ¼ë©´ ë¬´ì‹œ (ì™„í™”: 0.2)
        if maxObjectness < 0.2 {
            logToFile("âš ï¸ YOLO objectness ë„ˆë¬´ ë‚®ìŒ: \(maxObjectness) < 0.2 - í¬ì¦ˆ ë¬´ì‹œ")
            return []
        }

        logToFile("âœ… YOLO detection - objectness: \(maxObjectness)")

        // í•´ë‹¹ detectionì˜ 17ê°œ keypoints ì¶”ì¶œ
        for kpIdx in 0..<17 {
            let baseIdx = 5 + kpIdx * 3
            let x = output[[0, baseIdx, maxIndex] as [NSNumber]].floatValue / 1280.0
            let y = output[[0, baseIdx + 1, maxIndex] as [NSNumber]].floatValue / 1280.0
            let conf = output[[0, baseIdx + 2, maxIndex] as [NSNumber]].floatValue

            keypoints.append((
                point: CGPoint(x: CGFloat(x), y: CGFloat(y)),
                confidence: conf
            ))
        }

        // ğŸ”¥ ì‹ ë¢°ë„ ë†’ì€ í‚¤í¬ì¸íŠ¸ ê°œìˆ˜ ì„¸ê¸°
        let visibleCount = keypoints.filter { $0.confidence >= 0.5 }.count
        logToFile("   YOLO keypoints: ì „ì²´ \(keypoints.count)ê°œ, ì‹ ë¢°ë„ â‰¥ 0.5: \(visibleCount)ê°œ")

        return keypoints
    }

    // MARK: - MoveNet í‚¤í¬ì¸íŠ¸ íŒŒì‹±
    private func parseMoveNetKeypoints(from data: Data, originalSize: CGSize) -> [(point: CGPoint, confidence: Float)] {
        var keypoints: [(point: CGPoint, confidence: Float)] = []

        let floatArray = data.withUnsafeBytes { (ptr: UnsafeRawBufferPointer) -> [Float] in
            Array(ptr.bindMemory(to: Float.self))
        }

        // MoveNet ì¶œë ¥: [y, x, confidence] * 17
        for i in 0..<17 {
            let baseIdx = i * 3
            let y = CGFloat(floatArray[baseIdx])
            let x = CGFloat(floatArray[baseIdx + 1])
            let conf = floatArray[baseIdx + 2]

            keypoints.append((point: CGPoint(x: x, y: y), confidence: conf))
        }

        // ğŸ”¥ ì‹ ë¢°ë„ ë†’ì€ í‚¤í¬ì¸íŠ¸ ê°œìˆ˜ ì„¸ê¸°
        let visibleCount = keypoints.filter { $0.confidence >= 0.5 }.count
        logToFile("âœ… MoveNet keypoints: ì „ì²´ \(keypoints.count)ê°œ, ì‹ ë¢°ë„ â‰¥ 0.5: \(visibleCount)ê°œ")

        return keypoints
    }

    // MARK: - Helper Functions

    private func resize(cgImage: CGImage, to size: CGSize) -> CGImage? {
        let context = CGContext(
            data: nil,
            width: Int(size.width),
            height: Int(size.height),
            bitsPerComponent: 8,
            bytesPerRow: 0,
            space: CGColorSpaceCreateDeviceRGB(),
            bitmapInfo: CGImageAlphaInfo.noneSkipLast.rawValue
        )
        context?.draw(cgImage, in: CGRect(origin: .zero, size: size))
        return context?.makeImage()
    }

    private func cgImageToPixelBuffer(_ cgImage: CGImage, size: CGSize) -> CVPixelBuffer? {
        let attrs = [
            kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue,
            kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue
        ] as CFDictionary

        var pixelBuffer: CVPixelBuffer?
        let status = CVPixelBufferCreate(
            kCFAllocatorDefault,
            Int(size.width),
            Int(size.height),
            kCVPixelFormatType_32BGRA,
            attrs,
            &pixelBuffer
        )

        guard status == kCVReturnSuccess, let buffer = pixelBuffer else {
            return nil
        }

        CVPixelBufferLockBaseAddress(buffer, [])
        defer { CVPixelBufferUnlockBaseAddress(buffer, []) }

        let context = CGContext(
            data: CVPixelBufferGetBaseAddress(buffer),
            width: Int(size.width),
            height: Int(size.height),
            bitsPerComponent: 8,
            bytesPerRow: CVPixelBufferGetBytesPerRow(buffer),
            space: CGColorSpaceCreateDeviceRGB(),
            bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue
        )

        context?.draw(cgImage, in: CGRect(origin: .zero, size: size))

        return buffer
    }

    /// ë°ê¸° ê³„ì‚° (VisionAnalyzerì™€ í˜¸í™˜)
    func calculateBrightness(from cgImage: CGImage) -> Float {
        let width = min(cgImage.width, 100)
        let height = min(cgImage.height, 100)

        guard let context = CGContext(
            data: nil,
            width: width,
            height: height,
            bitsPerComponent: 8,
            bytesPerRow: width * 4,
            space: CGColorSpaceCreateDeviceRGB(),
            bitmapInfo: CGImageAlphaInfo.premultipliedLast.rawValue
        ) else { return 0.5 }

        context.draw(cgImage, in: CGRect(x: 0, y: 0, width: width, height: height))
        guard let data = context.data else { return 0.5 }

        let buffer = data.bindMemory(to: UInt8.self, capacity: width * height * 4)
        var totalBrightness: Float = 0

        for i in stride(from: 0, to: width * height * 4, by: 4) {
            let r = Float(buffer[i]) / 255.0
            let g = Float(buffer[i + 1]) / 255.0
            let b = Float(buffer[i + 2]) / 255.0
            totalBrightness += (r + g + b) / 3.0
        }

        return totalBrightness / Float(width * height)
    }

    /// MoveNet ì „ì²˜ë¦¬ (192x192 RGB â†’ Data)
    private func preprocessForMoveNet(_ cgImage: CGImage) -> Data? {
        let width = 192
        let height = 192

        guard let context = CGContext(
            data: nil,
            width: width,
            height: height,
            bitsPerComponent: 8,
            bytesPerRow: width * 3,
            space: CGColorSpaceCreateDeviceRGB(),
            bitmapInfo: CGImageAlphaInfo.none.rawValue
        ) else { return nil }

        context.draw(cgImage, in: CGRect(x: 0, y: 0, width: width, height: height))
        guard let data = context.data else { return nil }

        // RGB ë°ì´í„°ë¥¼ Dataë¡œ ë³€í™˜ (uint8)
        let pixelData = Data(bytes: data, count: width * height * 3)
        return pixelData
    }

    /// YOLOì™€ MoveNet í¬ì¦ˆ ê²°ê³¼ ìœµí•©
    private func fusePoseResults(yolo: PoseAnalysisResult?, moveNet: PoseAnalysisResult?) -> PoseAnalysisResult? {
        // ë‘˜ ë‹¤ ì—†ìœ¼ë©´ nil
        guard yolo != nil || moveNet != nil else { return nil }

        // í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ê·¸ê²ƒ ë°˜í™˜
        if yolo == nil { return moveNet }
        if moveNet == nil { return yolo }

        // ë‘˜ ë‹¤ ìˆìœ¼ë©´ confidence ê¸°ì¤€ìœ¼ë¡œ ìœµí•©
        guard let yoloKeypoints = yolo?.keypoints,
              let moveNetKeypoints = moveNet?.keypoints else {
            return yolo // fallback
        }

        var fusedKeypoints: [(point: CGPoint, confidence: Float)] = []

        for i in 0..<min(yoloKeypoints.count, moveNetKeypoints.count) {
            let yoloKp = yoloKeypoints[i]
            let moveNetKp = moveNetKeypoints[i]

            // Confidenceê°€ ë†’ì€ ìª½ ì„ íƒ
            if yoloKp.confidence > moveNetKp.confidence {
                fusedKeypoints.append(yoloKp)
            } else {
                fusedKeypoints.append(moveNetKp)
            }
        }

        return PoseAnalysisResult(keypoints: fusedKeypoints)
    }

    /// ì „ì‹  ì˜ì—­ ì¶”ì • (VisionAnalyzerì™€ í˜¸í™˜)
    func estimateBodyRect(from faceRect: CGRect?) -> CGRect? {
        guard let face = faceRect else { return nil }

        let bodyWidth = face.width * 3
        let bodyHeight = face.height * 7
        let bodyX = face.midX - bodyWidth / 2
        let bodyY = face.minY

        return CGRect(x: bodyX, y: bodyY, width: bodyWidth, height: bodyHeight)
    }
}

// UIImage extensionì€ ì´ë¯¸ UIImage+Orientation.swiftì— ì •ì˜ë˜ì–´ ìˆìŒ
