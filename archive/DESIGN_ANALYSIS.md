# TryAngle 설계 분석 및 개선 방안

## 📊 현재 설계 분석

### 현재 아키텍처
```
[iOS 앱]
   ↓ (레퍼런스 이미지 + 현재 프레임)
[FastAPI 서버]
   ↓
[ImageComparator]
   ├─ 포즈 분석 (MediaPipe)
   ├─ 깊이 분석 (MiDaS)
   ├─ 구도 분석 (OpenCV)
   ├─ 색상/밝기 분석
   └─ EXIF 분석
   ↓
[2700장 클러스터링 데이터베이스]
   ↓
[피드백 생성]
   ↓
[iOS 앱 표시]
```

### 문제점

#### 1. **성능 문제** ⚠️
- **서버 왕복 시간**: 0.5~1초 (네트워크 + 분석)
- **실시간성 부족**: 사용자 움직임에 즉각 반응 불가
- **2700장 클러스터링**: 실제로 사용되는지 불명확

#### 2. **복잡도 문제** 🔧
- 6개 모듈 (MediaPipe, MiDaS, OpenCV, EXIF, 클러스터링, 색상)
- 각 모듈 간 의존성 복잡
- 유지보수 어려움

#### 3. **사용자 경험 문제** 📱
- 피드백이 늦게 도착
- 어떤 피드백이 중요한지 불명확
- 완벽한 순간을 놓침

---

## 🎯 개선된 설계 (현재 적용 중)

### 하이브리드 아키텍처
```
[iOS 앱]
   ├─ 실시간 분석 (로컬, 60fps)
   │    ├─ Vision Framework (얼굴/신체 인식)
   │    ├─ 프레이밍 분석
   │    ├─ 구도 분석
   │    └─ 완벽도 점수 계산
   │    ↓
   │  [즉시 피드백] (16ms 지연)
   │
   └─ 서버 분석 (2초마다)
        ├─ MediaPipe 포즈 분석
        └─ 정밀 피드백
        ↓
      [보조 피드백]
```

### 장점 ✅
1. **실시간 반응**: 16ms (60fps)
2. **우선순위 명확**: 프레이밍 > 구도 > 포즈
3. **자동 촬영**: 완벽한 순간 자동 감지
4. **안정성**: 히스테리시스로 떨림 방지

---

## 🚀 추천 대안 (장기)

### Option 1: CoreML 온디바이스 모델 ⭐⭐⭐⭐⭐
**추천 이유**: 가장 실용적이고 효과적

```
[iOS 앱]
   └─ CoreML 모델 (온디바이스)
        ├─ PoseNet / MoveNet (포즈)
        ├─ Vision Framework (프레이밍)
        └─ 통합 점수 계산
        ↓
      [실시간 피드백] (< 30ms)
```

**구현 방법**:
1. Apple의 **PoseNet** 또는 **MoveNet** 모델 사용
   - CoreML 형식으로 변환 가능
   - 60fps 실시간 처리
   - MediaPipe보다 빠름

2. 커스텀 모델 학습
   - 2700장 데이터 활용
   - CreateML로 "좋은 구도" 분류 모델 학습
   - 이진 분류: "완벽함" vs "개선 필요"

**장점**:
- ✅ 완전한 실시간 처리
- ✅ 서버 불필요 (오프라인 작동)
- ✅ 프라이버시 보호
- ✅ 비용 절감

**단점**:
- ⚠️ 초기 모델 학습 필요
- ⚠️ 앱 크기 증가 (50~100MB)

**구현 난이도**: 중 (2~3주)

---

### Option 2: TensorFlow Lite (경량화) ⭐⭐⭐⭐
**추천 이유**: 빠르고 검증된 방법

```
[iOS 앱]
   └─ TensorFlow Lite 모델
        ├─ MoveNet (포즈)
        ├─ EfficientNet (구도 평가)
        └─ 커스텀 레이어
        ↓
      [실시간 피드백] (< 50ms)
```

**구현 방법**:
1. Google의 **MoveNet Lightning** 사용
   - 초당 50프레임 이상 처리
   - 모델 크기 3MB

2. 2700장 데이터로 **전이 학습**
   - EfficientNet-Lite로 "좋은 구도" 학습
   - 4가지 클래스: 완벽/좋음/보통/나쁨

**장점**:
- ✅ 매우 빠름
- ✅ 모델 크기 작음
- ✅ 크로스 플랫폼 (Android도 가능)

**단점**:
- ⚠️ TensorFlow Lite 통합 필요

**구현 난이도**: 중 (2주)

---

### Option 3: 클라우드 Vision API ⭐⭐⭐
**추천 이유**: 빠른 프로토타입

```
[iOS 앱]
   ↓
[Google Cloud Vision API / AWS Rekognition]
   ├─ 포즈 감지
   ├─ 얼굴 감지
   └─ 라벨링
   ↓
[커스텀 로직으로 점수 계산]
```

**장점**:
- ✅ 구현 빠름 (1주)
- ✅ 높은 정확도
- ✅ 유지보수 불필요

**단점**:
- ⚠️ 비용 발생 (이미지당 $1.5/1000장)
- ⚠️ 네트워크 필요
- ⚠️ 지연 여전히 존재

---

### Option 4: 현재 시스템 최적화 ⭐⭐
**추천 이유**: 가장 빠른 개선

**개선 방안**:
1. **불필요한 분석 제거**
   - MiDaS 깊이 분석 → Vision으로 대체
   - 색상 분석 제거
   - EXIF 분석 제거

2. **클러스터링 단순화**
   - 2700장 → 50개 대표 이미지
   - 실시간 클러스터링 제거
   - 규칙 기반 피드백

3. **서버 최적화**
   - FastAPI → gRPC (3배 빠름)
   - 이미지 압축 (품질 유지하며 1/4 크기)
   - 병렬 처리

**예상 성능**:
- 현재: 500~1000ms
- 개선 후: 200~300ms

---

## 💡 최종 추천

### 단기 (1주 이내) ✅ **현재 적용 완료**
- ✅ iOS 실시간 분석 (Vision Framework)
- ✅ 히스테리시스 안정화
- ✅ 자동 촬영 기능

### 중기 (1개월)
**Option 1: CoreML 모델 도입**
1. PoseNet CoreML 통합
2. 2700장으로 "완벽 구도" 분류 모델 학습
3. 서버 의존성 완전 제거

**예상 효과**:
- 지연: 16ms → **< 30ms** (체감 동일)
- 오프라인 작동
- 서버 비용 제로

### 장기 (3개월)
**전체 재설계**
1. CreateML로 커스텀 모델 학습
   - 입력: 레퍼런스 + 현재 프레임
   - 출력: 완벽도 점수 + 피드백 벡터

2. 온디바이스 ML
3. 서버는 모델 업데이트만 담당

---

## 📈 성능 비교

| 방식 | 지연 | 정확도 | 오프라인 | 비용 | 구현 난이도 |
|------|------|--------|----------|------|-------------|
| **현재 (개선 전)** | 500ms | ⭐⭐⭐⭐ | ❌ | $$ | - |
| **하이브리드 (현재)** | 16ms | ⭐⭐⭐⭐ | ❌ | $$ | ⭐⭐⭐ |
| **CoreML** | 30ms | ⭐⭐⭐⭐⭐ | ✅ | $ | ⭐⭐⭐ |
| **TensorFlow Lite** | 50ms | ⭐⭐⭐⭐ | ✅ | $ | ⭐⭐⭐⭐ |
| **Cloud API** | 300ms | ⭐⭐⭐⭐⭐ | ❌ | $$$ | ⭐⭐ |

---

## 🎬 결론

### 즉시 적용 가능 (완료됨)
✅ 하이브리드 아키텍처로 실시간성 확보
✅ 자동 촬영으로 완벽한 순간 포착

### 다음 단계 추천
**CoreML PoseNet 도입** (가성비 최고)
- 2주 내 구현 가능
- 완전한 실시간 처리
- 서버 비용 제로

### 2700장 클러스터링 데이터 활용법
1. **CoreML 모델 학습 데이터**로 사용
2. "좋은 구도" vs "나쁜 구도" 이진 분류
3. 온디바이스에서 즉시 평가

현재 시스템은 잘 작동하지만, CoreML로 전환하면 **완벽한 실시간 경험**을 제공할 수 있습니다.
